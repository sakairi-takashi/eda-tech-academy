{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Event-driven solution - Tech Academy Workshop \u00b6 The general objective of this academy training is to develop deeper technical skills, within the IBM technical community, to plan, design, build and execute a sub-set of key Proof Of Concept tasks and activities. In this Event-driven solution workshop, you will learn how to use of Event-Driven/Near Real-time Integration Patterns, Technologies and Implementation Strategies which should help you developing simple Streaming applications and proof of concepts. Two possible journeys \u00b6 We recognize that not every student will have the same level of knowledge of Event Streams and integration capability or even Java development background. We propose two different tracks depending of your current skill level: Beginner journey : do the demonstration lab, which is about being confortable to demonstrate Event Streams from \"A to Z\": this is a good 3 hours run thru exercise that will give you access to a lot of content you should be able to leverage overtime. You may also do the lab 1 as this is a system design mockup exercise, that should help you in the future. Go to Track 1 - demonstration lab >> Advance journey : Perform labs 1 to 4, this is about learning Kafka Stream, Open Liberty application, using Configuration as Code for OpenShift deployment and GitOps approach. A full story and a set of reusable assets you may want to reuse to build you future proof of concepts. You can also review quickly the demonstration lab, as it could be helpful, as a script to answer specific customer's questions. Continue to the goals section below >> Advance Journey Goals: \u00b6 Solidify a full body of knowledge in the near real-time / event driven architecture domain Design and create an initial prototype solution that addresses the DOU goals and objectives. Establish an initial point of view for adopting GitOps methods and practices for event streams implementations. Give you foundational knowledge to deeply address event-driven solution design Study [Optional] - Lecture: Review key EDA patterns, use cases and usage scenarios. \u00b6 Duration: 20 minutes. Delivery : presenter will go over some concepts and presentation of the material. Can be self space study. What are the technical use cases where Event Streams is a good fit. Why customers are going full speed to adopt EDA? Technical use cases - general positioning Assessment questions for Event Streams opportunity Detailed use case slides Read more EDA internal site - use cases EDA public web site we use to present content to customers Lab 1: System design for a real-time inventory solution \u00b6 Duration : 30 minutes Delivery: Can be done with 2 or 3 students and self pace. Review the Client provided requirements, and elaborate a system design for an EDA solution. Review the problem statement and the lab's instructions A potential solution may look like Lab 2 [Optional]: Implement a simple item sell or restock event stream processing with Kafka Streams API \u00b6 Learning the basic of Kafka Streams, and implement a store aggregation processing with Java. The code and environment should be self efficient. Optional : if you really hate programming you can skip this lab, or at least read the first few exercices so you can understand some of the streaming concepts. Duration : 90 minutes Delivery : Pair programming. Lab's instructions includes a set of progressing exercise to learn basic Kafka Streams programming. Last exercise solution . Lab 3: Deploy the real time inventory solution in one Click \u00b6 Duration : 20 minutes This is a simple of the end to end solution, you will be able to deploy in few commands. It is also important to review some deployment definition content: 20 minutes lab Lab 4: Deploy the solution with OpenShift GitOps \u00b6 Duration : 30 minutes In this lab, you will deploy the ArgoCD applications that monitor your git repository for any change to the configuration and deploy the different services and MQ broker in your own namespace. Lab 4 GitOps deployment Lab 5 [Optional]: Monitor with Instana \u00b6","title":"Introduction"},{"location":"#event-driven-solution-tech-academy-workshop","text":"The general objective of this academy training is to develop deeper technical skills, within the IBM technical community, to plan, design, build and execute a sub-set of key Proof Of Concept tasks and activities. In this Event-driven solution workshop, you will learn how to use of Event-Driven/Near Real-time Integration Patterns, Technologies and Implementation Strategies which should help you developing simple Streaming applications and proof of concepts.","title":"Event-driven solution - Tech Academy Workshop"},{"location":"#two-possible-journeys","text":"We recognize that not every student will have the same level of knowledge of Event Streams and integration capability or even Java development background. We propose two different tracks depending of your current skill level: Beginner journey : do the demonstration lab, which is about being confortable to demonstrate Event Streams from \"A to Z\": this is a good 3 hours run thru exercise that will give you access to a lot of content you should be able to leverage overtime. You may also do the lab 1 as this is a system design mockup exercise, that should help you in the future. Go to Track 1 - demonstration lab >> Advance journey : Perform labs 1 to 4, this is about learning Kafka Stream, Open Liberty application, using Configuration as Code for OpenShift deployment and GitOps approach. A full story and a set of reusable assets you may want to reuse to build you future proof of concepts. You can also review quickly the demonstration lab, as it could be helpful, as a script to answer specific customer's questions. Continue to the goals section below >>","title":"Two possible journeys"},{"location":"#advance-journey-goals","text":"Solidify a full body of knowledge in the near real-time / event driven architecture domain Design and create an initial prototype solution that addresses the DOU goals and objectives. Establish an initial point of view for adopting GitOps methods and practices for event streams implementations. Give you foundational knowledge to deeply address event-driven solution design","title":"Advance Journey Goals:"},{"location":"#study-optional-lecture-review-key-eda-patterns-use-cases-and-usage-scenarios","text":"Duration: 20 minutes. Delivery : presenter will go over some concepts and presentation of the material. Can be self space study. What are the technical use cases where Event Streams is a good fit. Why customers are going full speed to adopt EDA? Technical use cases - general positioning Assessment questions for Event Streams opportunity Detailed use case slides Read more EDA internal site - use cases EDA public web site we use to present content to customers","title":"Study [Optional] - Lecture: Review key EDA patterns, use cases and usage scenarios."},{"location":"#lab-1-system-design-for-a-real-time-inventory-solution","text":"Duration : 30 minutes Delivery: Can be done with 2 or 3 students and self pace. Review the Client provided requirements, and elaborate a system design for an EDA solution. Review the problem statement and the lab's instructions A potential solution may look like","title":"Lab 1: System design for a real-time inventory solution"},{"location":"#lab-2-optional-implement-a-simple-item-sell-or-restock-event-stream-processing-with-kafka-streams-api","text":"Learning the basic of Kafka Streams, and implement a store aggregation processing with Java. The code and environment should be self efficient. Optional : if you really hate programming you can skip this lab, or at least read the first few exercices so you can understand some of the streaming concepts. Duration : 90 minutes Delivery : Pair programming. Lab's instructions includes a set of progressing exercise to learn basic Kafka Streams programming. Last exercise solution .","title":"Lab 2 [Optional]: Implement a simple item sell or restock event stream processing with Kafka Streams API"},{"location":"#lab-3-deploy-the-real-time-inventory-solution-in-one-click","text":"Duration : 20 minutes This is a simple of the end to end solution, you will be able to deploy in few commands. It is also important to review some deployment definition content: 20 minutes lab","title":"Lab 3: Deploy the real time inventory solution in one Click"},{"location":"#lab-4-deploy-the-solution-with-openshift-gitops","text":"Duration : 30 minutes In this lab, you will deploy the ArgoCD applications that monitor your git repository for any change to the configuration and deploy the different services and MQ broker in your own namespace. Lab 4 GitOps deployment","title":"Lab 4: Deploy the solution with OpenShift GitOps"},{"location":"#lab-5-optional-monitor-with-instana","text":"","title":"Lab 5 [Optional]: Monitor with Instana"},{"location":"eda/","text":"Why event-driven architecture \u00b6 adoption for loosely coupled, event-driven microservice solutions, with new data pipeline used to inject data to modern data lakes, and the adoption of event backbone technology like Apache Kafka, or Apache Pulsar. Event-driven architecture (EDA) is an architecture pattern that promotes the production, detection, consumption of, and reaction to events. It supports asynchronous communication between components and most of the time a pub/sub programming model. The adoption of microservices brings some interesting challenges like data consistency, contract coupling, and scalability that EDA helps to address. From the business value point of view, adopting this architecture helps to scale business applications according to workload and supports easy extension by adding new components over time that are ready to produce or consume events that are already present in the overall system. New real-time data streaming applications can be developed which we were not able to do before. Technical needs \u00b6 At the technical level we can see three adoptions of event-driven solutions: Modern data pipeline to move the classical batch processing of extract, transform and load job to real-time ingestion, where data are continuously visible in a central messaging backbone. The data sources can be databases, queues, or specific producer applications, while the consumers can be applications, streaming flow, long storage bucket, queues, databases\u2026 Adopt asynchronous communication , publish-subscribe protocol between cloud-native microservices to help scaling and decoupling: the adoption of microservices for developing business applications, has helped to address maintenance and scalability, but pure RESTful or SOAP based solutions have brought integration and coupling challenges that inhibited the agility promised by microservice architecture. Pub/sub helps to improve decoupling, but design good practices are very important. See more about EDA advantages for microservices Real time analytics : this embraces pure analytic computations like aggregate on the data streams but also complex event processing, time window-based reasoning, or AI scoring integration on the data streams. Read more business requirements","title":"Why EDA now?"},{"location":"eda/#why-event-driven-architecture","text":"adoption for loosely coupled, event-driven microservice solutions, with new data pipeline used to inject data to modern data lakes, and the adoption of event backbone technology like Apache Kafka, or Apache Pulsar. Event-driven architecture (EDA) is an architecture pattern that promotes the production, detection, consumption of, and reaction to events. It supports asynchronous communication between components and most of the time a pub/sub programming model. The adoption of microservices brings some interesting challenges like data consistency, contract coupling, and scalability that EDA helps to address. From the business value point of view, adopting this architecture helps to scale business applications according to workload and supports easy extension by adding new components over time that are ready to produce or consume events that are already present in the overall system. New real-time data streaming applications can be developed which we were not able to do before.","title":"Why event-driven architecture"},{"location":"eda/#technical-needs","text":"At the technical level we can see three adoptions of event-driven solutions: Modern data pipeline to move the classical batch processing of extract, transform and load job to real-time ingestion, where data are continuously visible in a central messaging backbone. The data sources can be databases, queues, or specific producer applications, while the consumers can be applications, streaming flow, long storage bucket, queues, databases\u2026 Adopt asynchronous communication , publish-subscribe protocol between cloud-native microservices to help scaling and decoupling: the adoption of microservices for developing business applications, has helped to address maintenance and scalability, but pure RESTful or SOAP based solutions have brought integration and coupling challenges that inhibited the agility promised by microservice architecture. Pub/sub helps to improve decoupling, but design good practices are very important. See more about EDA advantages for microservices Real time analytics : this embraces pure analytic computations like aggregate on the data streams but also complex event processing, time window-based reasoning, or AI scoring integration on the data streams. Read more business requirements","title":"Technical needs"},{"location":"future/","text":"Next steps \u00b6 You can run on your own OpenShift cluster with existing assets and more assets available on the public Git repositories. EDA Community Call \u00b6 IBM - Internal: Register in Your Learning for a community call every Wednesday at 7:00am PST (Webex boyerje). We will organize the calls in different scope: Week 1 of every month: Beginner sessions for Kafka Week 2 of every month: Bring you own opportunity so we can share trick on how to make it progresses Week 3 of every month: Deeper dive: asset presentation, architecture, coding discussions Week 4 of event month: Project success story, opportunity success story, product roadmap update. Internal site \u00b6 IBM internal site for sharing knowledge on use cases / workshops / ... Kafka Connector World \u00b6 The Event Streams demonstration introduced the Kafka Connect framework, The real time inventory solution uses MQ source connector, with the Kafka connector cluster defined in this kafka-connect.yaml file as: apiVersion : eventstreams.ibm.com/v1beta2 kind : KafkaConnect metadata : name : std-1-connect-cluster annotations : eventstreams.ibm.com/use-connector-resources : \"true\" spec : version : 3.0.0 replicas : 2 bootstrapServers : es-demo-kafka-bootstrap.cp4i-eventstreams.svc:9093 image : quay.io/ibmcase/eda-kconnect-cluster-image:latest resources : limits : cpu : 2000m memory : 2Gi requests : cpu : 1000m memory : 2Gi template : pod : imagePullSecrets : [] metadata : annotations : productChargedContainers : std-1-connect-cluster-connect eventstreams.production.type : CloudPakForIntegrationNonProduction productID : 2a79e49111f44ec3acd89608e56138f5 productName : IBM Event Streams for Non Production productVersion : 11.0.0 productMetric : VIRTUAL_PROCESSOR_CORE cloudpakId : c8b82d189e7545f0892db9ef2731b90d cloudpakName : IBM Cloud Pak for Integration cloudpakVersion : 2022.1.1 productCloudpakRatio : \"2:1\" config : group.id : std-1-connect-cluster offset.storage.topic : std-1-connect-cluster-offsets config.storage.topic : std-1-connect-cluster-configs status.storage.topic : std-1-connect-cluster-status config.storage.replication.factor : 3 offset.storage.replication.factor : 3 status.storage.replication.factor : 3 tls : trustedCertificates : - secretName : es-demo-cluster-ca-cert certificate : ca.crt authentication : type : tls certificateAndKey : certificate : user.crt key : user.key secretName : std-1-tls-user And the source definition kafka-mq-src-connector.yaml you may need to go deeper with labs and best practices: a technical summary MQ connector lab Deploy cloud object storage sink connector lab Deploy a S3 sink connector using Apache Camel Mirror maker 2.0 as a Kafka Framework solution Code source of the MQ source connector Code source of the Rabbit MQ connector and the matching lab Code source of the JDBC sink connector Reactive Messaging Programming \u00b6 Event-driven microservices adopt the reactive manifesto , which means use messaging as a way to communicate between components. When the components are distributed, Kafka or MQ are used as broker. Microprofile Reactive Messaging is a very elegant and easier way to integrate with Kafka / Event Streams. The best support for it, is in Quarkus and this reactive with kafka guide is a first read . The Microprofile Reactive Messaging 1.0 is supported in OpenLiberty with Microprofile 3.0. The code template in the EDA quickstart repository includes reactive messaging code template. The SAGA implementation \u00b6 Long running process between microservice is addressed by the adoption of the SAGA pattern. You can read about the pattern in this note And visit the Choreography implementation done with, Event Streams, Reactive Programming here Order microservice keeping SAGA coherence - git repository Reefer microservice participant to the SAGA - git repo Voyage microservice SAGA participant - git repo The orchestration implementation with, Event Streams, Reactive Programming here Change data capture with Debezium and Outbox pattern \u00b6 Very interesting lab with Debezium using outbox pattern. Full GitOps story \u00b6 To get a better understanding of the EDA gitops process see this technical note and reuse the following git repostiories: EDA GitOps Catalog RT inventory gitops Instana monitoring \u00b6 Deploying Instana APM on the Event Streams Cluster running on Openshift/kubernetes. Create the instana-agent project and set the policy permissions to ensure the instana-agent service account is in the privileged security context. oc login -u system:admin oc new-project instana-agent oc adm policy add-scc-to-user privileged -z instana-agent Login to Instana console: https://training-kafka.instana.io Click on Deploy agent on the Top right corner Select OpenShift and enter the desired ClusterName/ClusterID that needs to be monitored with Instana. You find the clusterID from Openshift console. Download the yaml file generated by Instana Navigate to Openshift -> Workloads -> DeamonSets and import the instana-agent.yaml and create the deamonset per each of the document streams inside the yaml separately. For example, since we already created the name space and serviceaccount in Step1, we can skip the first two documents below and start from Kind:secret. Once DeamonSet has been successfully created, validate that the pods are running suceessfully, wait for 5 minutes for instana to automatically discover the kafka components and instrument them.","title":"More..."},{"location":"future/#next-steps","text":"You can run on your own OpenShift cluster with existing assets and more assets available on the public Git repositories.","title":"Next steps"},{"location":"future/#eda-community-call","text":"IBM - Internal: Register in Your Learning for a community call every Wednesday at 7:00am PST (Webex boyerje). We will organize the calls in different scope: Week 1 of every month: Beginner sessions for Kafka Week 2 of every month: Bring you own opportunity so we can share trick on how to make it progresses Week 3 of every month: Deeper dive: asset presentation, architecture, coding discussions Week 4 of event month: Project success story, opportunity success story, product roadmap update.","title":"EDA Community Call"},{"location":"future/#internal-site","text":"IBM internal site for sharing knowledge on use cases / workshops / ...","title":"Internal site"},{"location":"future/#kafka-connector-world","text":"The Event Streams demonstration introduced the Kafka Connect framework, The real time inventory solution uses MQ source connector, with the Kafka connector cluster defined in this kafka-connect.yaml file as: apiVersion : eventstreams.ibm.com/v1beta2 kind : KafkaConnect metadata : name : std-1-connect-cluster annotations : eventstreams.ibm.com/use-connector-resources : \"true\" spec : version : 3.0.0 replicas : 2 bootstrapServers : es-demo-kafka-bootstrap.cp4i-eventstreams.svc:9093 image : quay.io/ibmcase/eda-kconnect-cluster-image:latest resources : limits : cpu : 2000m memory : 2Gi requests : cpu : 1000m memory : 2Gi template : pod : imagePullSecrets : [] metadata : annotations : productChargedContainers : std-1-connect-cluster-connect eventstreams.production.type : CloudPakForIntegrationNonProduction productID : 2a79e49111f44ec3acd89608e56138f5 productName : IBM Event Streams for Non Production productVersion : 11.0.0 productMetric : VIRTUAL_PROCESSOR_CORE cloudpakId : c8b82d189e7545f0892db9ef2731b90d cloudpakName : IBM Cloud Pak for Integration cloudpakVersion : 2022.1.1 productCloudpakRatio : \"2:1\" config : group.id : std-1-connect-cluster offset.storage.topic : std-1-connect-cluster-offsets config.storage.topic : std-1-connect-cluster-configs status.storage.topic : std-1-connect-cluster-status config.storage.replication.factor : 3 offset.storage.replication.factor : 3 status.storage.replication.factor : 3 tls : trustedCertificates : - secretName : es-demo-cluster-ca-cert certificate : ca.crt authentication : type : tls certificateAndKey : certificate : user.crt key : user.key secretName : std-1-tls-user And the source definition kafka-mq-src-connector.yaml you may need to go deeper with labs and best practices: a technical summary MQ connector lab Deploy cloud object storage sink connector lab Deploy a S3 sink connector using Apache Camel Mirror maker 2.0 as a Kafka Framework solution Code source of the MQ source connector Code source of the Rabbit MQ connector and the matching lab Code source of the JDBC sink connector","title":"Kafka Connector World"},{"location":"future/#reactive-messaging-programming","text":"Event-driven microservices adopt the reactive manifesto , which means use messaging as a way to communicate between components. When the components are distributed, Kafka or MQ are used as broker. Microprofile Reactive Messaging is a very elegant and easier way to integrate with Kafka / Event Streams. The best support for it, is in Quarkus and this reactive with kafka guide is a first read . The Microprofile Reactive Messaging 1.0 is supported in OpenLiberty with Microprofile 3.0. The code template in the EDA quickstart repository includes reactive messaging code template.","title":"Reactive Messaging Programming"},{"location":"future/#the-saga-implementation","text":"Long running process between microservice is addressed by the adoption of the SAGA pattern. You can read about the pattern in this note And visit the Choreography implementation done with, Event Streams, Reactive Programming here Order microservice keeping SAGA coherence - git repository Reefer microservice participant to the SAGA - git repo Voyage microservice SAGA participant - git repo The orchestration implementation with, Event Streams, Reactive Programming here","title":"The SAGA implementation"},{"location":"future/#change-data-capture-with-debezium-and-outbox-pattern","text":"Very interesting lab with Debezium using outbox pattern.","title":"Change data capture with Debezium and Outbox pattern"},{"location":"future/#full-gitops-story","text":"To get a better understanding of the EDA gitops process see this technical note and reuse the following git repostiories: EDA GitOps Catalog RT inventory gitops","title":"Full GitOps story"},{"location":"future/#instana-monitoring","text":"Deploying Instana APM on the Event Streams Cluster running on Openshift/kubernetes. Create the instana-agent project and set the policy permissions to ensure the instana-agent service account is in the privileged security context. oc login -u system:admin oc new-project instana-agent oc adm policy add-scc-to-user privileged -z instana-agent Login to Instana console: https://training-kafka.instana.io Click on Deploy agent on the Top right corner Select OpenShift and enter the desired ClusterName/ClusterID that needs to be monitored with Instana. You find the clusterID from Openshift console. Download the yaml file generated by Instana Navigate to Openshift -> Workloads -> DeamonSets and import the instana-agent.yaml and create the deamonset per each of the document streams inside the yaml separately. For example, since we already created the name space and serviceaccount in Step1, we can skip the first two documents below and start from Kind:secret. Once DeamonSet has been successfully created, validate that the pods are running suceessfully, wait for 5 minutes for instana to automatically discover the kafka components and instrument them.","title":"Instana monitoring"},{"location":"demo/","text":"Demonstrating Event Streams from A to Z \u00b6 Warning This exercise is not a step by step lab, but more an explanation of all the concepts and components involved in an event-driven solution with Event Streams. We have provided scripts that can be leveraged (see the table of content on the right to get scripts) to demonstrate and talk about those items in front of your prospect. A typical demonstration script will include at least the following subjects (See right navigation bar to go to a specific sections): Review Event Streams Components Operator based deployment and Day 2 operations Topic creation Producer application Consumer application, consumer group concepts, offset concepts User access, authentication mechanism Monitoring Event Streaming Geo-replication As education enablement you can go step by step following the current structure. As a reusable asset for your future demonstration, you can pick and choose from the right navigation bar the items to highlight in front of your audiance. All the demonstration can be done on IBM CoC clusters: see the environments section in the EDA labs introduction. Pre-requisites \u00b6 You will need access to an Event Streams instance installed on an OpenShift cluster with access to the OpenShift Console to demonstrate Operators. You\u2019ll need the following as well: git client Have oc cli installed. It can be done once connected to the OpenShift cluster using the <?> icon on the top-right and \"Command Line Tool\" menu. Get docker desktop or podman on your local laptop Java 11 is need to run the Event Streams starter application . Review Event Streams components \u00b6 Narative : Event Streams is the IBM packaging of different Open Source projects to support an integrated user experience deploying and managing Kafka on OpenShift cluster. The following figure illustrates such components: src for this diagram is here Event streams ( Apache Kafka packaging) runs on OpenShift cluster. The deployment and the continuous monitoring of Event Streams resources definition and deployed resources is done via Operator ( Strimzi open source project ) Event Streams offers a user interface to manage resources and exposes simple dashboard. We will use it during the demonstration. The schema management is done via schema registry and the feature is integrated in Event Streams user interface but in the back end, is supported by Apicur.io registry External event sources can be integrated via the Kafka Connector framework and Event Streams offers a set of connectors and can partner to other companies to get specific connectors. External sinks can be used to persist messages for longer time period that the retention settings done at the topic level. S3 buckets can be use, IBM Cloud object storage, and Kafka Sink connectors. There is this cloud object storage lab , or S3 sink with Apache Camel lab to present such integrations. Event Streams monitoring is done using Dashboards in Event Streams user interface but also within OpenShift monitoring and Kibana dashboards. Green components are application specifics, and represent event-driven microservices (see eda-quickstart project for code templates ) or Kafka Streaming apps, or Apache Flink apps. For cluster optimization, Event Streams integrates Cruise Control, with goal constraints, to act on cluster resource usage. More argumentations Kafka is essentially a distributed platform to manage append log with a pub/sub protocol to get streams of events. Messages are saved for a long period of time. Kafka connectors can also being supported by APP Connect integration capabilities or Apache Camel kafka connectors . To learn more about Kafka Connector see our summary Concepts \u00b6 If needed there are some important concepts around Kafka to present to your audience. See this kafka technology overview . High Availability \u00b6 High availability is ensured by avoiding single point of failure, parallel, and replications. The following figure is a golden topology for OpenShift with Event Streams components deployed to it. Event Streams Brokers run in OpenShift worker nodes, and it may be relevant to use one broker per worker nodes using zone affinity policies. src for this diagram is here Kafka connectors, or streaming applications runs in worker node too and access brokers via mutual TLS authentication and SSL encryption. Kafka brokers are spread across worker nodes using anti-affinity policies. Read more Kafka High availability deeper dive See the OpenShift golden topology article in production deployment site. A production deployment descriptor for Event Streams Product documentation on planning installation Operator based deployment \u00b6 There are several ways to install Event Streams. We are going to look at this, with Operator Hub. Go to your Openshift console, select Operator Hub and search for Event Streams. Here you can install the operator to manage all cluster instances deployed to the OpenShift environment. Operator can automatically deploy new product version once released by IBM. In the OpenShift Console, select the project where Event Streams is deployed. On left menu select Operators > Installed Operators , scroll to select IBM Event Streams, you are now in the Operator user interface, from where you can see local resources and create new one. Go to the Event Streams menu and select existing cluster definition You are now viewing the cluster definition as it is deployed. Select the YAML choice and see the spec elements. You can see how easy it would be simple to add a broker by changing the spec.strimziOverrides.kafka.replicas value. Also in this view, the Samples menu presents some examples of cluster definitions. Kafka brokers, Zookeeper nodes or other components like Apicurio can all be scaled to meet your needs: Number of replicas CPU request or limit settings Memory request or limit settings JVM settings On the left side menu select Workloads->Pods. Here you see pods that are in the Event Streams namespace like Broker, zookeepers, user interface, schema registry: If needed, you can explain the concept of persistence and Storage class: Kafka save records on disk for each broker, and so it can use VM disk or network file systems. As Kubernetes deployed application, Event Streams define persistence via persistence claim and expected quality of service using storage class. On the left side menu select, Storage > PersistenceVolumesClaims in the OpenShift console, each broker has its own claim, OpenShift allocated Persistence Volumes with expected capacity. The Storage class was defined by OpenShift administrator, and in the example above, it use CEPH storage. Read more Cepth and block devise Kafka Brokers and architecture GitOps approach for Day1 and Day 2 operations Review Event Streams user interface features \u00b6 There are a number of ways to navigate to Event Streams Console by getting the exposed routes Using Routes in Openshift: On the left side menu select Networking > Routes in the OpenShift console. Find es-demo-ibm-es-ui and then go to Location. Select that link and it will take you to the Event Streams Console. Depending to your installation, you may reach Cloud Pak for Integration console, in this case, select Entreprise LDAP, and enter your userid and password. Using the cli: (replace es-demo with the name of your cluster, and cp4i-eventstreams with the name of the project where Event Streams runs into ) chrome $( oc get eventstreams es-demo -n cp4i-eventstreams -o jsonpath = '{.status.adminUiUrl}' ) Once you logged in using the LDAP credentials provided, you should reach the home page. The set of features available from this home page, are topic management, schema registry, consumer groups, monitoring, and toolbox ... you will review most of those features in this demo. Topic management \u00b6 Topics are append log, producer applications publish records to topics, and consumer applications subscribe to topics. Kafka messages themselves are immutable. Deletion and compaction of data are administrative operations. Navigate to the topic main page by using the Event Streams left side menu and select Topics. replicas are to support record replication and to ensure high availability. Producer can wait to get acknowledgement of replication. Replicas needs to be set to 3 to supports 2 broker failures at the same time. partition defines the number of append logs managed by the broker. Each partition has a leader, and then follower brokers that replicate records from the leader. Partitions are really done to do parallel processing at the consumer level. The following diagram can be used to explain those concepts. Create a topic for the Starter app, using the user interface: Warning When running on a multi-tenant Event Streams cluster you need to modify the name of the topic, to avoid conflicting with other topic name, use your userid as prefix. Use only one partition. The default retention time is 7 days, Kafka is keeping data for a long time period, so any consumer applications can come and process messages at any time. It helps for microservice resilience and increase decoupling. Finally the replicas for high availability. 3 is the production deployment, and in-sync replicas = 2, means producer get full acknowledge when there are 2 replicas done. Broker partition leader keeps information of in-sync replicas. Just as an important note, topic may be created via yaml file or using CLI command. Go to the rt-inventory GitOps - es-topics.yaml and explain some of the parameters. We will go over the process of adding new topic by using GitOps in this section Read more Topic summary Kafka topic configuration Understand Kafka producer Review Consumer Replication and partition leadership Run the Starter Application \u00b6 See the beginned dedicated lab to get the application started, once done: Go back to the Event Streams console, Topic management, and the starter-app topic, select the Messages tab and go to any messages. Explain that each messages has a timestamp, and an offset that is an increasing number. Offset are used by consumer to be able to replay from an older message, or when restarting from a failure. Offset management at the consumer application level is tricky, if needed you can have a deeper conversation on this topic later after the demonstration. At the topic level, it is possible to see the consumer of the topic: Go to the Consumer groups tab, to see who are the consumer, if the consumer is active or not (this will be controlled by the heartbeat exchanges when consumer poll records and commit their read offset). One important metric in this table is the unconsumed partition value. If the number of partitions is more than 1 and there are less than the number of consumer than of partition, then it means a consumer is processing two or more partitions. Going by to the starter application, you can start consuming the records. This is to demonstrate that consumer can connect at anytime, and that it will quickly consume all messages. Stopping and restarting is also demonstrating that consumer, continues from the last read offset. There is an alternate of running this application on your laptop, it can be deployed directly to the same OpenShift cluster, we have defined deployment and config map to do so. Deploy starter app on OpenShift Use the same kafka.properties and truststore.p12 files you have downloaded with the starter application to create two kubernetes secrets holding these files in your OpenShift cluster oc create secret generic demo-app-secret --from-file = ./kafka.properties oc create secret generic truststore-cert --from-file = ./truststore.p12 Clone the following GitHub repo that contains the Kubernetes artifacts that will run the starter application. git clone https://github.com/ibm-cloud-architecture/eda-quickstarts.git Change directory to where those Kubernetes artefacts are. cd eda-quickstarts/kafka-java-vertz-starter Deploy the Kubernetes artefacts. oc apply -k app-deployment Get the route to the starter application running on your OpenShift cluster. oc get route es-demo -o = jsonpath = '{.status.ingress[].host}' Point your browser to that url to work with the IBM Event Streams Starter Application. Back to the Cluster configuration \u00b6 Event Streams cluster can be configured with Yaml and you can review the following cluster definition to explain some of the major properties: EDA GitOps Catalog - example of production cluster.yaml : Property Description Replicas specify the # of brokers or zookeeper Resources CPU or mem requested and limit Listeners Define how to access the cluster: External with scram authentication and TLS encryption, and internal using TLS authentication or PLAIN. Entity operators Enable topic and user to be managed by operator Rack awareness To use zone attribute from node to allocate brokers in different AZ Cruise Control Open source for cluster rebalancing Metrics To export different Kafka metrics to Prometheus via JMX exporter For Kafka, the following aspects of a deployment can impact the resources you need: Throughput and size of messages The number of network threads handling messages The number of producers and consumers The number of topics and partitions Producing messages \u00b6 The product documentation - producing message section goes into details of the concepts. For a demonstration purpose, you need to illustrate that you can have multiple types of Kafka producer: Existing Queuing apps, which are using IBM MQ, and get their messages transparently sent to Event Streams, using IBM MQ Streaming Queue and MQ Source Kafka Connector . Microservice applications publishing events using Kafka producer API, or reactive messaging in Java Microprofile. For Nodejs, Python there is a C library which supports the Kafka APIs. We have code template for that. Change data capture product, like Debezium , that gets database updates and maps records to events in topic. One topic per table. Some data transformation can be done on the fly. Streaming applications, that do stateful computing, real-time analytics, consuming - processing - publishing events from one to many topics and produce to one topic. App connect flow can also being source for events to Events Streams, via connectors. The following diagram illustrates those event producers. Each producer needs to get a URL to the broker, defines the protocol to authenticate, and gets server side TLS certificate, the topic name, and that's it to start sending messages. For production deployment, event structures are well defined and schema are used to ensure consumer can understand how to read messages from the topic/partition. Event Streams offers a schema registry to manage those schema definitions. You can introduce the schema processing with the figure below: Schema flow explanations (1) Avro or Json schemas are defined in the context of a producer application. As an example you can use the OrderEvent.avsc in the EDA quickstart project. They are uploaded to Schema registry, you will demonstrate that in 2 minutes (2) Producer application uses Serializer that get schema ID from the registry (3) Message includes metadata about the schema ID (4) So each message in a topic/partition may have a different schema ID, which help consumer to be able to process old messages (5) Consumers get message definitions from the central schema registry. Schema registry \u00b6 This is really an introduction to the schema management, a deeper demo will take around 35 minutes and is described in this EDA lab Get the ItemEvent schema definition (Defined in the context of the real-time inventory demo) using the command below: curl https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-store-simulator/master/backend/src/main/avro/ItemEvent.avsc > ItemEvent.avsc Warning When running on a multi-tenant Event Streams cluster you need to modify the name of the schema name, to avoid conflicting with other schema name in the registry. In the context of the IBM Tech Academy , we propose you prefix the name with your assigned user-id. Go to the Schema registry in the Event Streams console, and click to Add Schema In the Add schema view, select Upload definition , select the ItemEvent.avsc The first ItemEvent schema is validated, You can see its definition too Do not forget to press Add schema to save your work. Now the schema is visible in the registry Now any future producer application discussions should be around level of control of the exactly once, at most once delivery, failover and back preasure. This is more complex discussion, what is important to say is that we can enforce producer to be sure records are replicated before continuing, we can enforce avoiding record duplication, producer can do message buffering and send in batch, so a lot of controls are available depending of the application needs. Reading more Producer best practices and considerations Using the outbox pattern with Debezium and Quarkus DB2 debezium lab Playing with Avro Schema Event Streams product documentation Consumer application - consumer group \u00b6 Let\u2019s take a look at consumer group and how consumer gets data from Topic/partition. The following figure will help supporting the discussion: Explanations Consumer application define a property to group multiple instances of those application into a group. Topic partitions are only here to support scaling consumer processing Brokers are keeping information about group, offset and partition allocation to consumer When a consumer is unique in a group, it will get data from all partitions. We cannot have more consumer than number of topic, if not the consumer will do nothing Membership in a consumer group is maintained dynamically When the consumer does not send heartbeats for a duration of session.timeout.ms , then it is considered unresponsive and its partitions will be reassigned. For each consumer group, Kafka remembers the committed offset for each partition being consumed. Understanding offset Get more details on consumer best practices Recall that offset is just a numeric identifier of a consumer position of the last record read within a partition. Consumers periodically need to commit the offsets they have received, to present a recovery point in case of failure. For reliable consumers, it means the commitment of the read offset is done by code once the consumer is done with its processing. There is an important limitation within App Connect Kafka consumer node as there is no way to commit by code, so do not propose App Connect as a viable solution if you need to do not loose message. Or support adding new java custom code to do the management of offset. From Event Streams demonstration point of view, we can only demonstrate consumer groups for a given topic, and if consumers are behind in term of reading records from a partition. In the Event Streams console go to the Topics view and Consumer Groups tab of one of the topic. The figure below shows that there is no active member for the consumer groups , and so one partition is not consumed by any application. Another view is in the Consumer Groups which lists all the consumer groups that have been connected to any topic in the cluster: This view helps to assess if consumer are balanced. Selecting one group will zoom into the partition and offset position for member of the group. Offset lag is what could be a concern. The consumer lag for a partition is the difference between the offset of the most recently published message and the consumer's committed offset. Consumer lag may show that consumers are not processing records at the same pace that producer is publishing them. This could not be a problem, until this lag is becoming too high and compaction or retention by time or size will trigger, removing old records. In this case consumers will miss messages. Reading more Review Consumer Product documentation - Consuming messages User management and security \u00b6 There are two types of user management in Event Streams: the human users, to access the user interface and the application users to access Brokers and Kafka Topics. Application users are defined with KafkaUser custom resources. The Yaml also describes access control list to the topic. The following KafkaUser yaml file is an example of application user used to authenticate with mutual TLS. Such user can also being created by using the connect to the cluster option in Event Streams console. The Acces Control Lists are defined by specifying the resource type and the type of operation authorized. User certificates and passwords are saved in secrets. The ACL rules define the operations allowed on Kafka resources based on the username: acls : - resource : type : topic name : 'rt-' patternType : prefix operation : Write - resource : type : topic name : '*' patternType : literal operation : Read - resource : type : topic name : '*' patternType : literal operation : Create For human authentication, users are defined using IBM Cloud Pak foundational services Identity and Access Management (IAM). Things to keep in mind: IAM is in Cloud Pak | Administation console. A URL like: https://cp-console.apps........ibm.com/common-nav/dashboard Need to define a team for resources, administrator users... using he Administration console and IAM menu: Define new team, with connection to an active directory / identity provider: Any groups or users added to an IAM team with the Cluster Administrator or Administrator role can log in to the Event Streams UI and CLI or non admin user: Any groups or users with the Administrator role will not be able to log in until the namespace that contains the Event Streams cluster is added as a resource for the IAM team. If the cluster definition includes spec.strimziOverrides.kafka.authorization: runas , users are mapped to a Kafka principal Read more Managing access - product documentation Managing team with IAM ACL and authorization ACLs rule schema reference Kafka Connect \u00b6 Kafka connect is used to connect external systems to Event Streams brokers. For production deployment the Kafka connect connectors run in cluster, (named distributed mode), to support automatic balancing, dynamic scaling and fault tolerance. In the figure below, we can see Kafka Connect cluster builds with 3 worker processes. The configuration if such worker is done with one file, that can be managed in your GitOps. (An example of such file is here ) Event Streams Operator supports custom resource to define Kafka connect cluster. Each connector is represented by another custom resource called KafkaConnector. When running in distributed mode, Kafka Connect uses three topics to store configuration, current offsets and status. Once the cluster is running, we can use custom resource to manage the connector. For example to get a MQ Source connector definition example, you can browse this yaml which specifies how to connect to the MQ broker and how to create records for Kafka. apiVersion : eventstreams.ibm.com/v1alpha1 kind : KafkaConnector metadata : name : mq-source labels : eventstreams.ibm.com/cluster : eda-kconnect-cluster spec : class : com.ibm.eventstreams.connect.mqsource.MQSourceConnector tasksMax : 1 config : mq.queue.manager : QM1 mq.connection.name.list : store-mq-ibm-mq.rt-inventory-dev.svc mq.channel.name : DEV.APP.SVRCONN mq.queue : ITEMS mq.bath.size : 250 producer.override.acks : 1 topic : items key.converter : org.apache.kafka.connect.storage.StringConverter value.converter : org.apache.kafka.connect.storage.StringConverter mq.record.builder : com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder mq.connection.mode : client mq.message.body.jms : true mq.record.builder.key.header : JMSCorrelationID to improve connector source throughput we can control the producer properties like the acknowledge level expected. The real time inventory demo includes MQ source connector. Read more Event Streams documentation - kafka connect Kafka Connect technology deeper dive Monitoring \u00b6 The IBM Event Streams UI provides information about the health of your environment at a glance. In the bottom right corner of the UI, a message shows a summary status of the system health. Using the JMX exporter, you can collect JMX metrics from Kafka brokers, ZooKeeper nodes, and Kafka Connect nodes, and export them to Prometheus Warning Be aware IBM Cloud Pak foundational services 3.8 and later does not include Prometheus so you will get Event Streams metrics not available error message. On Biggs as of 04/19/22, the cluster configuration was done. If you need to do it on your cluster see those two files: cluster-monitoring-cm.yaml to enable user workload monitoring with Prometheus and pod-monitors.yaml to declare the PodMonitor to define scrapeable endpoints of a Kubernetes Pod serving Prometheus metrics. Assess Event Streams cluster state: Go to the project where the cluster runs, select one of the Kafka Pod. You can see the pod via the OpenShift workloads menu, or by using the Event Streams Operator > Resources and then filter on pods: Select one of the pods and go to the metrics to see memory, CPU, network and filesystem usage metrics. Access the Cloud Pak | Administration console to select Monitoring Switch organization to select where Event Streams is running Then go to the grafana Dashboard menu on the left > Manage and select event streams dashboard In the Grafana dashboard select the namespace for event streams (e.g. cp4i-eventstreams ), the cluster name ( es-demo ), the brokers, and the topic to monitor. More reading Product documentation EDA monitoring study Event Streams Monitoring on OpenShift lab Creating alert from Prometheus Event Streaming \u00b6 Kafka Streams \u00b6 Kafka Streams is client API to build microservices with input and output data are in Kafka. It is based on programming a graph of processing nodes to support the business logic developer wants to apply on the event streams. Apache Flink as your streaming platform \u00b6 To be done. Real-time inventory demo \u00b6 It is possible to propose a more complex solution to illustrate modern data pipeline using components like MQ source Kafka Connector, Kafka Streams implementation and Cloud Object Storage sink, Elastic Search and Pinot. This scenario implements a simple real-time inventory management solution based on some real life MVPs we developed in 2020. For a full explanation of the use case and scenario demo go to this chapter in EDA reference architecture. The solution can be deployed using few commands or using GitOps. See Lab3-4 More Reading Description of the scenario and demo script GitOps project to deploy the solution EDA GitOps Catalog to deploy Cloud Pak for Integration operators Geo-replication \u00b6 We will go over two main concepts: replication to a passive and active Event Streams cluster. Geo Replication is the IBM packaging of Mirror Maker 2. Demonstrating Geo Replication \u00b6 The geo-replication feature creates copies of your selected topics to help with disaster recovery. Mirror Maker 2 \u00b6 Mirror Maker 2 is a Kafka Connect framework to replicate data between different Kafka Cluster, so it can be used between Event Streams clusters, but also between Confluent to/from Event Streams, Kafka to/from Event Streams... The following diagram can be used to present the MM2 topology Active - Passive \u00b6 See a demonstration for the real-time inventory and replication in this article Active - Active \u00b6 Read more Geo Replication - Product documentation EDA techno overview for Mirror Maker 2 Demonstration in the context of real-time inventory EDA lab on mirror maker 2 Day 2 operations \u00b6 In this section, you should be able to demonstrate some of the recurring activities, operation team may perform for the Event Streams and OpenShift platform for maintenance: Change Cluster configuration Add topic or change topic configuration like adding partition GitOps \u00b6 The core idea of GitOps is having a Git repository that always contains declarative descriptions of the infrastructure currently desired in the production environment and an automated process to make the production environment matches the described state in the repository. Git is the source of truth for both application code, application configuration, dependant service/product deployments, infrastructure config and deployment. In the following figure, we just present the major components that will be used to support GitOps and day 2 operations: Explanations cluster configuration, topics, users ACL are defined as yaml resources in the solution GitOps. Cluster example for prod (1) Operator definitions are also defined in the gitops and then change to the version subscription will help do product upgrade. Event Streams subscription with overlays for a new version. (2) ArgoCD apps are defined in the GitOps and then once running on the GitOps Server, will monitor changes to the source gitOps content (3) when change occurs, the underlying kubernetes resources are modified (4) Operator maintains control to the runtime pods according to the modified manifest Cloud Pak for integration, event streams, MQ, API Connect operators help to support deployment as a Day 1 operation, but also support maintenance or Day 2 operations. Operator is constantly watching your cluster\u2019s desired state for the software installed and act on them. Using a GitOps approach, we can design a high-level architecture view for the deployment of all the event-driven solution components: as in previous figure, operators, ArgoCD apps, cluster, topics... are defined in the solution gitops and then the apps deployment, config map, service, secrets, routes are also defined according to the expected deployment model. In the figure above, the dev, and staging projects have their own Event Streams clusters. Production is in a separate OpenShift Cluster and event streams cluster is multi-tenant. We are using a special Git repository to manage a catalog of operator definitions/ subscriptions. This is the goal of the eda-gitops-catalog repository . A solution will have a specific gitops repository that manages services (operands) and application specifics deployment manifests. Start the GitOps demo \u00b6 Warning In the context of the Tech academy , if you want to use Gitops you should use the lab 4 exercise as it is a little bit simpler than to execute next section. To be able to do continuous deployment we need to have some ArgoCD apps deployed on GitOps server. In all gitOps demos, we assume you have a fork of the eda-rt-inventory-gitops . If you are not using a cluster with Event Streams already installed in the cp4i-eventstreams , you may need to modify the Copy Secret job () so it can get the ibm-entitlement-key from the good namespace. If not done yet, jumpstart GitOps oc apply -k demo/argocd 1. Access to the ArgoCD console chrome https:// $( oc get route openshift-gitops-server -o jsonpath = '{.status.ingress[].host}' -n openshift-gitops ) 1. User is admin and password is the result of oc extract secret/openshift-gitops-cluster -n openshift-gitops --to = - You should have two apps running in the default scope/ project. The argocd apps are monitoring the content of the demo/ env folder and once deployed, you should have a simple Event Streams node with one zookeeper under the project es-demo-day2 . Event Streams cluster definition with GitOps \u00b6 The goal of this section is to demonstrate how to define an Event Stream cluster with configuration and change the number of replicas. This is a very simple use case to try to use the minimum resources. So the basic cluster definition use 1 broker and 1 zookeeper. The file is es-demo.yaml . it is using Event Streams version 10.5.0 and one replicas In GitOps console, if you go to the demo-env app, you will see there is one Kafka broker and also a lot of Kubernetes resources defined In the es-demo-day2 project, use oc get pods to demonstrate the number of brokers NAME READY STATUS RESTARTS AGE cpsecret-48k2c 0 /1 Completed 0 11m demo-entity-operator-558c94dc57-lxp6s 3 /3 Running 0 24m demo-ibm-es-admapi-6678c47b95-hg82v 1 /1 Running 0 14m demo-ibm-es-metrics-b974c7585-jpfc7 1 /1 Running 0 14m demo-kafka-0 1 /1 Running 0 24m demo-zookeeper-0 1 /1 Running 0 25m Modify the number of replicas kafka : replicas : 2 Commit and push your changes to your git repository and see ArgoCD changing the configuration, new pods should be added. You can enforce a refresh to get update from Git and then navigate the resources to see the new brokers added (demo-kafka-1): Adding a broker will generate reallocation for topic replicas. Event Streams Cluster upgrade \u00b6 This will be difficult to demonstrate but the flow can be explain using the OpenShift Console. First you need to be sure the cloud pak for integration services are upgraded. (See this note ) Two things to upgrade in this order: Event Streams operator and then the cluster instances. You can upgrade the Event Streams operator to version 2.5.2 directly from version 2.5.x, 2.4.x, 2.3.x, and 2.2.x.. You can upgrade the Event Streams operand to version 11.0.0 directly from version 10.5.0, 10.4.x Start by presenting the version of an existing running Cluster definition May be show some messages in a topic, for example the Store Simulator may have sent messages to the items topic Go to the openshift-operators and select the event streams operator, explain the existing chaneel then change the channel number Event Streams instance must have more than one ZooKeeper node or have persistent storage enabled. Upgrade operator is by changing the channel in the operator subscription. All Event Streams pods that need to be updated as part of the upgrade will be gracefully rolled. Where required ZooKeeper pods will roll one at a time, followed by Kafka brokers rolling one at a time. Update the cluster definition version to new version (10.5.0 in below screen shot), thne this will trigger zookeeper and kafka broker update. Topic management with GitOps \u00b6 The goal of this section is to demonstrate how to change topic definition using Argocd and git. Modify the file es-topic.yaml by adding a new topic inside this file with the following declaration: apiVersion : eventstreams.ibm.com/v1beta1 kind : KafkaTopic metadata : name : demo-2-topic labels : eventstreams.ibm.com/cluster : demo spec : partitions : 1 replicas : 1 Commit and push your changes to your git repository and see ArgoCD changing the configuration, new pods should be added. Do oc get kafkatopics or go to Event Streams operator in the es-demo-day2 project to see all the Event Streams component instances. Repartitioning \u00b6 You can demonstrate how to change the number of partition for an existing topic ( rt-items ) from 3 to 5 partitions: oc apply -f environments/rt-inventory-dev/services/ibm-eventstreams/base/update-es-topic.yaml Add more instances on the consumer part: taking the store-aggregator app and add more pods from the deployment view in OpenShift console, or change the deployment.yaml descriptor and push the change to the git repository so GitOps will catch and change the configuration: # modify https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/blob/main/environments/rt-inventory-dev/apps/store-inventory/services/store-inventory/base/config/deployment.yaml oc apply -f environments/rt-inventory-dev/apps/store-inventory/services/store-inventory/base/config/deployment.yaml Read more Event Streams doc on Kafka tools mapping to cli Clean your gitops \u00b6 Remove the ArgoCD apps oc delete -k demo/argocd Remove resources oc delete -k demo/env Read more Event driven solution with GitOps EDA GitOps Catalog Real time inventory demo gitops OpenShift Cluster version upgrade \u00b6 There may be some questions around how to migrate a version for OCP. Principles \u00b6 For clusters with internet accessibility, Red Hat provides over-the-air updates through an OpenShift Container Platform update service as a hosted service located behind public APIs. Due to fundamental Kubernetes design, all OpenShift Container Platform updates between minor versions must be serialized. What can be demonstrated \u00b6 At the demo level, you can go to the Administration console in Administration > Cluster Settings you get something like this: If you want to upgrade version within the same release Or upgrade release change the Channel version: As the operation will take sometime, it is not really demonstrable easily. Read more Openshift understanding upgrade channels release Canary rollout","title":"Demonstration A to Z"},{"location":"demo/#demonstrating-event-streams-from-a-to-z","text":"Warning This exercise is not a step by step lab, but more an explanation of all the concepts and components involved in an event-driven solution with Event Streams. We have provided scripts that can be leveraged (see the table of content on the right to get scripts) to demonstrate and talk about those items in front of your prospect. A typical demonstration script will include at least the following subjects (See right navigation bar to go to a specific sections): Review Event Streams Components Operator based deployment and Day 2 operations Topic creation Producer application Consumer application, consumer group concepts, offset concepts User access, authentication mechanism Monitoring Event Streaming Geo-replication As education enablement you can go step by step following the current structure. As a reusable asset for your future demonstration, you can pick and choose from the right navigation bar the items to highlight in front of your audiance. All the demonstration can be done on IBM CoC clusters: see the environments section in the EDA labs introduction.","title":"Demonstrating Event Streams from A to Z"},{"location":"demo/#pre-requisites","text":"You will need access to an Event Streams instance installed on an OpenShift cluster with access to the OpenShift Console to demonstrate Operators. You\u2019ll need the following as well: git client Have oc cli installed. It can be done once connected to the OpenShift cluster using the <?> icon on the top-right and \"Command Line Tool\" menu. Get docker desktop or podman on your local laptop Java 11 is need to run the Event Streams starter application .","title":"Pre-requisites"},{"location":"demo/#review-event-streams-components","text":"Narative : Event Streams is the IBM packaging of different Open Source projects to support an integrated user experience deploying and managing Kafka on OpenShift cluster. The following figure illustrates such components: src for this diagram is here Event streams ( Apache Kafka packaging) runs on OpenShift cluster. The deployment and the continuous monitoring of Event Streams resources definition and deployed resources is done via Operator ( Strimzi open source project ) Event Streams offers a user interface to manage resources and exposes simple dashboard. We will use it during the demonstration. The schema management is done via schema registry and the feature is integrated in Event Streams user interface but in the back end, is supported by Apicur.io registry External event sources can be integrated via the Kafka Connector framework and Event Streams offers a set of connectors and can partner to other companies to get specific connectors. External sinks can be used to persist messages for longer time period that the retention settings done at the topic level. S3 buckets can be use, IBM Cloud object storage, and Kafka Sink connectors. There is this cloud object storage lab , or S3 sink with Apache Camel lab to present such integrations. Event Streams monitoring is done using Dashboards in Event Streams user interface but also within OpenShift monitoring and Kibana dashboards. Green components are application specifics, and represent event-driven microservices (see eda-quickstart project for code templates ) or Kafka Streaming apps, or Apache Flink apps. For cluster optimization, Event Streams integrates Cruise Control, with goal constraints, to act on cluster resource usage. More argumentations Kafka is essentially a distributed platform to manage append log with a pub/sub protocol to get streams of events. Messages are saved for a long period of time. Kafka connectors can also being supported by APP Connect integration capabilities or Apache Camel kafka connectors . To learn more about Kafka Connector see our summary","title":"Review Event Streams components"},{"location":"demo/#concepts","text":"If needed there are some important concepts around Kafka to present to your audience. See this kafka technology overview .","title":"Concepts"},{"location":"demo/#high-availability","text":"High availability is ensured by avoiding single point of failure, parallel, and replications. The following figure is a golden topology for OpenShift with Event Streams components deployed to it. Event Streams Brokers run in OpenShift worker nodes, and it may be relevant to use one broker per worker nodes using zone affinity policies. src for this diagram is here Kafka connectors, or streaming applications runs in worker node too and access brokers via mutual TLS authentication and SSL encryption. Kafka brokers are spread across worker nodes using anti-affinity policies. Read more Kafka High availability deeper dive See the OpenShift golden topology article in production deployment site. A production deployment descriptor for Event Streams Product documentation on planning installation","title":"High Availability"},{"location":"demo/#operator-based-deployment","text":"There are several ways to install Event Streams. We are going to look at this, with Operator Hub. Go to your Openshift console, select Operator Hub and search for Event Streams. Here you can install the operator to manage all cluster instances deployed to the OpenShift environment. Operator can automatically deploy new product version once released by IBM. In the OpenShift Console, select the project where Event Streams is deployed. On left menu select Operators > Installed Operators , scroll to select IBM Event Streams, you are now in the Operator user interface, from where you can see local resources and create new one. Go to the Event Streams menu and select existing cluster definition You are now viewing the cluster definition as it is deployed. Select the YAML choice and see the spec elements. You can see how easy it would be simple to add a broker by changing the spec.strimziOverrides.kafka.replicas value. Also in this view, the Samples menu presents some examples of cluster definitions. Kafka brokers, Zookeeper nodes or other components like Apicurio can all be scaled to meet your needs: Number of replicas CPU request or limit settings Memory request or limit settings JVM settings On the left side menu select Workloads->Pods. Here you see pods that are in the Event Streams namespace like Broker, zookeepers, user interface, schema registry: If needed, you can explain the concept of persistence and Storage class: Kafka save records on disk for each broker, and so it can use VM disk or network file systems. As Kubernetes deployed application, Event Streams define persistence via persistence claim and expected quality of service using storage class. On the left side menu select, Storage > PersistenceVolumesClaims in the OpenShift console, each broker has its own claim, OpenShift allocated Persistence Volumes with expected capacity. The Storage class was defined by OpenShift administrator, and in the example above, it use CEPH storage. Read more Cepth and block devise Kafka Brokers and architecture GitOps approach for Day1 and Day 2 operations","title":"Operator based deployment"},{"location":"demo/#review-event-streams-user-interface-features","text":"There are a number of ways to navigate to Event Streams Console by getting the exposed routes Using Routes in Openshift: On the left side menu select Networking > Routes in the OpenShift console. Find es-demo-ibm-es-ui and then go to Location. Select that link and it will take you to the Event Streams Console. Depending to your installation, you may reach Cloud Pak for Integration console, in this case, select Entreprise LDAP, and enter your userid and password. Using the cli: (replace es-demo with the name of your cluster, and cp4i-eventstreams with the name of the project where Event Streams runs into ) chrome $( oc get eventstreams es-demo -n cp4i-eventstreams -o jsonpath = '{.status.adminUiUrl}' ) Once you logged in using the LDAP credentials provided, you should reach the home page. The set of features available from this home page, are topic management, schema registry, consumer groups, monitoring, and toolbox ... you will review most of those features in this demo.","title":"Review Event Streams user interface features"},{"location":"demo/#topic-management","text":"Topics are append log, producer applications publish records to topics, and consumer applications subscribe to topics. Kafka messages themselves are immutable. Deletion and compaction of data are administrative operations. Navigate to the topic main page by using the Event Streams left side menu and select Topics. replicas are to support record replication and to ensure high availability. Producer can wait to get acknowledgement of replication. Replicas needs to be set to 3 to supports 2 broker failures at the same time. partition defines the number of append logs managed by the broker. Each partition has a leader, and then follower brokers that replicate records from the leader. Partitions are really done to do parallel processing at the consumer level. The following diagram can be used to explain those concepts. Create a topic for the Starter app, using the user interface: Warning When running on a multi-tenant Event Streams cluster you need to modify the name of the topic, to avoid conflicting with other topic name, use your userid as prefix. Use only one partition. The default retention time is 7 days, Kafka is keeping data for a long time period, so any consumer applications can come and process messages at any time. It helps for microservice resilience and increase decoupling. Finally the replicas for high availability. 3 is the production deployment, and in-sync replicas = 2, means producer get full acknowledge when there are 2 replicas done. Broker partition leader keeps information of in-sync replicas. Just as an important note, topic may be created via yaml file or using CLI command. Go to the rt-inventory GitOps - es-topics.yaml and explain some of the parameters. We will go over the process of adding new topic by using GitOps in this section Read more Topic summary Kafka topic configuration Understand Kafka producer Review Consumer Replication and partition leadership","title":"Topic management"},{"location":"demo/#run-the-starter-application","text":"See the beginned dedicated lab to get the application started, once done: Go back to the Event Streams console, Topic management, and the starter-app topic, select the Messages tab and go to any messages. Explain that each messages has a timestamp, and an offset that is an increasing number. Offset are used by consumer to be able to replay from an older message, or when restarting from a failure. Offset management at the consumer application level is tricky, if needed you can have a deeper conversation on this topic later after the demonstration. At the topic level, it is possible to see the consumer of the topic: Go to the Consumer groups tab, to see who are the consumer, if the consumer is active or not (this will be controlled by the heartbeat exchanges when consumer poll records and commit their read offset). One important metric in this table is the unconsumed partition value. If the number of partitions is more than 1 and there are less than the number of consumer than of partition, then it means a consumer is processing two or more partitions. Going by to the starter application, you can start consuming the records. This is to demonstrate that consumer can connect at anytime, and that it will quickly consume all messages. Stopping and restarting is also demonstrating that consumer, continues from the last read offset. There is an alternate of running this application on your laptop, it can be deployed directly to the same OpenShift cluster, we have defined deployment and config map to do so. Deploy starter app on OpenShift Use the same kafka.properties and truststore.p12 files you have downloaded with the starter application to create two kubernetes secrets holding these files in your OpenShift cluster oc create secret generic demo-app-secret --from-file = ./kafka.properties oc create secret generic truststore-cert --from-file = ./truststore.p12 Clone the following GitHub repo that contains the Kubernetes artifacts that will run the starter application. git clone https://github.com/ibm-cloud-architecture/eda-quickstarts.git Change directory to where those Kubernetes artefacts are. cd eda-quickstarts/kafka-java-vertz-starter Deploy the Kubernetes artefacts. oc apply -k app-deployment Get the route to the starter application running on your OpenShift cluster. oc get route es-demo -o = jsonpath = '{.status.ingress[].host}' Point your browser to that url to work with the IBM Event Streams Starter Application.","title":"Run the Starter Application"},{"location":"demo/#back-to-the-cluster-configuration","text":"Event Streams cluster can be configured with Yaml and you can review the following cluster definition to explain some of the major properties: EDA GitOps Catalog - example of production cluster.yaml : Property Description Replicas specify the # of brokers or zookeeper Resources CPU or mem requested and limit Listeners Define how to access the cluster: External with scram authentication and TLS encryption, and internal using TLS authentication or PLAIN. Entity operators Enable topic and user to be managed by operator Rack awareness To use zone attribute from node to allocate brokers in different AZ Cruise Control Open source for cluster rebalancing Metrics To export different Kafka metrics to Prometheus via JMX exporter For Kafka, the following aspects of a deployment can impact the resources you need: Throughput and size of messages The number of network threads handling messages The number of producers and consumers The number of topics and partitions","title":"Back to the Cluster configuration"},{"location":"demo/#producing-messages","text":"The product documentation - producing message section goes into details of the concepts. For a demonstration purpose, you need to illustrate that you can have multiple types of Kafka producer: Existing Queuing apps, which are using IBM MQ, and get their messages transparently sent to Event Streams, using IBM MQ Streaming Queue and MQ Source Kafka Connector . Microservice applications publishing events using Kafka producer API, or reactive messaging in Java Microprofile. For Nodejs, Python there is a C library which supports the Kafka APIs. We have code template for that. Change data capture product, like Debezium , that gets database updates and maps records to events in topic. One topic per table. Some data transformation can be done on the fly. Streaming applications, that do stateful computing, real-time analytics, consuming - processing - publishing events from one to many topics and produce to one topic. App connect flow can also being source for events to Events Streams, via connectors. The following diagram illustrates those event producers. Each producer needs to get a URL to the broker, defines the protocol to authenticate, and gets server side TLS certificate, the topic name, and that's it to start sending messages. For production deployment, event structures are well defined and schema are used to ensure consumer can understand how to read messages from the topic/partition. Event Streams offers a schema registry to manage those schema definitions. You can introduce the schema processing with the figure below: Schema flow explanations (1) Avro or Json schemas are defined in the context of a producer application. As an example you can use the OrderEvent.avsc in the EDA quickstart project. They are uploaded to Schema registry, you will demonstrate that in 2 minutes (2) Producer application uses Serializer that get schema ID from the registry (3) Message includes metadata about the schema ID (4) So each message in a topic/partition may have a different schema ID, which help consumer to be able to process old messages (5) Consumers get message definitions from the central schema registry.","title":"Producing messages"},{"location":"demo/#schema-registry","text":"This is really an introduction to the schema management, a deeper demo will take around 35 minutes and is described in this EDA lab Get the ItemEvent schema definition (Defined in the context of the real-time inventory demo) using the command below: curl https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-store-simulator/master/backend/src/main/avro/ItemEvent.avsc > ItemEvent.avsc Warning When running on a multi-tenant Event Streams cluster you need to modify the name of the schema name, to avoid conflicting with other schema name in the registry. In the context of the IBM Tech Academy , we propose you prefix the name with your assigned user-id. Go to the Schema registry in the Event Streams console, and click to Add Schema In the Add schema view, select Upload definition , select the ItemEvent.avsc The first ItemEvent schema is validated, You can see its definition too Do not forget to press Add schema to save your work. Now the schema is visible in the registry Now any future producer application discussions should be around level of control of the exactly once, at most once delivery, failover and back preasure. This is more complex discussion, what is important to say is that we can enforce producer to be sure records are replicated before continuing, we can enforce avoiding record duplication, producer can do message buffering and send in batch, so a lot of controls are available depending of the application needs. Reading more Producer best practices and considerations Using the outbox pattern with Debezium and Quarkus DB2 debezium lab Playing with Avro Schema Event Streams product documentation","title":"Schema registry"},{"location":"demo/#consumer-application-consumer-group","text":"Let\u2019s take a look at consumer group and how consumer gets data from Topic/partition. The following figure will help supporting the discussion: Explanations Consumer application define a property to group multiple instances of those application into a group. Topic partitions are only here to support scaling consumer processing Brokers are keeping information about group, offset and partition allocation to consumer When a consumer is unique in a group, it will get data from all partitions. We cannot have more consumer than number of topic, if not the consumer will do nothing Membership in a consumer group is maintained dynamically When the consumer does not send heartbeats for a duration of session.timeout.ms , then it is considered unresponsive and its partitions will be reassigned. For each consumer group, Kafka remembers the committed offset for each partition being consumed. Understanding offset Get more details on consumer best practices Recall that offset is just a numeric identifier of a consumer position of the last record read within a partition. Consumers periodically need to commit the offsets they have received, to present a recovery point in case of failure. For reliable consumers, it means the commitment of the read offset is done by code once the consumer is done with its processing. There is an important limitation within App Connect Kafka consumer node as there is no way to commit by code, so do not propose App Connect as a viable solution if you need to do not loose message. Or support adding new java custom code to do the management of offset. From Event Streams demonstration point of view, we can only demonstrate consumer groups for a given topic, and if consumers are behind in term of reading records from a partition. In the Event Streams console go to the Topics view and Consumer Groups tab of one of the topic. The figure below shows that there is no active member for the consumer groups , and so one partition is not consumed by any application. Another view is in the Consumer Groups which lists all the consumer groups that have been connected to any topic in the cluster: This view helps to assess if consumer are balanced. Selecting one group will zoom into the partition and offset position for member of the group. Offset lag is what could be a concern. The consumer lag for a partition is the difference between the offset of the most recently published message and the consumer's committed offset. Consumer lag may show that consumers are not processing records at the same pace that producer is publishing them. This could not be a problem, until this lag is becoming too high and compaction or retention by time or size will trigger, removing old records. In this case consumers will miss messages. Reading more Review Consumer Product documentation - Consuming messages","title":"Consumer application - consumer group"},{"location":"demo/#user-management-and-security","text":"There are two types of user management in Event Streams: the human users, to access the user interface and the application users to access Brokers and Kafka Topics. Application users are defined with KafkaUser custom resources. The Yaml also describes access control list to the topic. The following KafkaUser yaml file is an example of application user used to authenticate with mutual TLS. Such user can also being created by using the connect to the cluster option in Event Streams console. The Acces Control Lists are defined by specifying the resource type and the type of operation authorized. User certificates and passwords are saved in secrets. The ACL rules define the operations allowed on Kafka resources based on the username: acls : - resource : type : topic name : 'rt-' patternType : prefix operation : Write - resource : type : topic name : '*' patternType : literal operation : Read - resource : type : topic name : '*' patternType : literal operation : Create For human authentication, users are defined using IBM Cloud Pak foundational services Identity and Access Management (IAM). Things to keep in mind: IAM is in Cloud Pak | Administation console. A URL like: https://cp-console.apps........ibm.com/common-nav/dashboard Need to define a team for resources, administrator users... using he Administration console and IAM menu: Define new team, with connection to an active directory / identity provider: Any groups or users added to an IAM team with the Cluster Administrator or Administrator role can log in to the Event Streams UI and CLI or non admin user: Any groups or users with the Administrator role will not be able to log in until the namespace that contains the Event Streams cluster is added as a resource for the IAM team. If the cluster definition includes spec.strimziOverrides.kafka.authorization: runas , users are mapped to a Kafka principal Read more Managing access - product documentation Managing team with IAM ACL and authorization ACLs rule schema reference","title":"User management and security"},{"location":"demo/#kafka-connect","text":"Kafka connect is used to connect external systems to Event Streams brokers. For production deployment the Kafka connect connectors run in cluster, (named distributed mode), to support automatic balancing, dynamic scaling and fault tolerance. In the figure below, we can see Kafka Connect cluster builds with 3 worker processes. The configuration if such worker is done with one file, that can be managed in your GitOps. (An example of such file is here ) Event Streams Operator supports custom resource to define Kafka connect cluster. Each connector is represented by another custom resource called KafkaConnector. When running in distributed mode, Kafka Connect uses three topics to store configuration, current offsets and status. Once the cluster is running, we can use custom resource to manage the connector. For example to get a MQ Source connector definition example, you can browse this yaml which specifies how to connect to the MQ broker and how to create records for Kafka. apiVersion : eventstreams.ibm.com/v1alpha1 kind : KafkaConnector metadata : name : mq-source labels : eventstreams.ibm.com/cluster : eda-kconnect-cluster spec : class : com.ibm.eventstreams.connect.mqsource.MQSourceConnector tasksMax : 1 config : mq.queue.manager : QM1 mq.connection.name.list : store-mq-ibm-mq.rt-inventory-dev.svc mq.channel.name : DEV.APP.SVRCONN mq.queue : ITEMS mq.bath.size : 250 producer.override.acks : 1 topic : items key.converter : org.apache.kafka.connect.storage.StringConverter value.converter : org.apache.kafka.connect.storage.StringConverter mq.record.builder : com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder mq.connection.mode : client mq.message.body.jms : true mq.record.builder.key.header : JMSCorrelationID to improve connector source throughput we can control the producer properties like the acknowledge level expected. The real time inventory demo includes MQ source connector. Read more Event Streams documentation - kafka connect Kafka Connect technology deeper dive","title":"Kafka Connect"},{"location":"demo/#monitoring","text":"The IBM Event Streams UI provides information about the health of your environment at a glance. In the bottom right corner of the UI, a message shows a summary status of the system health. Using the JMX exporter, you can collect JMX metrics from Kafka brokers, ZooKeeper nodes, and Kafka Connect nodes, and export them to Prometheus Warning Be aware IBM Cloud Pak foundational services 3.8 and later does not include Prometheus so you will get Event Streams metrics not available error message. On Biggs as of 04/19/22, the cluster configuration was done. If you need to do it on your cluster see those two files: cluster-monitoring-cm.yaml to enable user workload monitoring with Prometheus and pod-monitors.yaml to declare the PodMonitor to define scrapeable endpoints of a Kubernetes Pod serving Prometheus metrics. Assess Event Streams cluster state: Go to the project where the cluster runs, select one of the Kafka Pod. You can see the pod via the OpenShift workloads menu, or by using the Event Streams Operator > Resources and then filter on pods: Select one of the pods and go to the metrics to see memory, CPU, network and filesystem usage metrics. Access the Cloud Pak | Administration console to select Monitoring Switch organization to select where Event Streams is running Then go to the grafana Dashboard menu on the left > Manage and select event streams dashboard In the Grafana dashboard select the namespace for event streams (e.g. cp4i-eventstreams ), the cluster name ( es-demo ), the brokers, and the topic to monitor. More reading Product documentation EDA monitoring study Event Streams Monitoring on OpenShift lab Creating alert from Prometheus","title":"Monitoring"},{"location":"demo/#event-streaming","text":"","title":"Event Streaming"},{"location":"demo/#kafka-streams","text":"Kafka Streams is client API to build microservices with input and output data are in Kafka. It is based on programming a graph of processing nodes to support the business logic developer wants to apply on the event streams.","title":"Kafka Streams"},{"location":"demo/#apache-flink-as-your-streaming-platform","text":"To be done.","title":"Apache Flink as your streaming platform"},{"location":"demo/#real-time-inventory-demo","text":"It is possible to propose a more complex solution to illustrate modern data pipeline using components like MQ source Kafka Connector, Kafka Streams implementation and Cloud Object Storage sink, Elastic Search and Pinot. This scenario implements a simple real-time inventory management solution based on some real life MVPs we developed in 2020. For a full explanation of the use case and scenario demo go to this chapter in EDA reference architecture. The solution can be deployed using few commands or using GitOps. See Lab3-4 More Reading Description of the scenario and demo script GitOps project to deploy the solution EDA GitOps Catalog to deploy Cloud Pak for Integration operators","title":"Real-time inventory demo"},{"location":"demo/#geo-replication","text":"We will go over two main concepts: replication to a passive and active Event Streams cluster. Geo Replication is the IBM packaging of Mirror Maker 2.","title":"Geo-replication"},{"location":"demo/#demonstrating-geo-replication","text":"The geo-replication feature creates copies of your selected topics to help with disaster recovery.","title":"Demonstrating Geo Replication"},{"location":"demo/#mirror-maker-2","text":"Mirror Maker 2 is a Kafka Connect framework to replicate data between different Kafka Cluster, so it can be used between Event Streams clusters, but also between Confluent to/from Event Streams, Kafka to/from Event Streams... The following diagram can be used to present the MM2 topology","title":"Mirror Maker 2"},{"location":"demo/#active-passive","text":"See a demonstration for the real-time inventory and replication in this article","title":"Active - Passive"},{"location":"demo/#active-active","text":"Read more Geo Replication - Product documentation EDA techno overview for Mirror Maker 2 Demonstration in the context of real-time inventory EDA lab on mirror maker 2","title":"Active - Active"},{"location":"demo/#day-2-operations","text":"In this section, you should be able to demonstrate some of the recurring activities, operation team may perform for the Event Streams and OpenShift platform for maintenance: Change Cluster configuration Add topic or change topic configuration like adding partition","title":"Day 2 operations"},{"location":"demo/#gitops","text":"The core idea of GitOps is having a Git repository that always contains declarative descriptions of the infrastructure currently desired in the production environment and an automated process to make the production environment matches the described state in the repository. Git is the source of truth for both application code, application configuration, dependant service/product deployments, infrastructure config and deployment. In the following figure, we just present the major components that will be used to support GitOps and day 2 operations: Explanations cluster configuration, topics, users ACL are defined as yaml resources in the solution GitOps. Cluster example for prod (1) Operator definitions are also defined in the gitops and then change to the version subscription will help do product upgrade. Event Streams subscription with overlays for a new version. (2) ArgoCD apps are defined in the GitOps and then once running on the GitOps Server, will monitor changes to the source gitOps content (3) when change occurs, the underlying kubernetes resources are modified (4) Operator maintains control to the runtime pods according to the modified manifest Cloud Pak for integration, event streams, MQ, API Connect operators help to support deployment as a Day 1 operation, but also support maintenance or Day 2 operations. Operator is constantly watching your cluster\u2019s desired state for the software installed and act on them. Using a GitOps approach, we can design a high-level architecture view for the deployment of all the event-driven solution components: as in previous figure, operators, ArgoCD apps, cluster, topics... are defined in the solution gitops and then the apps deployment, config map, service, secrets, routes are also defined according to the expected deployment model. In the figure above, the dev, and staging projects have their own Event Streams clusters. Production is in a separate OpenShift Cluster and event streams cluster is multi-tenant. We are using a special Git repository to manage a catalog of operator definitions/ subscriptions. This is the goal of the eda-gitops-catalog repository . A solution will have a specific gitops repository that manages services (operands) and application specifics deployment manifests.","title":"GitOps"},{"location":"demo/#start-the-gitops-demo","text":"Warning In the context of the Tech academy , if you want to use Gitops you should use the lab 4 exercise as it is a little bit simpler than to execute next section. To be able to do continuous deployment we need to have some ArgoCD apps deployed on GitOps server. In all gitOps demos, we assume you have a fork of the eda-rt-inventory-gitops . If you are not using a cluster with Event Streams already installed in the cp4i-eventstreams , you may need to modify the Copy Secret job () so it can get the ibm-entitlement-key from the good namespace. If not done yet, jumpstart GitOps oc apply -k demo/argocd 1. Access to the ArgoCD console chrome https:// $( oc get route openshift-gitops-server -o jsonpath = '{.status.ingress[].host}' -n openshift-gitops ) 1. User is admin and password is the result of oc extract secret/openshift-gitops-cluster -n openshift-gitops --to = - You should have two apps running in the default scope/ project. The argocd apps are monitoring the content of the demo/ env folder and once deployed, you should have a simple Event Streams node with one zookeeper under the project es-demo-day2 .","title":"Start the GitOps demo"},{"location":"demo/#event-streams-cluster-definition-with-gitops","text":"The goal of this section is to demonstrate how to define an Event Stream cluster with configuration and change the number of replicas. This is a very simple use case to try to use the minimum resources. So the basic cluster definition use 1 broker and 1 zookeeper. The file is es-demo.yaml . it is using Event Streams version 10.5.0 and one replicas In GitOps console, if you go to the demo-env app, you will see there is one Kafka broker and also a lot of Kubernetes resources defined In the es-demo-day2 project, use oc get pods to demonstrate the number of brokers NAME READY STATUS RESTARTS AGE cpsecret-48k2c 0 /1 Completed 0 11m demo-entity-operator-558c94dc57-lxp6s 3 /3 Running 0 24m demo-ibm-es-admapi-6678c47b95-hg82v 1 /1 Running 0 14m demo-ibm-es-metrics-b974c7585-jpfc7 1 /1 Running 0 14m demo-kafka-0 1 /1 Running 0 24m demo-zookeeper-0 1 /1 Running 0 25m Modify the number of replicas kafka : replicas : 2 Commit and push your changes to your git repository and see ArgoCD changing the configuration, new pods should be added. You can enforce a refresh to get update from Git and then navigate the resources to see the new brokers added (demo-kafka-1): Adding a broker will generate reallocation for topic replicas.","title":"Event Streams cluster definition with GitOps"},{"location":"demo/#event-streams-cluster-upgrade","text":"This will be difficult to demonstrate but the flow can be explain using the OpenShift Console. First you need to be sure the cloud pak for integration services are upgraded. (See this note ) Two things to upgrade in this order: Event Streams operator and then the cluster instances. You can upgrade the Event Streams operator to version 2.5.2 directly from version 2.5.x, 2.4.x, 2.3.x, and 2.2.x.. You can upgrade the Event Streams operand to version 11.0.0 directly from version 10.5.0, 10.4.x Start by presenting the version of an existing running Cluster definition May be show some messages in a topic, for example the Store Simulator may have sent messages to the items topic Go to the openshift-operators and select the event streams operator, explain the existing chaneel then change the channel number Event Streams instance must have more than one ZooKeeper node or have persistent storage enabled. Upgrade operator is by changing the channel in the operator subscription. All Event Streams pods that need to be updated as part of the upgrade will be gracefully rolled. Where required ZooKeeper pods will roll one at a time, followed by Kafka brokers rolling one at a time. Update the cluster definition version to new version (10.5.0 in below screen shot), thne this will trigger zookeeper and kafka broker update.","title":"Event Streams Cluster upgrade"},{"location":"demo/#topic-management-with-gitops","text":"The goal of this section is to demonstrate how to change topic definition using Argocd and git. Modify the file es-topic.yaml by adding a new topic inside this file with the following declaration: apiVersion : eventstreams.ibm.com/v1beta1 kind : KafkaTopic metadata : name : demo-2-topic labels : eventstreams.ibm.com/cluster : demo spec : partitions : 1 replicas : 1 Commit and push your changes to your git repository and see ArgoCD changing the configuration, new pods should be added. Do oc get kafkatopics or go to Event Streams operator in the es-demo-day2 project to see all the Event Streams component instances.","title":"Topic management with GitOps"},{"location":"demo/#repartitioning","text":"You can demonstrate how to change the number of partition for an existing topic ( rt-items ) from 3 to 5 partitions: oc apply -f environments/rt-inventory-dev/services/ibm-eventstreams/base/update-es-topic.yaml Add more instances on the consumer part: taking the store-aggregator app and add more pods from the deployment view in OpenShift console, or change the deployment.yaml descriptor and push the change to the git repository so GitOps will catch and change the configuration: # modify https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/blob/main/environments/rt-inventory-dev/apps/store-inventory/services/store-inventory/base/config/deployment.yaml oc apply -f environments/rt-inventory-dev/apps/store-inventory/services/store-inventory/base/config/deployment.yaml Read more Event Streams doc on Kafka tools mapping to cli","title":"Repartitioning"},{"location":"demo/#clean-your-gitops","text":"Remove the ArgoCD apps oc delete -k demo/argocd Remove resources oc delete -k demo/env Read more Event driven solution with GitOps EDA GitOps Catalog Real time inventory demo gitops","title":"Clean your gitops"},{"location":"demo/#openshift-cluster-version-upgrade","text":"There may be some questions around how to migrate a version for OCP.","title":"OpenShift Cluster version upgrade"},{"location":"demo/#principles","text":"For clusters with internet accessibility, Red Hat provides over-the-air updates through an OpenShift Container Platform update service as a hosted service located behind public APIs. Due to fundamental Kubernetes design, all OpenShift Container Platform updates between minor versions must be serialized.","title":"Principles"},{"location":"demo/#what-can-be-demonstrated","text":"At the demo level, you can go to the Administration console in Administration > Cluster Settings you get something like this: If you want to upgrade version within the same release Or upgrade release change the Channel version: As the operation will take sometime, it is not really demonstrable easily. Read more Openshift understanding upgrade channels release Canary rollout","title":"What can be demonstrated"},{"location":"getting-started/","text":"Getting started with Event Streams \u00b6 Introduction and Getting Started \u00b6 Ty Harris IBM Event Streams is based on years of operational expertise that IBM has gained from running Apache Kafka\u00ae for enterprises. IBM Event Streams offers enterprise-grade security, scalability, and reliability running on Red Hat\u00ae OpenShift\u00ae Container Platform as certified container software. Building an event-driven architecture with IBM Event Streams allows organizations to transition from traditional monolith systems and silos to modern micro-services and event streaming applications that increase their agility and accelerate their time to innovation. IBM Event Streams builds on top of open-source Apache Kafka\u00ae to offer enterprise-grade event streaming capabilities. The following features are included as part of IBM Event Streams: Identity and Access Management (IAM) offers fine-grain security controls to manage the access that you want to grant each user for Kafka clusters, Topics, Consumer Groups, Producers, and more. Geo-replication enables the deployment of multipleEvent Stream instances in different locations and thesynchronization ofdata between your clusters to improve service availability. Visual driven management and monitoring experience with the Event Streams dashboard that displays metrics collected from the cluster, Kafka brokers, messages, consumers, and producers to provide health check information and options to resolve issues IBM Event Streams enables you to adopt event-driven architectures. Lab Objective \u00b6 The objective of this lab is to demonstrate the step-by-step process to download and install our Starter Apache Kafka application. The starter application provides a demonstration of a Java application that uses the Vert.x Kafka Client to send and receive events from Event Streams. The starter application also includes a user interface to easily view message propagation. The source code is provided in GitHub to allow you to understand the elements required to create your own Kafka application. App details: https://ibm.github.io/event-streams/getting-started/generating-starter-app/ Warning The API keys generated for the starter application can only be used to connect to the topic selected during generation. In addition, the consumer API key can only be used to connect with a consumer group ID set to the name of the generated application. Environment used for this lab \u00b6 IBM Cloud Pak for Integration Red Hat Openshift Container Platform IBM Event Streams version 11.x Apache Kafka 3.2 Java version 11 Lab Environment Pre-Requisites \u00b6 The Cloud Pak for Integration has been deployed and the access credentials are available. Java version 11 installed on local environment. Use the adoptium site to download Java Apache Maven Installed on local environment. Getting started with IBM Event Streams \u00b6 Click on one of the link below (depending on the OpenShift cluster allocated to you) to log into your Event Streams instance using the student credentials provided. Once you've logged in, you'll see the Event Streams homepage. Host URL Mandalorian https://cpd-cp4i.apps.mandalorian.coc-ibm.com/integration/kafka-clusters/cp4i-eventstreams/es-demo/gettingstarted Cody https://cpd-cp4i.apps.cody.coc-ibm.com/integration/kafka-clusters/cp4i-eventstreams/es-demo/gettingstarted Finn https://cpd-cp4i.apps.finn.coc-ibm.com/integration/kafka-clusters/cp4i-eventstreams/es-demo/gettingstarted Select \u201cTry the starter application\u201d tile. Create a working directory on your local drive then select the \u201cDownload JAR from GitHub\u201d tab. You\u2019ll want the demo-all.jar file for release 1.1.3. Copy the downloaded .jar file to your working directory. Now let\u2019s generate and download our properties file. Select the \u201cGenerate properties\u201d tab. Enter a name of your app. For guidance, the prefix should be your login ID similar to the example below (i.e. cody1app). This will help us identify which apps are running and who is the owner. Select the \u201cNew topic\u201d tab and enter a topic name. Again, use your login ID as the prefix for your topic name. Now select the \u201cGenerate and download .zip\u201d tab. Extract the downloaded .zip file in your working directory. Copy the properties file and p12 file to your working directory. Make sure demo-all.jar , kafka.properties and truststore.p12 are all in the same directory. Open a terminal and navigate to your working directory. Enter the following command: java -Dproperties_path = ./ -jar demo-all.jar After your applications starts, open a browser, and enter : chrome http://localhost:8080 Select the \u201cStart Producing\u201d and \u201cStart Consuming\u201d tabs. You should soon see messages being produced and consumed. Now, let\u2019s check our Event Streams cluster to verify our topic creation and monitoring of the messages. To do this, go back to the Event Streams homepage. Select \u201cToolbox\u201d. Select \u201cHome\u201d to go to the EventStreams homepage. Select \u201cTopics\u201d to see the topic you created in the Starter application. Click on your topic to see the Producers, Messages and Consumers. Congratulations! You have successfully run your starter application. Learning summary \u00b6 In summary, you have learned the following in this lab: Run an Apache Kafka Java application that has both a producer and consumer. View consumer and producer message traffic in IBM Event Streams console. Specifying the topic within IBM Event Streams and then connecting an Apache Kafka application to produce and consume messages to and from that topic. More code templates \u00b6 To start your Event Streams journey you can use reactive messaging, a Java microprofile extension, and use one of the template from this repository: eda-quickstart >> Next - Schema Registry","title":"Getting started with Event Streams"},{"location":"getting-started/#getting-started-with-event-streams","text":"","title":"Getting started with Event Streams"},{"location":"getting-started/#introduction-and-getting-started","text":"Ty Harris IBM Event Streams is based on years of operational expertise that IBM has gained from running Apache Kafka\u00ae for enterprises. IBM Event Streams offers enterprise-grade security, scalability, and reliability running on Red Hat\u00ae OpenShift\u00ae Container Platform as certified container software. Building an event-driven architecture with IBM Event Streams allows organizations to transition from traditional monolith systems and silos to modern micro-services and event streaming applications that increase their agility and accelerate their time to innovation. IBM Event Streams builds on top of open-source Apache Kafka\u00ae to offer enterprise-grade event streaming capabilities. The following features are included as part of IBM Event Streams: Identity and Access Management (IAM) offers fine-grain security controls to manage the access that you want to grant each user for Kafka clusters, Topics, Consumer Groups, Producers, and more. Geo-replication enables the deployment of multipleEvent Stream instances in different locations and thesynchronization ofdata between your clusters to improve service availability. Visual driven management and monitoring experience with the Event Streams dashboard that displays metrics collected from the cluster, Kafka brokers, messages, consumers, and producers to provide health check information and options to resolve issues IBM Event Streams enables you to adopt event-driven architectures.","title":"Introduction and Getting Started"},{"location":"getting-started/#lab-objective","text":"The objective of this lab is to demonstrate the step-by-step process to download and install our Starter Apache Kafka application. The starter application provides a demonstration of a Java application that uses the Vert.x Kafka Client to send and receive events from Event Streams. The starter application also includes a user interface to easily view message propagation. The source code is provided in GitHub to allow you to understand the elements required to create your own Kafka application. App details: https://ibm.github.io/event-streams/getting-started/generating-starter-app/ Warning The API keys generated for the starter application can only be used to connect to the topic selected during generation. In addition, the consumer API key can only be used to connect with a consumer group ID set to the name of the generated application.","title":"Lab Objective"},{"location":"getting-started/#environment-used-for-this-lab","text":"IBM Cloud Pak for Integration Red Hat Openshift Container Platform IBM Event Streams version 11.x Apache Kafka 3.2 Java version 11","title":"Environment used for this lab"},{"location":"getting-started/#lab-environment-pre-requisites","text":"The Cloud Pak for Integration has been deployed and the access credentials are available. Java version 11 installed on local environment. Use the adoptium site to download Java Apache Maven Installed on local environment.","title":"Lab Environment Pre-Requisites"},{"location":"getting-started/#getting-started-with-ibm-event-streams","text":"Click on one of the link below (depending on the OpenShift cluster allocated to you) to log into your Event Streams instance using the student credentials provided. Once you've logged in, you'll see the Event Streams homepage. Host URL Mandalorian https://cpd-cp4i.apps.mandalorian.coc-ibm.com/integration/kafka-clusters/cp4i-eventstreams/es-demo/gettingstarted Cody https://cpd-cp4i.apps.cody.coc-ibm.com/integration/kafka-clusters/cp4i-eventstreams/es-demo/gettingstarted Finn https://cpd-cp4i.apps.finn.coc-ibm.com/integration/kafka-clusters/cp4i-eventstreams/es-demo/gettingstarted Select \u201cTry the starter application\u201d tile. Create a working directory on your local drive then select the \u201cDownload JAR from GitHub\u201d tab. You\u2019ll want the demo-all.jar file for release 1.1.3. Copy the downloaded .jar file to your working directory. Now let\u2019s generate and download our properties file. Select the \u201cGenerate properties\u201d tab. Enter a name of your app. For guidance, the prefix should be your login ID similar to the example below (i.e. cody1app). This will help us identify which apps are running and who is the owner. Select the \u201cNew topic\u201d tab and enter a topic name. Again, use your login ID as the prefix for your topic name. Now select the \u201cGenerate and download .zip\u201d tab. Extract the downloaded .zip file in your working directory. Copy the properties file and p12 file to your working directory. Make sure demo-all.jar , kafka.properties and truststore.p12 are all in the same directory. Open a terminal and navigate to your working directory. Enter the following command: java -Dproperties_path = ./ -jar demo-all.jar After your applications starts, open a browser, and enter : chrome http://localhost:8080 Select the \u201cStart Producing\u201d and \u201cStart Consuming\u201d tabs. You should soon see messages being produced and consumed. Now, let\u2019s check our Event Streams cluster to verify our topic creation and monitoring of the messages. To do this, go back to the Event Streams homepage. Select \u201cToolbox\u201d. Select \u201cHome\u201d to go to the EventStreams homepage. Select \u201cTopics\u201d to see the topic you created in the Starter application. Click on your topic to see the Producers, Messages and Consumers. Congratulations! You have successfully run your starter application.","title":"Getting started with IBM Event Streams"},{"location":"getting-started/#learning-summary","text":"In summary, you have learned the following in this lab: Run an Apache Kafka Java application that has both a producer and consumer. View consumer and producer message traffic in IBM Event Streams console. Specifying the topic within IBM Event Streams and then connecting an Apache Kafka application to produce and consume messages to and from that topic.","title":"Learning summary"},{"location":"getting-started/#more-code-templates","text":"To start your Event Streams journey you can use reactive messaging, a Java microprofile extension, and use one of the template from this repository: eda-quickstart >> Next - Schema Registry","title":"More code templates"},{"location":"getting-started/eepm/","text":"Event endpoint management \u00b6 About this Lab \u00b6 In this lab, you will learn: The Application Developer capability to subscribe to topics easily from Event Endpoint Management (EEM). Consume messages from topics via EEM The following diagram explains some the components involved in the lab and the global flow, an API developer will do and an Application developer will also performs. This lab is NOT about API manager(Kafka topic owner in diagram above) creating asyncAPI for application developers to subscribe to. You will play the role of the App developer and mostly do the steps 5,6,8 and 9. References: Event Endpoint Management: https://www.ibm.com/docs/en/cloud-paks/cp-integration/2022.2?topic=capabilities-event-endpoint-management-deployment Understanding AsyncAPI and Kafka topic article Dale Lane's blog Lab Prerequisites \u00b6 Access to a API Connect Developer Portal The KafkaClient used in the Schema Registry lab. Lab Procedures \u00b6 Open the Developer portal (link provided in table below) and register a login for yourself (if this is the first time you are logging in. You need a real email address and you can use one of the student userid). If you already have a username created, login to the portal. For users who are creating the login for the first time, you will receive an email for confirmation. Please use one of these URLs to access the Developer Portal, matching the userid you have: Host URL Mandalorian Mandalorian API Portal Cody Cody API portal Finn Finn API portal then you should reach the portal main home page, with the TechJam product defined This portal is coming from the Admin user, and he defined an API product defining AsyncAPI on a customers topic in the event streams running in the OpenShift cluster under cp4i-eventstreams . Create an App. Go to Apps and Click on \u201cCreate a new app\u201d. Enter a name and click on save. The API Key and secret will be provided. Take note of it and close the box. The new app has been created. You can optionally click on \u2018Verify\u2019 and enter the secret copied to ensure the credentials you copied are correct. Subscribe to the Product and get connectivity details. In the main portal, Go to \"API Products\". Look for a product and Click on it. Take note of the connectivity details and click on \"Get Access\". Refer to the steps in the image below. Take note of the following: topic, bootstrap_server, client_id, sasl_mechanism and security_protocol. These fields will be needed to establish connection to the EventStreams via EEM. Click on \u201cSelect\u201d. Click on the newly created app and proceed to complete the subscription process. You have now subscribed to the AsyncAPI for the customers topic running in Event Streams within the same OpenShift cluster as the api management. Create the PEM certificate needed while accessing the EEM. Run this command from a terminal. echo | openssl s_client -connect EVENT_GATEWAY_URL:PORT -servername EVENT_GATEWAY_URL Example: openssl s_client -connect apim-demo-myegw-event-gw-client-apic.apps.cody.coc-ibm.com:443 -servername apim-demo-myegw-event-gw-client-apic.apps.cody.coc-ibm.com The EVENT_GATEWAY_URL is the server URL obtained in step 3. Copy the output lines between BEGIN CERTIFICATE and END CERTIFICATE to a file called eem_truststore.pem. The file should look like this: Copy the file to C:\\TechJam\\EventStreams_Lab\\KafkaClient_YYYYMMDD\\ Note If you do not have openssl installed in your computer or if you are unable to run the openssl command, you may use this for the PEM certificate. Copy and save one of the following (depending on the Openshift cluster you are using) as eem_truststore.pem. Test consuming the data in the topic you have just subscribed using the KafkaClient that you used for the Schema Registry lab. Make a backup copy of the config.properties file in the following folder: C:\\TechJam\\EventStreams_Lab\\KafkaClient_YYYYMMDD\\ Edit the config.properties file and make the following changes. Field Value enableschemaavro false bootstrap.servers Enter the bootstrap_server URL obtained in step 3. sasl.jaas.config Paste this string. Replace the <API_KEY> and <SECRET> with the details you obtained in step 2. org.apache.kafka.common.security.plain.PlainLoginModule required username='<API_KEY>' password='<SECRET>'; sasl.mechanism PLAIN security.protocol SASL_SSL topic Name of the topic to consume (obtained in step 3) ssl.truststore.location Point it to the location of the PEM file eem_truststore.pem ssl.truststore.type PEM client.id Insert the client ID obtained from step 3. The other fields can be left as it is. Now you can run the KafkaClient to consume data. cd C: \\T echJam \\E ventStreams_Lab \\K afkaClient_YYYYMMDD \\ java -jar KafkaClient.jar consumer config.properties You should see messages being consumed. Note EEM can only be used for consuming data. Not for producing data.","title":"Event-endpoint management"},{"location":"getting-started/eepm/#event-endpoint-management","text":"","title":"Event endpoint management"},{"location":"getting-started/eepm/#about-this-lab","text":"In this lab, you will learn: The Application Developer capability to subscribe to topics easily from Event Endpoint Management (EEM). Consume messages from topics via EEM The following diagram explains some the components involved in the lab and the global flow, an API developer will do and an Application developer will also performs. This lab is NOT about API manager(Kafka topic owner in diagram above) creating asyncAPI for application developers to subscribe to. You will play the role of the App developer and mostly do the steps 5,6,8 and 9. References: Event Endpoint Management: https://www.ibm.com/docs/en/cloud-paks/cp-integration/2022.2?topic=capabilities-event-endpoint-management-deployment Understanding AsyncAPI and Kafka topic article Dale Lane's blog","title":"About this Lab"},{"location":"getting-started/eepm/#lab-prerequisites","text":"Access to a API Connect Developer Portal The KafkaClient used in the Schema Registry lab.","title":"Lab Prerequisites"},{"location":"getting-started/eepm/#lab-procedures","text":"Open the Developer portal (link provided in table below) and register a login for yourself (if this is the first time you are logging in. You need a real email address and you can use one of the student userid). If you already have a username created, login to the portal. For users who are creating the login for the first time, you will receive an email for confirmation. Please use one of these URLs to access the Developer Portal, matching the userid you have: Host URL Mandalorian Mandalorian API Portal Cody Cody API portal Finn Finn API portal then you should reach the portal main home page, with the TechJam product defined This portal is coming from the Admin user, and he defined an API product defining AsyncAPI on a customers topic in the event streams running in the OpenShift cluster under cp4i-eventstreams . Create an App. Go to Apps and Click on \u201cCreate a new app\u201d. Enter a name and click on save. The API Key and secret will be provided. Take note of it and close the box. The new app has been created. You can optionally click on \u2018Verify\u2019 and enter the secret copied to ensure the credentials you copied are correct. Subscribe to the Product and get connectivity details. In the main portal, Go to \"API Products\". Look for a product and Click on it. Take note of the connectivity details and click on \"Get Access\". Refer to the steps in the image below. Take note of the following: topic, bootstrap_server, client_id, sasl_mechanism and security_protocol. These fields will be needed to establish connection to the EventStreams via EEM. Click on \u201cSelect\u201d. Click on the newly created app and proceed to complete the subscription process. You have now subscribed to the AsyncAPI for the customers topic running in Event Streams within the same OpenShift cluster as the api management. Create the PEM certificate needed while accessing the EEM. Run this command from a terminal. echo | openssl s_client -connect EVENT_GATEWAY_URL:PORT -servername EVENT_GATEWAY_URL Example: openssl s_client -connect apim-demo-myegw-event-gw-client-apic.apps.cody.coc-ibm.com:443 -servername apim-demo-myegw-event-gw-client-apic.apps.cody.coc-ibm.com The EVENT_GATEWAY_URL is the server URL obtained in step 3. Copy the output lines between BEGIN CERTIFICATE and END CERTIFICATE to a file called eem_truststore.pem. The file should look like this: Copy the file to C:\\TechJam\\EventStreams_Lab\\KafkaClient_YYYYMMDD\\ Note If you do not have openssl installed in your computer or if you are unable to run the openssl command, you may use this for the PEM certificate. Copy and save one of the following (depending on the Openshift cluster you are using) as eem_truststore.pem. Test consuming the data in the topic you have just subscribed using the KafkaClient that you used for the Schema Registry lab. Make a backup copy of the config.properties file in the following folder: C:\\TechJam\\EventStreams_Lab\\KafkaClient_YYYYMMDD\\ Edit the config.properties file and make the following changes. Field Value enableschemaavro false bootstrap.servers Enter the bootstrap_server URL obtained in step 3. sasl.jaas.config Paste this string. Replace the <API_KEY> and <SECRET> with the details you obtained in step 2. org.apache.kafka.common.security.plain.PlainLoginModule required username='<API_KEY>' password='<SECRET>'; sasl.mechanism PLAIN security.protocol SASL_SSL topic Name of the topic to consume (obtained in step 3) ssl.truststore.location Point it to the location of the PEM file eem_truststore.pem ssl.truststore.type PEM client.id Insert the client ID obtained from step 3. The other fields can be left as it is. Now you can run the KafkaClient to consume data. cd C: \\T echJam \\E ventStreams_Lab \\K afkaClient_YYYYMMDD \\ java -jar KafkaClient.jar consumer config.properties You should see messages being consumed. Note EEM can only be used for consuming data. Not for producing data.","title":"Lab Procedures"},{"location":"getting-started/mm2/","text":"Configuring and Running MirrorMaker 2 (MM2) \u00b6 Rajan Krishnan, IBM Introduction \u00b6 MirrorMaker2, released as part of Kafka 2.4.0, allows you to mirror multiple clusters and create many replication topologies. Learn all about this awesome new tool and how to reliably and easily mirror clusters. Lab Objectives \u00b6 In this lab, we will review the basic concepts of MirrorMaker2. You will demonstrate the ability to migrate a topic from a source Kafka to a destination Kafka platform. You will ensure the migration was successful by checking the main properties of topics and consumers and comparing the source / destination Kafka clusters. Pre-Requisites \u00b6 Working instance of Event Streams. A source Kafka cluster (e.g. Strimzi). Understanding MirrorMaker 2 (MM2) \u00b6 MM2 addresses some of the problems with legacy MM (MM1). Offset translation \u2013avoid duplicates. Consumer group checkpoints. Topics con figurations synced (e.g. partitions, ACLs). MM2 takes advantage of the Kafka Connect ecosystem. MM2 has 3 main components / connectors. MirrorSourceConnector Replicates remote topics, topic configurations. Emits Offset syncs2 MirrorCheckPointConnector Consumes offset syncs. Combines offset syncs and consumer offset to emit checkpoint MirrorHeartBeatConnector Used for monitoring replication flows. Sends source heartbeats to remote. As part of the lab, we will deploy the Source Connectorand Checkpoint Connector Lab Procedures \u00b6 In this lab, we will migrate data from an external Kafka topic to a local Kafka topic in Event Stream. The external Kafka can be Strimzi, Confluent, Opensource Kafka, another Event Stream platform or any other Kafka platform. For the purpose of this lab, we will use a Strimzi Kafka platform. When the lab steps are complete, the following details should be replicated between the source Kafka and destination Kafka: All the messages in the source kafka topic. Timestamp of messages should be preserved. The consumer groups and the respective lags. Message offset value will not be preserved due to retention policy in source kafka. 1. Take note of source Kafka topic details. \u00b6 You can check the status of strimzi topic here: https://kafdrop-strimzi.rajancluster-sng01-992844b4e64c83c3dbd5e7b5e2da5328-0000.jp-tok.containers.appdomain.cloud/topic/basicuserdata Take note of the topic name, consumer group, consumer lags and last offset message. 2. Take note of your KafkaConnect_URL. \u00b6 Use one of the following KafkaConnect_URL depending on the Openshift Cluster that has been assigned to you Host URL Mandalorian connect-cp4i-eventstreams.apps.mandalorian.coc-ibm.com Cody connect-cp4i-eventstreams.apps.cody.coc-ibm.com Finn connect-cp4i-eventstreams.apps.finn.coc-ibm.com 3. Kafka Connect \u00b6 Check if Kafka Connect is already setup in the cluster and the MirrorMaker 2 plugins are available. Open a Command Terminal and enter this command. curl -ki -X GET -H \"Accept: application/json\" https://<KAFKACONNECT_URL>/connector-plugins Example: curl -ki -X GET -H \"Accept: application/json\" https://connect-cp4i-eventstreams.apps.cody.coc-ibm.com/connector-plugins/ You should get an output similar to this. Note You may use tools like jq to format the json output in an easy to read format. Ensure the MirrorMaker related plugins are listed. [ { \"class\" : \"org.apache.kafka.connect.file.FileStreamSinkConnector\" , \"type\" : \"sink\" , \"version\" : \"3.1.1\" }, { \"class\" : \"org.apache.kafka.connect.file.FileStreamSourceConnector\" , \"type\" : \"source\" , \"version\" : \"3.1.0\" }, { \"class\" : \"org.apache.kafka.connect.mirror.MirrorCheckpointConnector\" , \"type\" : \"source\" , \"version\" : \"1\" }, { \"class\" : \"org.apache.kafka.connect.mirror.MirrorHeartbeatConnector\" , \"type\" : \"source\" , \"version\" : \"1\" }, { \"class\" : \"org.apache.kafka.connect.mirror.MirrorSourceConnector\" , \"type\" : \"source\" , \"version\" : \"1\" } ] 4. MM2 Connector Definitions \u00b6 Download the MM2 connectors JSON file sample that will be used for creating connectors. curl -X GET https://raw.githubusercontent.com/ibm-cloud-architecture/eda-tech-academy/main/tech-jam/mm2_json_files/mm2_source.json > mm2_source.json curl -X GET https://raw.githubusercontent.com/ibm-cloud-architecture/eda-tech-academy/main/tech-jam/mm2_json_files/mm2_checkpoint.json > mm2_checkpoint.json You will find 2 json files ( mm2_source.json & mm2_checkpoint.json ). These 2 files will be used in the next steps. Copy the files to C:\\TechJam\\EventStreams_Lab\\ Folder. Create the MM2 source connector JSON file. Edit the sample file mm2_source.json in the C:\\TechJam\\EventStreams_Lab\\ Folder. Fill-in the details accordingly. Fields not mentioned in this table can be left to their default values. parameter Value name Enter your studentID. Example: student60 source.cluster.alias A name that you will give to the source cluster. All replicated topics will have a prefix of this alias.Make sure to use the studentID as the prefix. Example: student60-source target.cluster.alias A name that you will give to the target cluster. Make sure to use the studentID as the prefix. Example: student60-target source.cluster.bootstrap.servers Kafka Bootstrap URL of the source Strimzi cluster. You can use this. rajancluster-sng01-992844b4e64c83c3dbd5e7b5e2da5328-0000.jp-tok.containers.appdomain.cloud:32463 target.cluster.bootstrap.servers Kafka Bootstrap URL of the target Event Stream Cluster. For the purpose of this lab, we have created a PLAIN connection. es-demo-kafka-bootstrap.cp4i-eventstreams.svc:9092 topics Use the topic called \u201cbasicuserdata\u201d that has been created for the purpose of this lab. This is a sample of how the file should look like. 5. MM2 connector deployment \u00b6 Create a MM2 Source Connector. We will use REST API to create the connector. Open a command prompt. Type this command. cd C: \\T echJam \\E ventStreams_Lab \\ curl -ki -X POST -H \"Content-Type: application/json\" https://<KAFKACONNECT_URL>/connectors --data \"@./mm2_source.json\" # Example: curl -ki -X POST -H \"Content-Type: application/json\" https://connect-cp4i-eventstreams.apps.cody.coc-ibm.com/connectors --data \"@./mm2_source.json\" Check the status of the MM2 connector. From the command prompt, type: # First Get the connector name from the output. curl -ki -X GET -H \"Accept: application/json\" https://<KAFKACONNECT_URL>/connectors # Example: curl -ki -X GET -H \"Accept: application/json\" https://connect-cp4i-eventstreams.apps.cody.coc-ibm.com/connectors/ [ \"cody20\" ] # Then, get the status of the connector. curl -ki -X GET -H \"Accept: application/json\" https://<KAFKACONNECT_URL>/<connector_name>/status # Example: curl -ki -X GET -H \"Accept: application/json\" https://connect-cp4i-eventstreams.apps.cody.coc-ibm.com/connectors/cody20/status/ { \"name\" : \"cody20\" , \"connector\" : { \"state\" : \"RUNNING\" , \"worker_id\" : \"10.128.0.52:8083\" } , \"tasks\" : [ { \"id\" : 0 , \"state\" : \"RUNNING\" , \"worker_id\" : \"10.128.0.52:8083\" } ] , \"type\" : \"source\" } 6. Verify topic creation \u00b6 Check the EventStreams web console and a new topic should have been created. Look for the replicated topic. The topic should be named as: <SOURCE_CLUSTER_ALIAS>-basicuserdata Example: cody20-source.basicuserdata 7. Create MM2 Checkpoint \u00b6 Create the MM2 Checkpoint connector JSON file. Edit the sample file mm2_checkpoint.json in the C:\\TechJam\\EventStreams_Lab\\ Folder. Fill-in the details accordingly. Fields not mentioned in this table can be left to it\u2019s default values. parameter Value name Enter your studentID-checkpoint. Example: cody20-checkpoint source.cluster.alias A name that you will give to the source cluster. All replicated topics will have a prefix of this alias.Make sure to use the studentID as the prefix. Example: cody20-source target.cluster.alias A name that you will give to the target cluster. Make sure to use the studentID as the prefix. Example: cody20-target source.cluster.bootstrap.servers Kafka Bootstrap URL of the source Strimzi cluster. rajancluster-sng01-992844b4e64c83c3dbd5e7b5e2da5328-0000.jp-tok.containers.appdomain.cloud:32463 target.cluster.bootstrap.servers Kafka Bootstrap URL of the target Event Stream Cluster. For the purpose of this lab, we have created a PLAIN connection. es-demo-kafka-bootstrap.cp4i-eventstreams.svc:9092 groups The consumer groups that has to be replicated. For the purpose of this lab we will replicate all groups in the source Kafka. So, use the value \u201c.*\u201d This is a sample of how the file should look like. Create a MM2 Checkpoint Connector. We will use REST API to create the connector. Open a command prompt. Type this command. cd C: \\T echJam \\E ventStreams_Lab \\ curl -ki -X POST -H \"Content-Type: application/json\" https://<KAFKACONNECT_URL>/connectors --data \"@./mm2_checkpoint.json\" # Example: curl -ki -X POST -H \"Content-Type: application/json\" https://connect-cp4i-eventstreams.apps.cody.coc-ibm.com/connectors --data \"@./mm2_checkpoint.json\" Check the status of the MM2 Checkpoint connector. From the command prompt, type: First Get the connector name from the output. curl -ki -X GET -H \"Accept: application/json\" <KAFKACONNECT_URL>/connectors # Example: curl -ki -X GET -H \"Accept: application/json\" https://connect-cp4i-eventstreams.apps.cody.coc-ibm.com/connectors/ [ \"cody20\" , \"cody20-checkpoint\" ] # Then, get the status of the checkpoint connector. curl -ki -X GET -H \"Accept: application/json\" https://<KAFKACONNECT_URL>/<connector_name>/status Example: curl -ki -X GET -H \"Accept: application/json\" https://connect-cp4i-eventstreams.apps.cody.coc-ibm.com/connectors/cody20-checkpoint/status/ { \"name\" : \"cody20-checkpoint\" , \"connector\" : { \"state\" : \"RUNNING\" , \"worker_id\" : \"10.128.0.52:8083\" } , \"tasks\" : [ { \"id\" : 0 , \"state\" : \"RUNNING\" , \"worker_id\" : \"10.128.0.52:8083\" } ] , \"type\" : \"source\" } 8. Check consumer Lag. \u00b6 Check consumer lags in Event Streams. Go to Event Streams home page -> Topics -> Click on the replicated topic -> Click on \u201cConsumer Groups\u201d. The new consumer group should appear and the offset lag should match with that of strimzi. Do a random check on the timestamp of a message. Now you can consume the data from the Event Streams cluster. You will only be consuming the remaining messages. Make a copy of the config.properties file. Call it config.properties.mm2 Edit the config.properties.mm2 file and change the following fields. Field Value enableschemaavro False topic The newly replicated topic. Example: student60-source.basicuserdata group.id You should use the same Consumer Group as the one that has been replicated. Student60-general-group-v1 Run the KafkaClient in consumer mode: java -jar KafkaClient.jar consumer config.properties.mm2 It should only consume the number of messages as listed in the lags (and not the entire message data in the topic).","title":"Mirror maker 2 lab"},{"location":"getting-started/mm2/#configuring-and-running-mirrormaker-2-mm2","text":"Rajan Krishnan, IBM","title":"Configuring and Running MirrorMaker 2 (MM2)"},{"location":"getting-started/mm2/#introduction","text":"MirrorMaker2, released as part of Kafka 2.4.0, allows you to mirror multiple clusters and create many replication topologies. Learn all about this awesome new tool and how to reliably and easily mirror clusters.","title":"Introduction"},{"location":"getting-started/mm2/#lab-objectives","text":"In this lab, we will review the basic concepts of MirrorMaker2. You will demonstrate the ability to migrate a topic from a source Kafka to a destination Kafka platform. You will ensure the migration was successful by checking the main properties of topics and consumers and comparing the source / destination Kafka clusters.","title":"Lab Objectives"},{"location":"getting-started/mm2/#pre-requisites","text":"Working instance of Event Streams. A source Kafka cluster (e.g. Strimzi).","title":"Pre-Requisites"},{"location":"getting-started/mm2/#understanding-mirrormaker-2-mm2","text":"MM2 addresses some of the problems with legacy MM (MM1). Offset translation \u2013avoid duplicates. Consumer group checkpoints. Topics con figurations synced (e.g. partitions, ACLs). MM2 takes advantage of the Kafka Connect ecosystem. MM2 has 3 main components / connectors. MirrorSourceConnector Replicates remote topics, topic configurations. Emits Offset syncs2 MirrorCheckPointConnector Consumes offset syncs. Combines offset syncs and consumer offset to emit checkpoint MirrorHeartBeatConnector Used for monitoring replication flows. Sends source heartbeats to remote. As part of the lab, we will deploy the Source Connectorand Checkpoint Connector","title":"Understanding MirrorMaker 2 (MM2)"},{"location":"getting-started/mm2/#lab-procedures","text":"In this lab, we will migrate data from an external Kafka topic to a local Kafka topic in Event Stream. The external Kafka can be Strimzi, Confluent, Opensource Kafka, another Event Stream platform or any other Kafka platform. For the purpose of this lab, we will use a Strimzi Kafka platform. When the lab steps are complete, the following details should be replicated between the source Kafka and destination Kafka: All the messages in the source kafka topic. Timestamp of messages should be preserved. The consumer groups and the respective lags. Message offset value will not be preserved due to retention policy in source kafka.","title":"Lab Procedures"},{"location":"getting-started/mm2/#1-take-note-of-source-kafka-topic-details","text":"You can check the status of strimzi topic here: https://kafdrop-strimzi.rajancluster-sng01-992844b4e64c83c3dbd5e7b5e2da5328-0000.jp-tok.containers.appdomain.cloud/topic/basicuserdata Take note of the topic name, consumer group, consumer lags and last offset message.","title":"1. Take note of source Kafka topic details."},{"location":"getting-started/mm2/#2-take-note-of-your-kafkaconnect_url","text":"Use one of the following KafkaConnect_URL depending on the Openshift Cluster that has been assigned to you Host URL Mandalorian connect-cp4i-eventstreams.apps.mandalorian.coc-ibm.com Cody connect-cp4i-eventstreams.apps.cody.coc-ibm.com Finn connect-cp4i-eventstreams.apps.finn.coc-ibm.com","title":"2. Take note of your KafkaConnect_URL."},{"location":"getting-started/mm2/#3-kafka-connect","text":"Check if Kafka Connect is already setup in the cluster and the MirrorMaker 2 plugins are available. Open a Command Terminal and enter this command. curl -ki -X GET -H \"Accept: application/json\" https://<KAFKACONNECT_URL>/connector-plugins Example: curl -ki -X GET -H \"Accept: application/json\" https://connect-cp4i-eventstreams.apps.cody.coc-ibm.com/connector-plugins/ You should get an output similar to this. Note You may use tools like jq to format the json output in an easy to read format. Ensure the MirrorMaker related plugins are listed. [ { \"class\" : \"org.apache.kafka.connect.file.FileStreamSinkConnector\" , \"type\" : \"sink\" , \"version\" : \"3.1.1\" }, { \"class\" : \"org.apache.kafka.connect.file.FileStreamSourceConnector\" , \"type\" : \"source\" , \"version\" : \"3.1.0\" }, { \"class\" : \"org.apache.kafka.connect.mirror.MirrorCheckpointConnector\" , \"type\" : \"source\" , \"version\" : \"1\" }, { \"class\" : \"org.apache.kafka.connect.mirror.MirrorHeartbeatConnector\" , \"type\" : \"source\" , \"version\" : \"1\" }, { \"class\" : \"org.apache.kafka.connect.mirror.MirrorSourceConnector\" , \"type\" : \"source\" , \"version\" : \"1\" } ]","title":"3. Kafka Connect"},{"location":"getting-started/mm2/#4-mm2-connector-definitions","text":"Download the MM2 connectors JSON file sample that will be used for creating connectors. curl -X GET https://raw.githubusercontent.com/ibm-cloud-architecture/eda-tech-academy/main/tech-jam/mm2_json_files/mm2_source.json > mm2_source.json curl -X GET https://raw.githubusercontent.com/ibm-cloud-architecture/eda-tech-academy/main/tech-jam/mm2_json_files/mm2_checkpoint.json > mm2_checkpoint.json You will find 2 json files ( mm2_source.json & mm2_checkpoint.json ). These 2 files will be used in the next steps. Copy the files to C:\\TechJam\\EventStreams_Lab\\ Folder. Create the MM2 source connector JSON file. Edit the sample file mm2_source.json in the C:\\TechJam\\EventStreams_Lab\\ Folder. Fill-in the details accordingly. Fields not mentioned in this table can be left to their default values. parameter Value name Enter your studentID. Example: student60 source.cluster.alias A name that you will give to the source cluster. All replicated topics will have a prefix of this alias.Make sure to use the studentID as the prefix. Example: student60-source target.cluster.alias A name that you will give to the target cluster. Make sure to use the studentID as the prefix. Example: student60-target source.cluster.bootstrap.servers Kafka Bootstrap URL of the source Strimzi cluster. You can use this. rajancluster-sng01-992844b4e64c83c3dbd5e7b5e2da5328-0000.jp-tok.containers.appdomain.cloud:32463 target.cluster.bootstrap.servers Kafka Bootstrap URL of the target Event Stream Cluster. For the purpose of this lab, we have created a PLAIN connection. es-demo-kafka-bootstrap.cp4i-eventstreams.svc:9092 topics Use the topic called \u201cbasicuserdata\u201d that has been created for the purpose of this lab. This is a sample of how the file should look like.","title":"4. MM2 Connector Definitions"},{"location":"getting-started/mm2/#5-mm2-connector-deployment","text":"Create a MM2 Source Connector. We will use REST API to create the connector. Open a command prompt. Type this command. cd C: \\T echJam \\E ventStreams_Lab \\ curl -ki -X POST -H \"Content-Type: application/json\" https://<KAFKACONNECT_URL>/connectors --data \"@./mm2_source.json\" # Example: curl -ki -X POST -H \"Content-Type: application/json\" https://connect-cp4i-eventstreams.apps.cody.coc-ibm.com/connectors --data \"@./mm2_source.json\" Check the status of the MM2 connector. From the command prompt, type: # First Get the connector name from the output. curl -ki -X GET -H \"Accept: application/json\" https://<KAFKACONNECT_URL>/connectors # Example: curl -ki -X GET -H \"Accept: application/json\" https://connect-cp4i-eventstreams.apps.cody.coc-ibm.com/connectors/ [ \"cody20\" ] # Then, get the status of the connector. curl -ki -X GET -H \"Accept: application/json\" https://<KAFKACONNECT_URL>/<connector_name>/status # Example: curl -ki -X GET -H \"Accept: application/json\" https://connect-cp4i-eventstreams.apps.cody.coc-ibm.com/connectors/cody20/status/ { \"name\" : \"cody20\" , \"connector\" : { \"state\" : \"RUNNING\" , \"worker_id\" : \"10.128.0.52:8083\" } , \"tasks\" : [ { \"id\" : 0 , \"state\" : \"RUNNING\" , \"worker_id\" : \"10.128.0.52:8083\" } ] , \"type\" : \"source\" }","title":"5. MM2 connector deployment"},{"location":"getting-started/mm2/#6-verify-topic-creation","text":"Check the EventStreams web console and a new topic should have been created. Look for the replicated topic. The topic should be named as: <SOURCE_CLUSTER_ALIAS>-basicuserdata Example: cody20-source.basicuserdata","title":"6. Verify topic creation"},{"location":"getting-started/mm2/#7-create-mm2-checkpoint","text":"Create the MM2 Checkpoint connector JSON file. Edit the sample file mm2_checkpoint.json in the C:\\TechJam\\EventStreams_Lab\\ Folder. Fill-in the details accordingly. Fields not mentioned in this table can be left to it\u2019s default values. parameter Value name Enter your studentID-checkpoint. Example: cody20-checkpoint source.cluster.alias A name that you will give to the source cluster. All replicated topics will have a prefix of this alias.Make sure to use the studentID as the prefix. Example: cody20-source target.cluster.alias A name that you will give to the target cluster. Make sure to use the studentID as the prefix. Example: cody20-target source.cluster.bootstrap.servers Kafka Bootstrap URL of the source Strimzi cluster. rajancluster-sng01-992844b4e64c83c3dbd5e7b5e2da5328-0000.jp-tok.containers.appdomain.cloud:32463 target.cluster.bootstrap.servers Kafka Bootstrap URL of the target Event Stream Cluster. For the purpose of this lab, we have created a PLAIN connection. es-demo-kafka-bootstrap.cp4i-eventstreams.svc:9092 groups The consumer groups that has to be replicated. For the purpose of this lab we will replicate all groups in the source Kafka. So, use the value \u201c.*\u201d This is a sample of how the file should look like. Create a MM2 Checkpoint Connector. We will use REST API to create the connector. Open a command prompt. Type this command. cd C: \\T echJam \\E ventStreams_Lab \\ curl -ki -X POST -H \"Content-Type: application/json\" https://<KAFKACONNECT_URL>/connectors --data \"@./mm2_checkpoint.json\" # Example: curl -ki -X POST -H \"Content-Type: application/json\" https://connect-cp4i-eventstreams.apps.cody.coc-ibm.com/connectors --data \"@./mm2_checkpoint.json\" Check the status of the MM2 Checkpoint connector. From the command prompt, type: First Get the connector name from the output. curl -ki -X GET -H \"Accept: application/json\" <KAFKACONNECT_URL>/connectors # Example: curl -ki -X GET -H \"Accept: application/json\" https://connect-cp4i-eventstreams.apps.cody.coc-ibm.com/connectors/ [ \"cody20\" , \"cody20-checkpoint\" ] # Then, get the status of the checkpoint connector. curl -ki -X GET -H \"Accept: application/json\" https://<KAFKACONNECT_URL>/<connector_name>/status Example: curl -ki -X GET -H \"Accept: application/json\" https://connect-cp4i-eventstreams.apps.cody.coc-ibm.com/connectors/cody20-checkpoint/status/ { \"name\" : \"cody20-checkpoint\" , \"connector\" : { \"state\" : \"RUNNING\" , \"worker_id\" : \"10.128.0.52:8083\" } , \"tasks\" : [ { \"id\" : 0 , \"state\" : \"RUNNING\" , \"worker_id\" : \"10.128.0.52:8083\" } ] , \"type\" : \"source\" }","title":"7. Create MM2 Checkpoint"},{"location":"getting-started/mm2/#8-check-consumer-lag","text":"Check consumer lags in Event Streams. Go to Event Streams home page -> Topics -> Click on the replicated topic -> Click on \u201cConsumer Groups\u201d. The new consumer group should appear and the offset lag should match with that of strimzi. Do a random check on the timestamp of a message. Now you can consume the data from the Event Streams cluster. You will only be consuming the remaining messages. Make a copy of the config.properties file. Call it config.properties.mm2 Edit the config.properties.mm2 file and change the following fields. Field Value enableschemaavro False topic The newly replicated topic. Example: student60-source.basicuserdata group.id You should use the same Consumer Group as the one that has been replicated. Student60-general-group-v1 Run the KafkaClient in consumer mode: java -jar KafkaClient.jar consumer config.properties.mm2 It should only consume the number of messages as listed in the lags (and not the entire message data in the topic).","title":"8. Check consumer Lag."},{"location":"getting-started/schema-lab/","text":"Producing & Consuming Data with Event Streams and Schema \u00b6 Introduction \u00b6 Version control can be a nightmare for organizations. With Kafka, it\u2019s no different. With stream processing pipelines, there are no files to act as containers for messages with a single format. Let take a look at how Event Streams handles Schema Management with the Schema Registry. Lab Objective \u00b6 In this lab, we\u2019ll do the following: Create a topic and attach a schema to it Create a Kafka User with appropriate rights to produce and consume data Gather information needed to connect to the Kafka cluster and Schema registry. Test producing / consuming data. Make changes to the Schema and see the impact to producer/consumer. The following figure illustrates the components involved in this lab: You will run producer and consumer apps on your laptop, and they will contact schema registry and brokers using SCRAM authentication and TLS encryption. Setting Up The Client Machine \u00b6 Setting up the sample Kafka Client to be used for the lab. This section provides the instructions for setting up the Kafka Client that will be used throughout the labs. Check java install C: \\U sers \\r ajan>java -version At least version 1 .8.0_301 should be available. If it\u2019s not installed, download and install the Java Runtime. Use the adoptium site to download Java Download the sample Kafka Client code from here: to be used on your local laptop. Unzip the downloaded Kafka Client (KafkaClient_YYYYMMDD.zip) into a folder: C: \\T echJam \\E ventStreams_Lab \\ unzip KafkaClient_20220131.zip Test the client: Open a Command Prompt. cd C: \\T echJam \\E ventStreams_Lab \\K afkaClient_YYYYMMDD \\ java -jar KafkaClient.jar Pre-Requisites \u00b6 Have setup the client machine properly. Able to access the Event Streams web interface. Understanding Schema Registry \u00b6 What is a Schema Registry? \u00b6 Schema Registry provides a serving layer for your metadata. It provides a RESTful interface for storing and retrieving your Avro\u00ae, JSON Schema, and Protobuf schemas. It stores a versioned history of all schemas based on a specified subject name strategy, provides multiple compatibility settings. Allows evolution of schemas according to the configured compatibility settings and expanded support for these schema types. Provides serializers that plug into Apache Kafka\u00ae clients that handle schema storage and retrieval for Kafka messages that are sent in any of the supported formats. In Event Streams, Schemas are stored in internal Kafka topics by the Apicur.io Registry , an open-source schema registry led by Red Hat. In addition to storing a versioned history of schemas, Apicurio Registry provides an interface for retrieving them. Each Event Streams cluster has its own instance of Apicurio Registry providing schema registry functionality. How the Schema Registry Works? \u00b6 Now, let\u2019s take a look at how the Schema Registry works. Sending applications request schema from the Schema Registry. The scheme is used to automatically validates and serializes be for the data is sent. Data is sent, serializing makes transmission more efficient. The receiving application receives the serialized data. Receiving application request the schema from the Schema Registry. Receiving application deserializes the same data automatically as it receives the message. Lab Procedures \u00b6 Creating a topic and attaching a schema to it \u00b6 Click on one of the links below (depending on the OpenShift cluster allocated to you) to log into your Event Streams instance using the student credentials provided. Once you've logged in, you'll see the Event Streams homepage. Host URL Finn https://cpd-cp4i.apps.finn.coc-ibm.com/integration/kafka-clusters/cp4i-eventstreams/es-demo/gettingstarted Cody https://cpd-cp4i.apps.cody.coc-ibm.com/integration/kafka-clusters/cp4i-eventstreams/es-demo/gettingstarted Grievous https://cpd-cp4i.apps.grievous.coc-ibm.com/integration/kafka-clusters/cp4i-eventstreams/es-demo/gettingstarted Create Topic. Click on Create a Topic. Use only lower cases for the topic name (e.g. finn20-customers ). Please refer to screenshots attached as sample. Next create the schema and attach to the topic. Click on the Schema Registry tab in the left. Click on Add Schema (in the right) Click Upload Definition -> Choose customer.avsc located in the Kafka Client unzipped folder. ( C:\\TechJam\\EventStreams_Lab\\KafkaClient_YYYYMMDD\\com\\example ) Check the details and make sure the schema is valid. Change the name of the schema to avoid conflict with other students: The name of the schema maps the schema to the topic. To attach this schema to your topic, the schema should be named according to the topic: -value. (For example, if your topic is finn20-customers \u201d, the schema should be named) Click on Add Schema. The schema is now attached to the topic. Creating a Kafka User with appropriate rights. \u00b6 Go to the Event Streams home page. Select \u201cConnect to this Cluster\u201d -> Generate SCRAM Credentials. Refer to the screenshot attached as reference. Keep information about the SCRAM password. Gather Connection Details \u00b6 Creating connection from Consumer / Producer requires some connectivity details. These details can be gathered from the Event Stream\u2019s portal. Connectivity details needed will depend on type of authentication and SASL mechanism used. From the Event Stream home page, click on \u201cConnect to this Cluster\u201d. Get the following information from the page. Refer to screenshot below on how to get these. Bootstrap URL Truststore Certificate File. Copy the downloaded file to the Kafka Client folder. Truststore Password. (Password will be generated once Download Certificate is clicked). Schema Registry URL Test Producer / Consumer \u00b6 Prepare the config.properties file located in C:\\TechJam\\EventStreams_Lab\\KafkaClient_YYYYMMDD\\ Check and change the following fields. The fields not mentioned here can be left default. Field Value enableschemaavro True (as we have schema attached to the topic) bootstrap.servers Enter the URL obtained in previous section e.g. es1-kafka-bootstrap-cp4i.apps.ocp46.tec.uk.ibm.com:443 sasl.jaas.config Paste this string. Replace the Username and Password. org.apache.kafka.common.security.scram.ScramLoginModule required username=' ' password=' '; sasl.mechanism SCRAM-SHA-512 security.protocol SASL_SSL topic Topic created previously. E.g. jam60-topic1 group.id Enter a Consumer Group ID. You can enter a Consumer Group. Remember that it should have a prefix of your studentID. E.g. jam60-consumer-group-v1 ssl.truststore.location Should point to the Truststore certificate downloaded. Example: ./es-cert.p12 ssl.truststore.password Enter the Truststore password obtained. schema.registry.url Enter the URL obtained in previous section e.g. https://es1-ibm-es-ac-reg-external-cp4i.apps.ocp46.tec.uk.ibm.com schema.registry.basic.auth.user.info <SCRAM_USER>:<SCRAM_PASSEORD> schema.registry.ssl.truststore.location Same as ssl.truststore.location schema.registry.ssl.truststore.password Same as ssl.truststore.password This is how your config.properties should look like after the changes. This is a sample. Do not copy and paste this contents. ## Mandatory Section ## # Set to true if avro schema is enabled for the topic enableschemaavro = true # Set to true if want to enable Intercept Monitoring. enableintercept = false # Set this to true if mTLS (2-way authentication) is enabled. enablemtls = false # Confluent Broker related properties bootstrap.servers = minimal-prod-kafka-bootstrap-es.mycluster-rajan09-992844b4e64c83c3dbd5e7b5e2da5328-0000.sng01.containers.appdomain.cloud:443 sasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required username = 'jam60-kafka01' password = 'Do0vIJuwnANZ' ; # Options are PLAIN, SCRAM, GSSAPI sasl.mechanism = SCRAM-SHA-512 # Options are SSL, PLAINTEXT, SASL_SSL, SASL_PLAINTEXT security.protocol = SASL_SSL topic = jam60-topic1 #topic=UserDatabase # Consumer Group ID group.id = jam60-student-group-v1 client.id = student-client-v1 #-------------------------------- ## To be filled in if TLS is enabled for the Brokers # Options are PKCS12, JKS, PEM. Password not required for PEM. ssl.truststore.type = PKCS12 ssl.truststore.location = ./es-cert.p12 ssl.truststore.password = muuJr3QFiiwa #-------------------------------- ## To be filled if mTLS (Mutual TLS) is enabled in Brokers ssl.keystore.location = /home/rajan/load_security/kafka.client.keystore.jks ssl.keystore.password = clientpass ssl.key.password = clientpass #------------------------------- ## To be filled in if Schema is enabled schema.registry.url = https://minimal-prod-ibm-es-ac-reg-external-es.mycluster-rajan09-992844b4e64c83c3dbd5e7b5e2da5328-0000.sng01.containers.appdomain.cloud # The following parameter MUST be set to false if connecting to EventStreams (APICURIO Schema). auto.register.schemas = true ## To be filled in if Schema Registry requires Authentication (e.g. with RBAC enabled). Otherwise leave it as default. basic.auth.credentials.source = USER_INFO schema.registry.basic.auth.user.info = jam60-kafka01:Do0vIJuwnANZ #-------------------------------- ## To be filled in if TLS is enabled for Schema Registry schema.registry.ssl.truststore.location = ./es-cert.p12 schema.registry.ssl.truststore.password = muuJr3QFiiwa #-------------------------------- ## To be filled if Consumer / Producer Intercept should be turned on intercept_bootstrapServers = es3minimal-kafka-bootstrap-es3.mycluster-rajan07-992844b4e64c83c3dbd5e7b5e2da5328-0000.jp-tok.containers.appdomain.cloud:443 intercept_sasljaas = org.apache.kafka.common.security.scram.ScramLoginModule required username = 'rajan' password = 'CfKQZG9Cm7g5' ; intercept_security = SASL_SSL intercept_saslmechanism = SCRAM-SHA-512 #-------------------------------- ## To be used when Kerberos Authentication is used sasl.kerberos.service.name = kafka #-------------------------------- ## Required parameters if Confluent in Confluent Cloud is used retries = 2 Test producing message. Go to this folder in command prompt: cd C: \\T echJam \\E ventStreams_Lab \\K afkaClient_YYYYMMDD \\ java -jar KafkaClient.jar producer 10 config.properties Check if the message is listed in the topic. In the Event Streams portal, go to Topics. Look for the topic that you created. Click on it. Then click on messages. You should see the messages produced. Warning The messages content may not be displayed correctly in the portal due to binary serialization with Avro. Test consuming message. java -jar KafkaClient.jar consumer config.properties Messages should be consumed correctly. Message content should be displayed correctly. Press CTRL-C to stop the consumer. Check the impact of changing the Schema Registry \u00b6 We will change the schema registry by adding a new field with default value, and check what happens when producing / consuming. In the client computer, make a copy of the customer.avsc file (located in C:\\TechJam\\EventStreams_Lab\\KafkaClient_YYYYMMDD\\com\\example> ) and name it customer_v2.avsc . You can do this from Windows Explorer. Edit the file using Notepad++. Add this line right after country. Change the version to version 1.1 . { \"name\" : \"company\" , \"type\" : \"string\" , \"doc\" : \"Customer Company\" }, The customer_v2.avsc should look like this: From the Event Streams portal, Go to Schema Registry -> Click on your Schema. Then, click on \u201cAdd New Version\u201d. Click on \u201cUpload Definition\u201d and select the edited avsc file ( customer_v2.avsc ). You should get a validation failed message. Understanding Schema Registry Evolution When a schema is created, it has to have a compatibility mode. The most used compatibility modes are: BACKWARD - new schema can be used to read data written with old schema [e.g. consumer uses the new schema and read an older offset data] FORWARD - old schema can still be used (e.g. by consumers) to read data written in new schema FULL - Both forward and backward In Event Streams, the default compatibility mode is FULL. In our customer_v2.avsc we have added a new mandatory field. Older consumers may not be aware of this field until they update their code. Hence, our schema is NOT FORWARD compatible and so, it fails validation. Now, edit the schema file (customer_v2.avsc) again and add a default value to the newly added line. The line should look like this: { \"name\" : \"company\" , \"type\" : \"string\" , \"default\" : \"IBM\" , \"doc\" : \"Customer Company\" }, The customer_v2.avsc should look like this. Now try updating the schema. Validation should pass. Change the version number and click on \u201cAdd Schema\u201d. Test producing / consuming data \u00b6 Getting details about the schema. The Event Streams schema registry supports a Rest Endpoint that provides details about the schema. First make sure you have the Basic Authentication Token created during the process of creating the Kafka SCRAM User. If you missed copying the token, you can generate the token from the SCRAM USERNAME and SCRAM PASSWORD. Open this URL: https://www.base64encode.org/ Enter your SCRAM USERNAME and SCRAM PASSWORD separated by a colon. E.g. : Click on Encode and it will generate the Basic Authentication Token. Get the default compatibility. curl -ki -X GET -H \"Accept: application/json\" -H \"Authorization: Basic <BASIC AUTH TOKEN>\" https://<SCHEMA_REGISTRY_URL>/rules/COMPATIBILITY E.g. curl -ki -X GET -H \"Accept: application/json\" -H \"Authorization: Basic <BASIC_AUTH_TOKEN>\" https://es1-ibm-es-ac-reg-external-cp4i.apps.ocp46.tec.uk.ibm.com/rules/COMPATIBILITY The response should be something like: { \"config\" : \"FULL\" , \"type\" : \"COMPATIBILITY\" } ```` This shows t ha t t he de fault compa t ibili t y is FULL. Nex t ge t t he compa t ibili t y o f t he speci f ic schema t ha t we are usi n g. ```sh curl - ki - X GET - H \u201cAccep t : applica t io n /jso n \u201d - H \u201cAu t horiza t io n : Basic <BASIC_AUTH_TOKEN>\u201d h tt ps : //es1-ibm-es-ac-reg-external-cp4i.apps.ocp46.tec.uk.ibm.com/artifacts/<YOUR_SCHEMA_NAME>/rules This should give you an empty response [] Which basically means \u2013 the schema uses the default global setting \u2013 which is FULL (as we saw when we tried changing the schema). Test sending some message, you will see default value for the company new field. java -jar KafkaClient.jar producer 10 config.properties !!! success \"Congratulations!\" You've completed the schema registry lab. >> Next - Event End Point Management","title":"Event Streams schema"},{"location":"getting-started/schema-lab/#producing-consuming-data-with-event-streams-and-schema","text":"","title":"Producing &amp; Consuming Data with Event Streams and Schema"},{"location":"getting-started/schema-lab/#introduction","text":"Version control can be a nightmare for organizations. With Kafka, it\u2019s no different. With stream processing pipelines, there are no files to act as containers for messages with a single format. Let take a look at how Event Streams handles Schema Management with the Schema Registry.","title":"Introduction"},{"location":"getting-started/schema-lab/#lab-objective","text":"In this lab, we\u2019ll do the following: Create a topic and attach a schema to it Create a Kafka User with appropriate rights to produce and consume data Gather information needed to connect to the Kafka cluster and Schema registry. Test producing / consuming data. Make changes to the Schema and see the impact to producer/consumer. The following figure illustrates the components involved in this lab: You will run producer and consumer apps on your laptop, and they will contact schema registry and brokers using SCRAM authentication and TLS encryption.","title":"Lab Objective"},{"location":"getting-started/schema-lab/#setting-up-the-client-machine","text":"Setting up the sample Kafka Client to be used for the lab. This section provides the instructions for setting up the Kafka Client that will be used throughout the labs. Check java install C: \\U sers \\r ajan>java -version At least version 1 .8.0_301 should be available. If it\u2019s not installed, download and install the Java Runtime. Use the adoptium site to download Java Download the sample Kafka Client code from here: to be used on your local laptop. Unzip the downloaded Kafka Client (KafkaClient_YYYYMMDD.zip) into a folder: C: \\T echJam \\E ventStreams_Lab \\ unzip KafkaClient_20220131.zip Test the client: Open a Command Prompt. cd C: \\T echJam \\E ventStreams_Lab \\K afkaClient_YYYYMMDD \\ java -jar KafkaClient.jar","title":"Setting Up The Client Machine"},{"location":"getting-started/schema-lab/#pre-requisites","text":"Have setup the client machine properly. Able to access the Event Streams web interface.","title":"Pre-Requisites"},{"location":"getting-started/schema-lab/#understanding-schema-registry","text":"","title":"Understanding Schema Registry"},{"location":"getting-started/schema-lab/#what-is-a-schema-registry","text":"Schema Registry provides a serving layer for your metadata. It provides a RESTful interface for storing and retrieving your Avro\u00ae, JSON Schema, and Protobuf schemas. It stores a versioned history of all schemas based on a specified subject name strategy, provides multiple compatibility settings. Allows evolution of schemas according to the configured compatibility settings and expanded support for these schema types. Provides serializers that plug into Apache Kafka\u00ae clients that handle schema storage and retrieval for Kafka messages that are sent in any of the supported formats. In Event Streams, Schemas are stored in internal Kafka topics by the Apicur.io Registry , an open-source schema registry led by Red Hat. In addition to storing a versioned history of schemas, Apicurio Registry provides an interface for retrieving them. Each Event Streams cluster has its own instance of Apicurio Registry providing schema registry functionality.","title":"What is a Schema Registry?"},{"location":"getting-started/schema-lab/#how-the-schema-registry-works","text":"Now, let\u2019s take a look at how the Schema Registry works. Sending applications request schema from the Schema Registry. The scheme is used to automatically validates and serializes be for the data is sent. Data is sent, serializing makes transmission more efficient. The receiving application receives the serialized data. Receiving application request the schema from the Schema Registry. Receiving application deserializes the same data automatically as it receives the message.","title":"How the Schema Registry Works?"},{"location":"getting-started/schema-lab/#lab-procedures","text":"","title":"Lab Procedures"},{"location":"getting-started/schema-lab/#creating-a-topic-and-attaching-a-schema-to-it","text":"Click on one of the links below (depending on the OpenShift cluster allocated to you) to log into your Event Streams instance using the student credentials provided. Once you've logged in, you'll see the Event Streams homepage. Host URL Finn https://cpd-cp4i.apps.finn.coc-ibm.com/integration/kafka-clusters/cp4i-eventstreams/es-demo/gettingstarted Cody https://cpd-cp4i.apps.cody.coc-ibm.com/integration/kafka-clusters/cp4i-eventstreams/es-demo/gettingstarted Grievous https://cpd-cp4i.apps.grievous.coc-ibm.com/integration/kafka-clusters/cp4i-eventstreams/es-demo/gettingstarted Create Topic. Click on Create a Topic. Use only lower cases for the topic name (e.g. finn20-customers ). Please refer to screenshots attached as sample. Next create the schema and attach to the topic. Click on the Schema Registry tab in the left. Click on Add Schema (in the right) Click Upload Definition -> Choose customer.avsc located in the Kafka Client unzipped folder. ( C:\\TechJam\\EventStreams_Lab\\KafkaClient_YYYYMMDD\\com\\example ) Check the details and make sure the schema is valid. Change the name of the schema to avoid conflict with other students: The name of the schema maps the schema to the topic. To attach this schema to your topic, the schema should be named according to the topic: -value. (For example, if your topic is finn20-customers \u201d, the schema should be named) Click on Add Schema. The schema is now attached to the topic.","title":"Creating a topic and attaching a schema to it"},{"location":"getting-started/schema-lab/#creating-a-kafka-user-with-appropriate-rights","text":"Go to the Event Streams home page. Select \u201cConnect to this Cluster\u201d -> Generate SCRAM Credentials. Refer to the screenshot attached as reference. Keep information about the SCRAM password.","title":"Creating a Kafka User with appropriate rights."},{"location":"getting-started/schema-lab/#gather-connection-details","text":"Creating connection from Consumer / Producer requires some connectivity details. These details can be gathered from the Event Stream\u2019s portal. Connectivity details needed will depend on type of authentication and SASL mechanism used. From the Event Stream home page, click on \u201cConnect to this Cluster\u201d. Get the following information from the page. Refer to screenshot below on how to get these. Bootstrap URL Truststore Certificate File. Copy the downloaded file to the Kafka Client folder. Truststore Password. (Password will be generated once Download Certificate is clicked). Schema Registry URL","title":"Gather Connection Details"},{"location":"getting-started/schema-lab/#test-producer-consumer","text":"Prepare the config.properties file located in C:\\TechJam\\EventStreams_Lab\\KafkaClient_YYYYMMDD\\ Check and change the following fields. The fields not mentioned here can be left default. Field Value enableschemaavro True (as we have schema attached to the topic) bootstrap.servers Enter the URL obtained in previous section e.g. es1-kafka-bootstrap-cp4i.apps.ocp46.tec.uk.ibm.com:443 sasl.jaas.config Paste this string. Replace the Username and Password. org.apache.kafka.common.security.scram.ScramLoginModule required username=' ' password=' '; sasl.mechanism SCRAM-SHA-512 security.protocol SASL_SSL topic Topic created previously. E.g. jam60-topic1 group.id Enter a Consumer Group ID. You can enter a Consumer Group. Remember that it should have a prefix of your studentID. E.g. jam60-consumer-group-v1 ssl.truststore.location Should point to the Truststore certificate downloaded. Example: ./es-cert.p12 ssl.truststore.password Enter the Truststore password obtained. schema.registry.url Enter the URL obtained in previous section e.g. https://es1-ibm-es-ac-reg-external-cp4i.apps.ocp46.tec.uk.ibm.com schema.registry.basic.auth.user.info <SCRAM_USER>:<SCRAM_PASSEORD> schema.registry.ssl.truststore.location Same as ssl.truststore.location schema.registry.ssl.truststore.password Same as ssl.truststore.password This is how your config.properties should look like after the changes. This is a sample. Do not copy and paste this contents. ## Mandatory Section ## # Set to true if avro schema is enabled for the topic enableschemaavro = true # Set to true if want to enable Intercept Monitoring. enableintercept = false # Set this to true if mTLS (2-way authentication) is enabled. enablemtls = false # Confluent Broker related properties bootstrap.servers = minimal-prod-kafka-bootstrap-es.mycluster-rajan09-992844b4e64c83c3dbd5e7b5e2da5328-0000.sng01.containers.appdomain.cloud:443 sasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required username = 'jam60-kafka01' password = 'Do0vIJuwnANZ' ; # Options are PLAIN, SCRAM, GSSAPI sasl.mechanism = SCRAM-SHA-512 # Options are SSL, PLAINTEXT, SASL_SSL, SASL_PLAINTEXT security.protocol = SASL_SSL topic = jam60-topic1 #topic=UserDatabase # Consumer Group ID group.id = jam60-student-group-v1 client.id = student-client-v1 #-------------------------------- ## To be filled in if TLS is enabled for the Brokers # Options are PKCS12, JKS, PEM. Password not required for PEM. ssl.truststore.type = PKCS12 ssl.truststore.location = ./es-cert.p12 ssl.truststore.password = muuJr3QFiiwa #-------------------------------- ## To be filled if mTLS (Mutual TLS) is enabled in Brokers ssl.keystore.location = /home/rajan/load_security/kafka.client.keystore.jks ssl.keystore.password = clientpass ssl.key.password = clientpass #------------------------------- ## To be filled in if Schema is enabled schema.registry.url = https://minimal-prod-ibm-es-ac-reg-external-es.mycluster-rajan09-992844b4e64c83c3dbd5e7b5e2da5328-0000.sng01.containers.appdomain.cloud # The following parameter MUST be set to false if connecting to EventStreams (APICURIO Schema). auto.register.schemas = true ## To be filled in if Schema Registry requires Authentication (e.g. with RBAC enabled). Otherwise leave it as default. basic.auth.credentials.source = USER_INFO schema.registry.basic.auth.user.info = jam60-kafka01:Do0vIJuwnANZ #-------------------------------- ## To be filled in if TLS is enabled for Schema Registry schema.registry.ssl.truststore.location = ./es-cert.p12 schema.registry.ssl.truststore.password = muuJr3QFiiwa #-------------------------------- ## To be filled if Consumer / Producer Intercept should be turned on intercept_bootstrapServers = es3minimal-kafka-bootstrap-es3.mycluster-rajan07-992844b4e64c83c3dbd5e7b5e2da5328-0000.jp-tok.containers.appdomain.cloud:443 intercept_sasljaas = org.apache.kafka.common.security.scram.ScramLoginModule required username = 'rajan' password = 'CfKQZG9Cm7g5' ; intercept_security = SASL_SSL intercept_saslmechanism = SCRAM-SHA-512 #-------------------------------- ## To be used when Kerberos Authentication is used sasl.kerberos.service.name = kafka #-------------------------------- ## Required parameters if Confluent in Confluent Cloud is used retries = 2 Test producing message. Go to this folder in command prompt: cd C: \\T echJam \\E ventStreams_Lab \\K afkaClient_YYYYMMDD \\ java -jar KafkaClient.jar producer 10 config.properties Check if the message is listed in the topic. In the Event Streams portal, go to Topics. Look for the topic that you created. Click on it. Then click on messages. You should see the messages produced. Warning The messages content may not be displayed correctly in the portal due to binary serialization with Avro. Test consuming message. java -jar KafkaClient.jar consumer config.properties Messages should be consumed correctly. Message content should be displayed correctly. Press CTRL-C to stop the consumer.","title":"Test Producer / Consumer"},{"location":"getting-started/schema-lab/#check-the-impact-of-changing-the-schema-registry","text":"We will change the schema registry by adding a new field with default value, and check what happens when producing / consuming. In the client computer, make a copy of the customer.avsc file (located in C:\\TechJam\\EventStreams_Lab\\KafkaClient_YYYYMMDD\\com\\example> ) and name it customer_v2.avsc . You can do this from Windows Explorer. Edit the file using Notepad++. Add this line right after country. Change the version to version 1.1 . { \"name\" : \"company\" , \"type\" : \"string\" , \"doc\" : \"Customer Company\" }, The customer_v2.avsc should look like this: From the Event Streams portal, Go to Schema Registry -> Click on your Schema. Then, click on \u201cAdd New Version\u201d. Click on \u201cUpload Definition\u201d and select the edited avsc file ( customer_v2.avsc ). You should get a validation failed message. Understanding Schema Registry Evolution When a schema is created, it has to have a compatibility mode. The most used compatibility modes are: BACKWARD - new schema can be used to read data written with old schema [e.g. consumer uses the new schema and read an older offset data] FORWARD - old schema can still be used (e.g. by consumers) to read data written in new schema FULL - Both forward and backward In Event Streams, the default compatibility mode is FULL. In our customer_v2.avsc we have added a new mandatory field. Older consumers may not be aware of this field until they update their code. Hence, our schema is NOT FORWARD compatible and so, it fails validation. Now, edit the schema file (customer_v2.avsc) again and add a default value to the newly added line. The line should look like this: { \"name\" : \"company\" , \"type\" : \"string\" , \"default\" : \"IBM\" , \"doc\" : \"Customer Company\" }, The customer_v2.avsc should look like this. Now try updating the schema. Validation should pass. Change the version number and click on \u201cAdd Schema\u201d.","title":"Check the impact of changing the Schema Registry"},{"location":"getting-started/schema-lab/#test-producing-consuming-data","text":"Getting details about the schema. The Event Streams schema registry supports a Rest Endpoint that provides details about the schema. First make sure you have the Basic Authentication Token created during the process of creating the Kafka SCRAM User. If you missed copying the token, you can generate the token from the SCRAM USERNAME and SCRAM PASSWORD. Open this URL: https://www.base64encode.org/ Enter your SCRAM USERNAME and SCRAM PASSWORD separated by a colon. E.g. : Click on Encode and it will generate the Basic Authentication Token. Get the default compatibility. curl -ki -X GET -H \"Accept: application/json\" -H \"Authorization: Basic <BASIC AUTH TOKEN>\" https://<SCHEMA_REGISTRY_URL>/rules/COMPATIBILITY E.g. curl -ki -X GET -H \"Accept: application/json\" -H \"Authorization: Basic <BASIC_AUTH_TOKEN>\" https://es1-ibm-es-ac-reg-external-cp4i.apps.ocp46.tec.uk.ibm.com/rules/COMPATIBILITY The response should be something like: { \"config\" : \"FULL\" , \"type\" : \"COMPATIBILITY\" } ```` This shows t ha t t he de fault compa t ibili t y is FULL. Nex t ge t t he compa t ibili t y o f t he speci f ic schema t ha t we are usi n g. ```sh curl - ki - X GET - H \u201cAccep t : applica t io n /jso n \u201d - H \u201cAu t horiza t io n : Basic <BASIC_AUTH_TOKEN>\u201d h tt ps : //es1-ibm-es-ac-reg-external-cp4i.apps.ocp46.tec.uk.ibm.com/artifacts/<YOUR_SCHEMA_NAME>/rules This should give you an empty response [] Which basically means \u2013 the schema uses the default global setting \u2013 which is FULL (as we saw when we tried changing the schema). Test sending some message, you will see default value for the company new field. java -jar KafkaClient.jar producer 10 config.properties !!! success \"Congratulations!\" You've completed the schema registry lab. >> Next - Event End Point Management","title":"Test producing / consuming data"},{"location":"lab1/","text":"Real-time inventory solution design \u00b6 This use case is coming from four customer's engagements since the last three years. Duration : 30 minutes Pre-requisites - MAC Users \u00b6 Have a git client installed Have a git account into IBM Internal github or public github . A JDK 11. Install the make tool: brew install make Have oc cli installed. It can be done once connected to the OpenShift cluster using the <?> icon on the top-right and \"Command Line Tool\" menu. Get a Java development IDE, we use Visual Code in our group. Install Java Coding Pack for Visual Studio . This will download JDK and the necessary plugins. OCP access with CP4I installed, could be ROKS, TechZone with CP4I cluster, we are using CoC environment as a base for our deployments See environment section for your assigned cluster To access to the git repository click on the top right icon from the documentation page: ibm-cloud-architecture/eda-tech-academy Fork this repository to your own git account so you can modify content and deploy code from your repository when using GitOps. and then clone it to your local laptop: git clone https://github.com/ibm-cloud-architecture/eda-tech-academy/ Environments \u00b6 We have two OpenShift clusters available with 25 userids each. Finn cluster is console-openshift-console.apps.finn.coc-ibm.com Cody cluster is console-openshift-console.apps.cody.coc-ibm.com Userids will be finn1 to finn25 for Finn cluster Userids will be cody1 to cody25 for the Cody cluster. Scripts \u00b6 All the scripts and configurations were developed from a Mac so no problem for you. Pre-requisites - Windows Users \u00b6 Have a git client installed Have a git account into IBM Internal github or public github . Clone https://github.com/ibm-cloud-architecture/eda-tech-academy/ repositary: In GitHub Desktop, Choose to clone a URL. Enter the above URL. Clone to a local folder (e.g. C:\\GitHub). Ensure the C:\\Github folder and sub folders do not have the \u2018Read Only\u2019 turned on. Install Visual Code. https://code.visualstudio.com/ . Ignore the warning about installing as an Admin user and continue. In the \u2018Additional Tasks\u2019 screen, pick all the tasks. Install Java Coding Pack for Visual Studio. https://aka.ms/vscode-java-installer-win . This will download JDK and the necessary plugins. Setup Windows Subsystem For Linux. Enable WSL. Open a PowerShell screen and run this command. Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux. This step will require a Reboot of the machine. Proceed to reboot. Enable Virtual Machine Feature dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart. [May require restart for Windows 11] Download and install the Linux kernel update package. [https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi] If you are in a Windows 11 environment, you may have to set the default WSL version to 1. wsl --set-default-version 1. Download and install Ubuntu Linux distribution from Microsoft Store. https://aka.ms/wslstore Search for Ubuntu and click on \u2018Get\u2019. This will download and install Ubuntu terminal environment with WSL. Once installed Click on \u2018Open\u2019. The first time, it will take some time to decompress some files. Wait for this step to complete. At the end, you\u2019ll be asked to create a username and password to be used to login. Finally, you will be in the Linux shell prompt. Subsequently, open a CMD (or PowerShell) terminal and enter \u2018bash\u2019 to get access to the Linux shell. Setup a few tools in the Ubuntu system. Run these commands in the Ubuntu Shell screen. Install dos2unix. sudo apt-get update. sudo apt-get install dos2unix. Install 'oc' CLI. wget https://downloads-openshift-console.apps.cody.coc-ibm.com/amd64/linux/oc.tar --no-check-certificate. tar -xvf oc.tar. sudo mv oc /usr/local/bin. Install 'make' sudo apt install make. Install 'zsh' shell. sudo apt install zsh. OCP access with CP4I installed, could be ROKS, TechZone with CP4I cluster, we are using CoC environment as a base for our deployments See environment section for your assigned cluster To access to the git repository click on the top right icon from the documentation page: ibm-cloud-architecture/eda-tech-academy Fork this repository to your own git account so you can modify content and deploy code from your repository when using GitOps. and then clone it to your local laptop: git clone https://github.com/ibm-cloud-architecture/eda-tech-academy/ Environments \u00b6 We have two OpenShift clusters available with 25 userids each. Finn cluster is console-openshift-console.apps.finn.coc-ibm.com Cody cluster is console-openshift-console.apps.cody.coc-ibm.com Userids will be finn1 to finn25 for Finn cluster Userids will be cody1 to cody25 for the Cody cluster. Problem statement \u00b6 Today, a lot of companies which are managing item / product inventory are facing real challenges to get a close to real-time view of item availability and global inventory view. The solution can be very complex to implement while integrating Enterprise Resource Planning products and other custom legacy systems. Acme customer is asking you to present how to address this problem using an event-driven architecture and cloud native microservices. Normally to do MVP, we propose to use an Event Storming workshop to discover the problem statment, the stakeholders, the business goals, and the business process from an event point of view. The image below is an example of the outcomes of such event storming session. A MVP will be complex to implement, but the customer wants first to select a technology, so a proof of concept needs to be done to guide them on how our product portfolio will address their problems. How to start? Exercise 1: system design \u00b6 Giving the problem statement: How to design a near real-time inventory view of Stores, warehouses, scaling at millions of transactions per minutes? how do you design a solution, that can get visibility of the transactions, at scale, and give a near-real time consistent view of the items in store, cross stores and what do you propose to the customer as a proof of concept. Start from a white page, design components that you think will be relevant. Duration : 30 minutes Expected outcome A diagram illustrating the design. If you use Visual Code you can get the drawio plugin A list of questions you would like to see addressed Some information you gathered from your framing meeting \u00b6 SAP is used to be the final system of record Company has multiple stores and warehouses Sale and restock transactions are in a TLOG format Stores and warehouses have local view of their own inventory on local servers and cash machines. Some inventory for warehouses and transactions are done in mainframe and shared with IBM MQ Item has code bar and SKU so people moving stock internally can scan item so local system may have visibility of where items are after some small latency. The volume of transactions is around 5 millions a day Deployment will be between 3 data centers to cover the territory the company work in. Customer has heard about kafka, but they use MQ today, CICS, Java EE applications on WebSphere Architect wants to deploy to the cloud and adopt flexible microservice architecture SAP system will stay in enterprise Architect wants to use data lake solution to let data scientists develop statistical and AI models. Guidances Think about enterprise network, cloud providers, stores and warehouses as sources. SAP - Hana is one of the system of record and will get data from it It is also possible to get data before it reaches SAP SAP has database tables that can be used as source of events As warehouse systems are using MQ, queue replication is something to think about What could be a PoC scope? You may want to demonstrate Event Streams capabilities as Kafka backbone You want to demonstrate MQ source connector to get message from MQ to Kafka You may want to explain how messages are published from a microservice using Microprofile Reactive Messaging The real-time aggregation to compute store inventory can be done with streaming processing, and Kafka Streams APIs can be used for that. For Data lake integration, you can illustrates messages landing in S3 buckets in IBM Cloud Object Storage using Kafka Sink connector. See the Solution","title":"Problem Statement"},{"location":"lab1/#real-time-inventory-solution-design","text":"This use case is coming from four customer's engagements since the last three years. Duration : 30 minutes","title":"Real-time inventory solution design"},{"location":"lab1/#pre-requisites-mac-users","text":"Have a git client installed Have a git account into IBM Internal github or public github . A JDK 11. Install the make tool: brew install make Have oc cli installed. It can be done once connected to the OpenShift cluster using the <?> icon on the top-right and \"Command Line Tool\" menu. Get a Java development IDE, we use Visual Code in our group. Install Java Coding Pack for Visual Studio . This will download JDK and the necessary plugins. OCP access with CP4I installed, could be ROKS, TechZone with CP4I cluster, we are using CoC environment as a base for our deployments See environment section for your assigned cluster To access to the git repository click on the top right icon from the documentation page: ibm-cloud-architecture/eda-tech-academy Fork this repository to your own git account so you can modify content and deploy code from your repository when using GitOps. and then clone it to your local laptop: git clone https://github.com/ibm-cloud-architecture/eda-tech-academy/","title":"Pre-requisites - MAC Users"},{"location":"lab1/#environments","text":"We have two OpenShift clusters available with 25 userids each. Finn cluster is console-openshift-console.apps.finn.coc-ibm.com Cody cluster is console-openshift-console.apps.cody.coc-ibm.com Userids will be finn1 to finn25 for Finn cluster Userids will be cody1 to cody25 for the Cody cluster.","title":"Environments"},{"location":"lab1/#scripts","text":"All the scripts and configurations were developed from a Mac so no problem for you.","title":"Scripts"},{"location":"lab1/#pre-requisites-windows-users","text":"Have a git client installed Have a git account into IBM Internal github or public github . Clone https://github.com/ibm-cloud-architecture/eda-tech-academy/ repositary: In GitHub Desktop, Choose to clone a URL. Enter the above URL. Clone to a local folder (e.g. C:\\GitHub). Ensure the C:\\Github folder and sub folders do not have the \u2018Read Only\u2019 turned on. Install Visual Code. https://code.visualstudio.com/ . Ignore the warning about installing as an Admin user and continue. In the \u2018Additional Tasks\u2019 screen, pick all the tasks. Install Java Coding Pack for Visual Studio. https://aka.ms/vscode-java-installer-win . This will download JDK and the necessary plugins. Setup Windows Subsystem For Linux. Enable WSL. Open a PowerShell screen and run this command. Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux. This step will require a Reboot of the machine. Proceed to reboot. Enable Virtual Machine Feature dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart. [May require restart for Windows 11] Download and install the Linux kernel update package. [https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi] If you are in a Windows 11 environment, you may have to set the default WSL version to 1. wsl --set-default-version 1. Download and install Ubuntu Linux distribution from Microsoft Store. https://aka.ms/wslstore Search for Ubuntu and click on \u2018Get\u2019. This will download and install Ubuntu terminal environment with WSL. Once installed Click on \u2018Open\u2019. The first time, it will take some time to decompress some files. Wait for this step to complete. At the end, you\u2019ll be asked to create a username and password to be used to login. Finally, you will be in the Linux shell prompt. Subsequently, open a CMD (or PowerShell) terminal and enter \u2018bash\u2019 to get access to the Linux shell. Setup a few tools in the Ubuntu system. Run these commands in the Ubuntu Shell screen. Install dos2unix. sudo apt-get update. sudo apt-get install dos2unix. Install 'oc' CLI. wget https://downloads-openshift-console.apps.cody.coc-ibm.com/amd64/linux/oc.tar --no-check-certificate. tar -xvf oc.tar. sudo mv oc /usr/local/bin. Install 'make' sudo apt install make. Install 'zsh' shell. sudo apt install zsh. OCP access with CP4I installed, could be ROKS, TechZone with CP4I cluster, we are using CoC environment as a base for our deployments See environment section for your assigned cluster To access to the git repository click on the top right icon from the documentation page: ibm-cloud-architecture/eda-tech-academy Fork this repository to your own git account so you can modify content and deploy code from your repository when using GitOps. and then clone it to your local laptop: git clone https://github.com/ibm-cloud-architecture/eda-tech-academy/","title":"Pre-requisites - Windows Users"},{"location":"lab1/#environments_1","text":"We have two OpenShift clusters available with 25 userids each. Finn cluster is console-openshift-console.apps.finn.coc-ibm.com Cody cluster is console-openshift-console.apps.cody.coc-ibm.com Userids will be finn1 to finn25 for Finn cluster Userids will be cody1 to cody25 for the Cody cluster.","title":"Environments"},{"location":"lab1/#problem-statement","text":"Today, a lot of companies which are managing item / product inventory are facing real challenges to get a close to real-time view of item availability and global inventory view. The solution can be very complex to implement while integrating Enterprise Resource Planning products and other custom legacy systems. Acme customer is asking you to present how to address this problem using an event-driven architecture and cloud native microservices. Normally to do MVP, we propose to use an Event Storming workshop to discover the problem statment, the stakeholders, the business goals, and the business process from an event point of view. The image below is an example of the outcomes of such event storming session. A MVP will be complex to implement, but the customer wants first to select a technology, so a proof of concept needs to be done to guide them on how our product portfolio will address their problems. How to start?","title":"Problem statement"},{"location":"lab1/#exercise-1-system-design","text":"Giving the problem statement: How to design a near real-time inventory view of Stores, warehouses, scaling at millions of transactions per minutes? how do you design a solution, that can get visibility of the transactions, at scale, and give a near-real time consistent view of the items in store, cross stores and what do you propose to the customer as a proof of concept. Start from a white page, design components that you think will be relevant. Duration : 30 minutes Expected outcome A diagram illustrating the design. If you use Visual Code you can get the drawio plugin A list of questions you would like to see addressed","title":"Exercise 1: system design"},{"location":"lab1/#some-information-you-gathered-from-your-framing-meeting","text":"SAP is used to be the final system of record Company has multiple stores and warehouses Sale and restock transactions are in a TLOG format Stores and warehouses have local view of their own inventory on local servers and cash machines. Some inventory for warehouses and transactions are done in mainframe and shared with IBM MQ Item has code bar and SKU so people moving stock internally can scan item so local system may have visibility of where items are after some small latency. The volume of transactions is around 5 millions a day Deployment will be between 3 data centers to cover the territory the company work in. Customer has heard about kafka, but they use MQ today, CICS, Java EE applications on WebSphere Architect wants to deploy to the cloud and adopt flexible microservice architecture SAP system will stay in enterprise Architect wants to use data lake solution to let data scientists develop statistical and AI models. Guidances Think about enterprise network, cloud providers, stores and warehouses as sources. SAP - Hana is one of the system of record and will get data from it It is also possible to get data before it reaches SAP SAP has database tables that can be used as source of events As warehouse systems are using MQ, queue replication is something to think about What could be a PoC scope? You may want to demonstrate Event Streams capabilities as Kafka backbone You want to demonstrate MQ source connector to get message from MQ to Kafka You may want to explain how messages are published from a microservice using Microprofile Reactive Messaging The real-time aggregation to compute store inventory can be done with streaming processing, and Kafka Streams APIs can be used for that. For Data lake integration, you can illustrates messages landing in S3 buckets in IBM Cloud Object Storage using Kafka Sink connector. See the Solution","title":"Some information you gathered from your framing meeting"},{"location":"lab1/lab1-sol/","text":"Real-time inventory system design proposal \u00b6 The system design of this proof of concept should be simple, but not the global solution, and in the presale work we have to pitch a higher view of what the solution may look like so customer's feels confortable about how your proof of concept solution will fit in a bigger solution. Global solution view of real-time inventory \u00b6 The core principle, is that each components responsible of managing some inventory elements will push events about their own inventory updates to a central data hub, that will be used to update back ends, ERP, systems. This central event backbone also exposes data so it will be easy to plug and play streaming processing for computing different statistical aggregates. The following figure is such high level business view of what a solution looks like Servers in the stores and warehouses ( Store Server and Warehouse Server ) are sending sale transactions or item restock messages to a central messaging platform, where streaming components (the green components) are computing the different aggregates and are publishing those aggregation results to other topics. The architecture decision is to adopt loosely coupling component, asynchronous communication, being able to scale horizontally and be resilient. A pub/sub model is used, where components can start consuming messages long time after they were published, or immediately after. The system design is based on the lambda architecture, a classical data streaming pipeline architecture, as presented in IBM EDA reference architecture: Streaming applications work as consuming events, processing them and producing new events in different topic. Those applications will guaranty exaclty once delivery, so it will help for inventory consistency. IBM MQ and IBM Event Streams are used to support a shared message as a services architecture where any asynchronous communication between applications is supported. We want to use MQ to get messages from mainframe applications or MDB JEE apps. MQ Streaming Queue or queue to queue replication will help moving those messages to the OpenShift cluster. MQ Source kafka connector will be used to inject to Event Streams topic. A items topic will be used to receive transactions about items as a restock or sale events. Sink connectors, based on Kafka Connect framework, may be used to move data to long persistence storage like s3 bucket, datalake, Database,... or to integrate back to Legacy ERP systems. The Enterprise Network column includes the potential applications the data pipeline is integrated with. More information If you want to reuse the diagram the source is at this url EDA reference architecture presentation and argumentations Kappa architecture Demos / proof of concept view \u00b6 Now that you have positionned a high level view, for the proof of concept, you want to leverage the cloud pak for integration packaging, and use at least MQ, Kafka Connector, Event Streams, streaming application, and a sink to cloud object storage or AWS S3. You want to demonstrate MQ as a source for message coming from warehouses. Those messages are sent to Event Streams topic via kafka connector. You may need to simulate store or warehouse events and you can use simple json files for that and the kafka-console-producer.sh shell you can get from any Kafka deployment. You can also leverage the Store Simulator asset that can send messages to MQ, Kafka or RabbitMQ, random messages, or controlled scenario. This is what we will use in lab 3. You will deploy a kafka connector cluster with MQ source connector and different sink connector like elastic search or cloud object storage bucket. The following figure illustrates what a proof of concept may look like. Obviously you can reuse IBM demo for that: Next lab will teach you how to develop the store inventory with Kafka Streams to compute item inventory cross stores. More information The store simulator application is a Reactive Messaging Java app, which generates item sales to different possible messaging middlewares ( RabbitMQ, MQ or directly to Kafka). The code of this application is in this https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator. If you want to browse the code, the main readme of this project includes how to package and run this app with docker compose, and explains how the code works. The docker image is quay.io/ibmcase/eda-store-simulator/ The item inventory aggregator is a Kafka Stream application, done with Kafka Stream API. The source code is in the refarch-eda-item-inventory project . Consider this more as a black box in the context of the scenario, it consumes items events, aggregate them, expose APIs on top of Kafka Streams interactive queries and publish inventory events on item.inventory topic. The store inventory aggregator is a Kafka Stream application, done with Kafka Stream API. The source code is in the refarch-eda-store-inventory project the output is in store.inventory topic.","title":"Solution Overview"},{"location":"lab1/lab1-sol/#real-time-inventory-system-design-proposal","text":"The system design of this proof of concept should be simple, but not the global solution, and in the presale work we have to pitch a higher view of what the solution may look like so customer's feels confortable about how your proof of concept solution will fit in a bigger solution.","title":"Real-time inventory system design proposal"},{"location":"lab1/lab1-sol/#global-solution-view-of-real-time-inventory","text":"The core principle, is that each components responsible of managing some inventory elements will push events about their own inventory updates to a central data hub, that will be used to update back ends, ERP, systems. This central event backbone also exposes data so it will be easy to plug and play streaming processing for computing different statistical aggregates. The following figure is such high level business view of what a solution looks like Servers in the stores and warehouses ( Store Server and Warehouse Server ) are sending sale transactions or item restock messages to a central messaging platform, where streaming components (the green components) are computing the different aggregates and are publishing those aggregation results to other topics. The architecture decision is to adopt loosely coupling component, asynchronous communication, being able to scale horizontally and be resilient. A pub/sub model is used, where components can start consuming messages long time after they were published, or immediately after. The system design is based on the lambda architecture, a classical data streaming pipeline architecture, as presented in IBM EDA reference architecture: Streaming applications work as consuming events, processing them and producing new events in different topic. Those applications will guaranty exaclty once delivery, so it will help for inventory consistency. IBM MQ and IBM Event Streams are used to support a shared message as a services architecture where any asynchronous communication between applications is supported. We want to use MQ to get messages from mainframe applications or MDB JEE apps. MQ Streaming Queue or queue to queue replication will help moving those messages to the OpenShift cluster. MQ Source kafka connector will be used to inject to Event Streams topic. A items topic will be used to receive transactions about items as a restock or sale events. Sink connectors, based on Kafka Connect framework, may be used to move data to long persistence storage like s3 bucket, datalake, Database,... or to integrate back to Legacy ERP systems. The Enterprise Network column includes the potential applications the data pipeline is integrated with. More information If you want to reuse the diagram the source is at this url EDA reference architecture presentation and argumentations Kappa architecture","title":"Global solution view of real-time inventory"},{"location":"lab1/lab1-sol/#demos-proof-of-concept-view","text":"Now that you have positionned a high level view, for the proof of concept, you want to leverage the cloud pak for integration packaging, and use at least MQ, Kafka Connector, Event Streams, streaming application, and a sink to cloud object storage or AWS S3. You want to demonstrate MQ as a source for message coming from warehouses. Those messages are sent to Event Streams topic via kafka connector. You may need to simulate store or warehouse events and you can use simple json files for that and the kafka-console-producer.sh shell you can get from any Kafka deployment. You can also leverage the Store Simulator asset that can send messages to MQ, Kafka or RabbitMQ, random messages, or controlled scenario. This is what we will use in lab 3. You will deploy a kafka connector cluster with MQ source connector and different sink connector like elastic search or cloud object storage bucket. The following figure illustrates what a proof of concept may look like. Obviously you can reuse IBM demo for that: Next lab will teach you how to develop the store inventory with Kafka Streams to compute item inventory cross stores. More information The store simulator application is a Reactive Messaging Java app, which generates item sales to different possible messaging middlewares ( RabbitMQ, MQ or directly to Kafka). The code of this application is in this https://github.com/ibm-cloud-architecture/refarch-eda-store-simulator. If you want to browse the code, the main readme of this project includes how to package and run this app with docker compose, and explains how the code works. The docker image is quay.io/ibmcase/eda-store-simulator/ The item inventory aggregator is a Kafka Stream application, done with Kafka Stream API. The source code is in the refarch-eda-item-inventory project . Consider this more as a black box in the context of the scenario, it consumes items events, aggregate them, expose APIs on top of Kafka Streams interactive queries and publish inventory events on item.inventory topic. The store inventory aggregator is a Kafka Stream application, done with Kafka Stream API. The source code is in the refarch-eda-store-inventory project the output is in store.inventory topic.","title":"Demos / proof of concept view"},{"location":"lab2/","text":"Lab 2: Store inventory with Kafka Streams \u00b6 In this Lab, you will learn simple Kafka Streams exercises using Java APIs, and then finish by implementing on of the proof of concept component to compute the store inventory for each items sold. Pre-requisites \u00b6 This lab requires some java experience. Be sure to have completed the pre-requisites listed here Be sure your IDE, like VScode has java extension. For VSCode you can use Red Hat java extension maven is used to package and run some test. The project has ./mvnw command that brings maven in the project. Open Visual Code. Click on \u201cOpen Java Project\u201d and pick the refarch-eda-store-inventory from the eda-tech-academy\\lab2\\ folder From there, select / modify the java code and run it from Visual Code. Context \u00b6 The figure illustrates what you need to build, the green rectangle, which is a Java application using Kafka Streams API consuming items events and computing store inventory events by aggregating at the store level. In fact, most of the code is already done for you, and the lab is to help you building event streaming knowledge by increment. At the end of this training you should be able to reuse this approach in your future work. All the development is done by running on your local computer, not need to deploy to OpenShift. In the global view of the real-time inventory solution presented in the lab 1, the goal is to do the store aggregation processing. Step by step exercises \u00b6 Before implementing the store inventory, you need to learn Kafka streams programming with a set of simple exercises. Review the Kafka Streams API concepts and basic APIs here . To do the hands-on exerices, go to the /lab2/refarch-eda-store-inventory folder of this repository, load this folder into your IDE. Be sure you can run tests inside the IDE, it may be easier to debug. Exercise 1: Test your first topology \u00b6 Duration: 10 minutes Goal: Test a basic topology inside unit test. In your IDE go to the src/test/java/ut folder and open the class: TestYourFirstTopology.java . The most important elements of this test are: The TopologyTestDriver is a class to test a Kafka Streams topology without Kafka. It uses dummy configuration. the TestInputTopic class is used to pipe test records to a topic in TopologyTestDriver. This is used to send test messages. The creation of this instance needs to specify the serializer or deserialer used. The @BeforeAll annotation on the setup() method means that it will be run before any test is executed, while the @AfterAll annotation on the teardown method ensures that it will be run after last test execution. We use this approach to define the topology under test in this class. Build test driver and topics import org.apache.kafka.streams.StreamsConfig ; import org.apache.kafka.streams.TestInputTopic ; import org.apache.kafka.streams.TestOutputTopic ; import org.apache.kafka.streams.TopologyTestDriver ; import org.apache.kafka.common.serialization.StringDeserializer ; import org.apache.kafka.common.serialization.StringSerializer ; //... Topology topology = buildTopologyFlow (); testDriver = new TopologyTestDriver ( topology , getStreamsConfig ()); inTopic = testDriver . createInputTopic ( inTopicName , new StringSerializer (), new StringSerializer ()); outTopic = testDriver . createOutputTopic ( outTopicName , new StringDeserializer (), new StringDeserializer ()); The buildTopology method utilizes the StreamsBuilder class to construct a simple Kafka Streams topology, reading from the input Kafka topic defined by the inTopicName String. The logic is simply to print the message, the filter on a blue value, and generates the output to a topic: buildTopology(): Filter text message topology KStream < String , String > basicColors = builder . stream ( inTopicName , Consumed . with ( Serdes . String (), Serdes . String ())); basicColors . peek (( key , value ) -> System . out . println ( \"PRE-FILTER: key=\" + key + \", value=\" + value )) // (1) . filter (( key , value ) -> ( \"BLUE\" . equalsIgnoreCase ( value ))) // (2) . peek (( key , value ) -> System . out . println ( \"POST-FILTER: key=\" + key + \", value=\" + value )) . to ( outTopicName ); // (3) Peek() to get the message, apply a lambda function and do nothing else. Used for printing filter() to select records. So here there is a transformation of the input kstream and output kstream to() is to output the stream to a topic. Run this test you should see the following output in the Debug Console. If your IDE is Visual Studio Code, click the \"Run Tests\" icon right of TestYourFirstTopology on the explorer. . The first part is a print of the defined topology and then the execution of the topology on the different input records. Topologies: Sub-topology: 0 Source: KSTREAM-SOURCE-0000000000 ( topics: [ my-input-topic ]) --> KSTREAM-PEEK-0000000001 Processor: KSTREAM-PEEK-0000000001 ( stores: []) --> KSTREAM-FILTER-0000000002 <-- KSTREAM-SOURCE-0000000000 Processor: KSTREAM-FILTER-0000000002 ( stores: []) --> KSTREAM-PEEK-0000000003 <-- KSTREAM-PEEK-0000000001 Processor: KSTREAM-PEEK-0000000003 ( stores: []) --> KSTREAM-SINK-0000000004 <-- KSTREAM-FILTER-0000000002 Sink: KSTREAM-SINK-0000000004 ( topic: my-output-topic ) <-- KSTREAM-PEEK-0000000003 [ or.ap.ka.st.pr.in.StreamTask ] ( main ) stream-thread [ main ] task [ 0_0 ] Initialized [ or.ap.ka.st.pr.in.StreamTask ] ( main ) stream-thread [ main ] task [ 0_0 ] Restored and ready to run PRE-FILTER: key = C01, value = blue POST-FILTER: key = C01, value = blue PRE-FILTER: key = C02, value = red PRE-FILTER: key = C03, value = green PRE-FILTER: key = C04, value = Blue POST-FILTER: key = C04, value = Blue PRE-FILTER: key = C01, value = blue POST-FILTER: key = C01, value = blue [ or.ap.ka.st.pr.in.StreamTask ] ( main ) stream-thread [ main ] task [ 0_0 ] Suspended running [ or.ap.ka.st.pr.in.RecordCollectorImpl ] ( main ) topology-test-driver Closing record collector clean [ or.ap.ka.st.pr.in.StreamTask ] ( main ) stream-thread [ main ] task [ 0_0 ] Closed clean See comments in test class for more information. May be you can play with the data, and change the filter condition. Exercise 2: Filter item transaction without a store \u00b6 Problem: Given item sold transactions, keep transactions with store and sku information populated. Try to do a topology taking the ItemTransaction class as the code defining the content from the input topic. The following code does not need to be copied/paste. It is here to explain you the structure of the data. The class is in the classpath, so tests run in maven or in the IDE will work. public class ItemTransaction { public static String RESTOCK = \"RESTOCK\" ; public static String SALE = \"SALE\" ; public Long id ; public String storeName ; public String sku ; public int quantity ; public String type ; public Double price ; public String timestamp ; So the Stream Topology needs to use the filter() function to select items with valid sku and storeName. You should use the TestSecondTopology test class , and implement the Kafka Streams topology in the buildTopologyFlow() method: Build filtering public static Topology buildTopologyFlow (){ final StreamsBuilder builder = new StreamsBuilder (); // 1- get the input stream // 2 filter // Generate to output topic return builder . build (); } In the first exercice, we used String Deserializer. This time we need Java Bean serializer/deserializer. This is a classical task in any Kafka Streams projects to have to define such serdes. The Serialization and Deserialization are defined in the StoreSerdes.class which uses a JSON generic class based on Jackson parser. You should be able to reuse this class in all your project. Do not copy / past the following code. This is just for information so you can see the relationship between Serdes and the Java bean they are supposed to deserialize. JSONSerdes import org.apache.kafka.common.serialization.Deserializer ; import org.apache.kafka.common.serialization.Serde ; import org.apache.kafka.common.serialization.Serializer ; public class JSONSerde < T > implements Serializer < T > , Deserializer < T > , Serde < T > { private static final ObjectMapper OBJECT_MAPPER = new ObjectMapper (); private String className ; public JSONSerde ( String cn ){ this . className = cn ; } //.. An example of specific usage for this class is the StoreSerdes class. public class StoreSerdes { public static Serde < ItemTransaction > ItemTransactionSerde () { return new JSONSerde < ItemTransaction > ( ItemTransaction . class . getCanonicalName ()); } // .. } Using the StoreSerdes, the test topic declarations in the TestCase class, are using the serdes like: setup method // Key of the record is a String, value is a ItemTransaction inputTopic = testDriver . createInputTopic ( inTopicName , new StringSerializer (), StoreSerdes . ItemTransactionSerde (). serializer ()); outputTopic = testDriver . createOutputTopic ( outTopicName , new StringDeserializer (), StoreSerdes . ItemTransactionSerde (). deserializer ()); Now some guidances of what the Stream topology should look like: Think to build a Kstream from the input stream The record should have key and value use filter and predicate to test if value.storeName has no value or value.sku is empty or null then drop the message generate output to a topic Think to chain the functions to get accurate result Use Test Driven Development to build tests before the topology, but here tests are already defined for you. Happy Path Test @Test public void sendValidRecord (){ ItemTransaction item = new ItemTransaction ( \"Store-1\" , \"Item-1\" , ItemTransaction . RESTOCK , 5 , 33.2 ); inputTopic . pipeInput ( item . storeName , item ); assertThat ( outputTopic . getQueueSize (), equalTo ( 1L ) ); ItemTransaction filteredItem = outputTopic . readValue (); assertThat ( filteredItem . storeName , equalTo ( \"Store-1\" )); assertThat ( filteredItem . sku , equalTo ( \"Item-1\" )); } Here is a typical trace: the id of the transaction is a timestamp as long, so those values will change. Expected execution trace Topologies : Sub - topology : 0 Source : KSTREAM - SOURCE - 0000000000 ( topics : [ my - input - topic ] ) --> KSTREAM - PEEK - 0000000001 Processor : KSTREAM - PEEK - 0000000001 ( stores : [] ) --> KSTREAM - FILTER - 0000000002 <-- KSTREAM - SOURCE - 0000000000 Processor : KSTREAM - FILTER - 0000000002 ( stores : [] ) --> KSTREAM - PEEK - 0000000003 <-- KSTREAM - PEEK - 0000000001 Processor : KSTREAM - PEEK - 0000000003 ( stores : [] ) --> KSTREAM - SINK - 0000000004 <-- KSTREAM - FILTER - 0000000002 Sink : KSTREAM - SINK - 0000000004 ( topic : my - output - topic ) <-- KSTREAM - PEEK - 0000000003 .... PRE - FILTER : key = Store - 1 , value = { id : 1653431482009 Store : Store - 1 Item : null Type : RESTOCK Quantity : 5 } PRE - FILTER : key = Store - 1 , value = { id : 1653431482107 Store : Store - 1 Item : Item - 1 Type : RESTOCK Quantity : 5 } POST - FILTER : key = Store - 1 , value = { id : 1653431482107 Store : Store - 1 Item : Item - 1 Type : RESTOCK Quantity : 5 } PRE - FILTER : key = , value = { id : 1653431482114 Store : Item : Item - 1 Type : RESTOCK Quantity : 5 } PRE - FILTER : key = Store - 1 , value = { id : 1653431482116 Store : Store - 1 Item : Type : RESTOCK Quantity : 5 } PRE - FILTER : key = null , value = { id : 1653431482118 Store : null Item : Item - 1 Type : RESTOCK Quantity : 5 } Solution The topology looks like KStream < String , ItemTransaction > items = builder . stream ( inTopicName , Consumed . with ( Serdes . String (), StoreSerdes . ItemTransactionSerde ())); items . peek (( key , value ) -> System . out . println ( \"PRE-FILTER: key=\" + key + \", value= {\" + value + \"}\" )) . filter (( k , v ) -> ( v . storeName != null && ! v . storeName . isEmpty () && v . sku != null && ! v . sku . isEmpty ())) . peek (( key , value ) -> System . out . println ( \"POST-FILTER: key=\" + key + \", value= {\" + value + \"}\" )) . to ( outTopicName ); Exercise 3: Dead letter topic \u00b6 Problem: from the previous example, we would like to route the messages with errors to a dead letter topic, for future processing. May be a human intervention? The dead letter topic is a classical integration pattern, and when dealing with Kafka and streaming it looks like: Transform the previous topology to support branches and routing the records in error to a dead letter topic. You should use the TestDeadLetterTopic test class, and implement the Kafka Streams topology in the buildTopologyFlow() method: Some guidances: Define a dead letter topic Create a [StreamsBuilder]: Builder for Kafka Streams topology Create a KStream with key as string and value as ItemTransaction Use the concept of branches by splitting the input stream Use lambda function for testing condition on sku and storeName attributes Output the branches to topics Solution Dead letter topic declaration private static TestOutputTopic < String , ItemTransaction > dlTopic ; private static String deadLetterTopicName = \"dl-topic\" ; Define topology final StreamsBuilder builder = new StreamsBuilder (); // 1- get the input stream KStream < String , ItemTransaction > items = builder . stream ( inTopicName , Consumed . with ( Serdes . String (), StoreSerdes . ItemTransactionSerde ())); // 2 build branches Map < String , KStream < String , ItemTransaction >> branches = items . split ( Named . as ( \"B-\" )) . branch (( k , v ) -> ( v . storeName == null || v . storeName . isEmpty () || v . sku == null || v . sku . isEmpty ()), Branched . as ( \"wrong-tx\" ) ). defaultBranch ( Branched . as ( \"good-tx\" )); // Generate to output topic branches . get ( \"B-good-tx\" ). to ( outTopicName ); branches . get ( \"B-wrong-tx\" ). to ( deadLetterTopicName ); Define output topic in setup() dlTopic = testDriver . createOutputTopic ( deadLetterTopicName , new StringDeserializer (), StoreSerdes . ItemTransactionSerde (). deserializer ()); Exercice 4 - Using Ktable \u00b6 Problem: now you want to group record in table and keep last item transaction per store Input examples: { \"id\" : 9 , \"price\" : 30.0 , \"quantity\" : 10 , \"sku\" : \"Item_1\" , \"storeName\" : \"Store_1\" , \"timestamp\" : \"2022-05-19T16:22:37.287937\" , \"type\" : \"RESTOCK\" } { \"id\" : 6 , \"price\" : 10.0 , \"quantity\" : 5 , \"sku\" : \"Item_3\" , \"storeName\" : \"Store_1\" , \"timestamp\" : \"2022-05-12T23:13:31.338671\" , \"type\" : \"SALE\" } Expected result in a Ktable - store: Item_3 Guidances The input topic is keyed by the SKU so we need to rekey to store name: inputTopic . pipeInput ( item . sku , item ); Create a Kstream from the items stream use map to generate new KeyValue with key being the storeName Create a Ktable from the output of the map Use materialized view to keep data in a state store (KeyValueStore) Solution The topology looks like KStream < String , ItemTransaction > items = builder . stream ( inTopicName , Consumed . with ( Serdes . String (), StoreSerdes . ItemTransactionSerde ())); KTable < String , ItemTransaction > lastItemInStore = items . map (( k , v ) -> { return new KeyValue < String , ItemTransaction > ( v . storeName , v ); }). toTable ( Materialized . < String , ItemTransaction , KeyValueStore < Bytes , byte []>> as ( storeName ) . withKeySerde ( Serdes . String ()). withValueSerde ( StoreSerdes . ItemTransactionSerde ())); The trace for the topology: Topologies: Sub-topology: 0 Source: KSTREAM-SOURCE-0000000000 (topics: [my-input-topic]) --> KSTREAM-MAP-0000000001 Processor: KSTREAM-MAP-0000000001 (stores: []) --> KSTREAM-FILTER-0000000004 <-- KSTREAM-SOURCE-0000000000 Processor: KSTREAM-FILTER-0000000004 (stores: []) --> KSTREAM-SINK-0000000003 <-- KSTREAM-MAP-0000000001 Sink: KSTREAM-SINK-0000000003 (topic: KSTREAM-TOTABLE-0000000002-repartition) <-- KSTREAM-FILTER-0000000004 Sub-topology: 1 Source: KSTREAM-SOURCE-0000000005 (topics: [KSTREAM-TOTABLE-0000000002-repartition]) --> KSTREAM-TOTABLE-0000000002 Processor: KSTREAM-TOTABLE-0000000002 (stores: [ItemTable]) --> none <-- KSTREAM-SOURCE-0000000005 More examples \u00b6 For your information here are some more test classes that demonstrate some streaming examples: TestAccumulateItemSold to demonstrate counting item sold event per sku. It groups by sku, and then count and emit events. Without Ktable caching the sequence of output records is emitted for key that represent changes in the resulting aggregation table. KStream < String , ItemTransaction > items = builder . stream ( inTopicName , Consumed . with ( Serdes . String (), StoreSerdes . ItemTransactionSerde ())); // 2- to compute aggregate we need to group records by key to create KGroupTable or stream // here we group the records by their current key into a KGroupedStream KTable < String , Long > countedItems = items . filter (( k , v ) -> ItemTransaction . SALE . equals ( v . type )) . groupByKey () // 3- change the stream type from KGroupedStream<String, ItemTransaction> to KTable<String, Long> . count (); // Generate to output topic countedItems . toStream (). to ( outTopicName ); Final store inventory exercice \u00b6 As decided during the system design, you need now to implement the proof of concept around streaming restock or sale transaction. Problem statement \u00b6 How to get the real-time view of the inventory per store? The ItemTransaction.java represents the message structure of the input topic. Below is an extract of this definition: Here is an example of json message you may see in a topic: { \"id\" : 9 , \"price\" : 30.0 , \"quantity\" : 10 , \"sku\" : \"Item_1\" , \"storeName\" : \"Store_5\" , \"timestamp\" : \"2022-05-19T16:22:37.287937\" , \"type\" : \"RESTOCK\" } { \"id\" : 6 , \"price\" : 10.0 , \"quantity\" : 5 , \"sku\" : \"Item_3\" , \"storeName\" : \"Store_1\" , \"timestamp\" : \"2022-05-12T23:13:31.338671\" , \"type\" : \"SALE\" } We need to compute aggregate for each store level. For example the outcome may look like: { \"storeName\" : \"Store_1\" , \"stock\" :{ \"Item_3\" : 20 , \"Item_2\" : 0 , \"Item_1\" : 20 }} Events could be RESTOCK or SALE. The type attribute defines this. sku represents the item identifier. The input topic is items and the output topic is store.inventory . We assume ItemTransaction fields are all present, so the streaming logic does not need to assess data quality. Design questions \u00b6 What is the data model you need to use to keep store inventory? What event will be produced to the store.inventory topic? We need to process 5 million messages per day. Day is from 6:00 am to 10 pm every day. Some hints Define a New class to support keeping Store and a Map for items and current stock value. This class will be the event structure to get to the output topic Use a method to update the quantity of an existing inventory instance with a new retrieved event Use KStream aggregate function to create new entry and update existing store entry Solution review and code explanation>>","title":"Lab 2 exercises"},{"location":"lab2/#lab-2-store-inventory-with-kafka-streams","text":"In this Lab, you will learn simple Kafka Streams exercises using Java APIs, and then finish by implementing on of the proof of concept component to compute the store inventory for each items sold.","title":"Lab 2: Store inventory with Kafka Streams"},{"location":"lab2/#pre-requisites","text":"This lab requires some java experience. Be sure to have completed the pre-requisites listed here Be sure your IDE, like VScode has java extension. For VSCode you can use Red Hat java extension maven is used to package and run some test. The project has ./mvnw command that brings maven in the project. Open Visual Code. Click on \u201cOpen Java Project\u201d and pick the refarch-eda-store-inventory from the eda-tech-academy\\lab2\\ folder From there, select / modify the java code and run it from Visual Code.","title":"Pre-requisites"},{"location":"lab2/#context","text":"The figure illustrates what you need to build, the green rectangle, which is a Java application using Kafka Streams API consuming items events and computing store inventory events by aggregating at the store level. In fact, most of the code is already done for you, and the lab is to help you building event streaming knowledge by increment. At the end of this training you should be able to reuse this approach in your future work. All the development is done by running on your local computer, not need to deploy to OpenShift. In the global view of the real-time inventory solution presented in the lab 1, the goal is to do the store aggregation processing.","title":"Context"},{"location":"lab2/#step-by-step-exercises","text":"Before implementing the store inventory, you need to learn Kafka streams programming with a set of simple exercises. Review the Kafka Streams API concepts and basic APIs here . To do the hands-on exerices, go to the /lab2/refarch-eda-store-inventory folder of this repository, load this folder into your IDE. Be sure you can run tests inside the IDE, it may be easier to debug.","title":"Step by step exercises"},{"location":"lab2/#exercise-1-test-your-first-topology","text":"Duration: 10 minutes Goal: Test a basic topology inside unit test. In your IDE go to the src/test/java/ut folder and open the class: TestYourFirstTopology.java . The most important elements of this test are: The TopologyTestDriver is a class to test a Kafka Streams topology without Kafka. It uses dummy configuration. the TestInputTopic class is used to pipe test records to a topic in TopologyTestDriver. This is used to send test messages. The creation of this instance needs to specify the serializer or deserialer used. The @BeforeAll annotation on the setup() method means that it will be run before any test is executed, while the @AfterAll annotation on the teardown method ensures that it will be run after last test execution. We use this approach to define the topology under test in this class. Build test driver and topics import org.apache.kafka.streams.StreamsConfig ; import org.apache.kafka.streams.TestInputTopic ; import org.apache.kafka.streams.TestOutputTopic ; import org.apache.kafka.streams.TopologyTestDriver ; import org.apache.kafka.common.serialization.StringDeserializer ; import org.apache.kafka.common.serialization.StringSerializer ; //... Topology topology = buildTopologyFlow (); testDriver = new TopologyTestDriver ( topology , getStreamsConfig ()); inTopic = testDriver . createInputTopic ( inTopicName , new StringSerializer (), new StringSerializer ()); outTopic = testDriver . createOutputTopic ( outTopicName , new StringDeserializer (), new StringDeserializer ()); The buildTopology method utilizes the StreamsBuilder class to construct a simple Kafka Streams topology, reading from the input Kafka topic defined by the inTopicName String. The logic is simply to print the message, the filter on a blue value, and generates the output to a topic: buildTopology(): Filter text message topology KStream < String , String > basicColors = builder . stream ( inTopicName , Consumed . with ( Serdes . String (), Serdes . String ())); basicColors . peek (( key , value ) -> System . out . println ( \"PRE-FILTER: key=\" + key + \", value=\" + value )) // (1) . filter (( key , value ) -> ( \"BLUE\" . equalsIgnoreCase ( value ))) // (2) . peek (( key , value ) -> System . out . println ( \"POST-FILTER: key=\" + key + \", value=\" + value )) . to ( outTopicName ); // (3) Peek() to get the message, apply a lambda function and do nothing else. Used for printing filter() to select records. So here there is a transformation of the input kstream and output kstream to() is to output the stream to a topic. Run this test you should see the following output in the Debug Console. If your IDE is Visual Studio Code, click the \"Run Tests\" icon right of TestYourFirstTopology on the explorer. . The first part is a print of the defined topology and then the execution of the topology on the different input records. Topologies: Sub-topology: 0 Source: KSTREAM-SOURCE-0000000000 ( topics: [ my-input-topic ]) --> KSTREAM-PEEK-0000000001 Processor: KSTREAM-PEEK-0000000001 ( stores: []) --> KSTREAM-FILTER-0000000002 <-- KSTREAM-SOURCE-0000000000 Processor: KSTREAM-FILTER-0000000002 ( stores: []) --> KSTREAM-PEEK-0000000003 <-- KSTREAM-PEEK-0000000001 Processor: KSTREAM-PEEK-0000000003 ( stores: []) --> KSTREAM-SINK-0000000004 <-- KSTREAM-FILTER-0000000002 Sink: KSTREAM-SINK-0000000004 ( topic: my-output-topic ) <-- KSTREAM-PEEK-0000000003 [ or.ap.ka.st.pr.in.StreamTask ] ( main ) stream-thread [ main ] task [ 0_0 ] Initialized [ or.ap.ka.st.pr.in.StreamTask ] ( main ) stream-thread [ main ] task [ 0_0 ] Restored and ready to run PRE-FILTER: key = C01, value = blue POST-FILTER: key = C01, value = blue PRE-FILTER: key = C02, value = red PRE-FILTER: key = C03, value = green PRE-FILTER: key = C04, value = Blue POST-FILTER: key = C04, value = Blue PRE-FILTER: key = C01, value = blue POST-FILTER: key = C01, value = blue [ or.ap.ka.st.pr.in.StreamTask ] ( main ) stream-thread [ main ] task [ 0_0 ] Suspended running [ or.ap.ka.st.pr.in.RecordCollectorImpl ] ( main ) topology-test-driver Closing record collector clean [ or.ap.ka.st.pr.in.StreamTask ] ( main ) stream-thread [ main ] task [ 0_0 ] Closed clean See comments in test class for more information. May be you can play with the data, and change the filter condition.","title":"Exercise 1: Test your first topology"},{"location":"lab2/#exercise-2-filter-item-transaction-without-a-store","text":"Problem: Given item sold transactions, keep transactions with store and sku information populated. Try to do a topology taking the ItemTransaction class as the code defining the content from the input topic. The following code does not need to be copied/paste. It is here to explain you the structure of the data. The class is in the classpath, so tests run in maven or in the IDE will work. public class ItemTransaction { public static String RESTOCK = \"RESTOCK\" ; public static String SALE = \"SALE\" ; public Long id ; public String storeName ; public String sku ; public int quantity ; public String type ; public Double price ; public String timestamp ; So the Stream Topology needs to use the filter() function to select items with valid sku and storeName. You should use the TestSecondTopology test class , and implement the Kafka Streams topology in the buildTopologyFlow() method: Build filtering public static Topology buildTopologyFlow (){ final StreamsBuilder builder = new StreamsBuilder (); // 1- get the input stream // 2 filter // Generate to output topic return builder . build (); } In the first exercice, we used String Deserializer. This time we need Java Bean serializer/deserializer. This is a classical task in any Kafka Streams projects to have to define such serdes. The Serialization and Deserialization are defined in the StoreSerdes.class which uses a JSON generic class based on Jackson parser. You should be able to reuse this class in all your project. Do not copy / past the following code. This is just for information so you can see the relationship between Serdes and the Java bean they are supposed to deserialize. JSONSerdes import org.apache.kafka.common.serialization.Deserializer ; import org.apache.kafka.common.serialization.Serde ; import org.apache.kafka.common.serialization.Serializer ; public class JSONSerde < T > implements Serializer < T > , Deserializer < T > , Serde < T > { private static final ObjectMapper OBJECT_MAPPER = new ObjectMapper (); private String className ; public JSONSerde ( String cn ){ this . className = cn ; } //.. An example of specific usage for this class is the StoreSerdes class. public class StoreSerdes { public static Serde < ItemTransaction > ItemTransactionSerde () { return new JSONSerde < ItemTransaction > ( ItemTransaction . class . getCanonicalName ()); } // .. } Using the StoreSerdes, the test topic declarations in the TestCase class, are using the serdes like: setup method // Key of the record is a String, value is a ItemTransaction inputTopic = testDriver . createInputTopic ( inTopicName , new StringSerializer (), StoreSerdes . ItemTransactionSerde (). serializer ()); outputTopic = testDriver . createOutputTopic ( outTopicName , new StringDeserializer (), StoreSerdes . ItemTransactionSerde (). deserializer ()); Now some guidances of what the Stream topology should look like: Think to build a Kstream from the input stream The record should have key and value use filter and predicate to test if value.storeName has no value or value.sku is empty or null then drop the message generate output to a topic Think to chain the functions to get accurate result Use Test Driven Development to build tests before the topology, but here tests are already defined for you. Happy Path Test @Test public void sendValidRecord (){ ItemTransaction item = new ItemTransaction ( \"Store-1\" , \"Item-1\" , ItemTransaction . RESTOCK , 5 , 33.2 ); inputTopic . pipeInput ( item . storeName , item ); assertThat ( outputTopic . getQueueSize (), equalTo ( 1L ) ); ItemTransaction filteredItem = outputTopic . readValue (); assertThat ( filteredItem . storeName , equalTo ( \"Store-1\" )); assertThat ( filteredItem . sku , equalTo ( \"Item-1\" )); } Here is a typical trace: the id of the transaction is a timestamp as long, so those values will change. Expected execution trace Topologies : Sub - topology : 0 Source : KSTREAM - SOURCE - 0000000000 ( topics : [ my - input - topic ] ) --> KSTREAM - PEEK - 0000000001 Processor : KSTREAM - PEEK - 0000000001 ( stores : [] ) --> KSTREAM - FILTER - 0000000002 <-- KSTREAM - SOURCE - 0000000000 Processor : KSTREAM - FILTER - 0000000002 ( stores : [] ) --> KSTREAM - PEEK - 0000000003 <-- KSTREAM - PEEK - 0000000001 Processor : KSTREAM - PEEK - 0000000003 ( stores : [] ) --> KSTREAM - SINK - 0000000004 <-- KSTREAM - FILTER - 0000000002 Sink : KSTREAM - SINK - 0000000004 ( topic : my - output - topic ) <-- KSTREAM - PEEK - 0000000003 .... PRE - FILTER : key = Store - 1 , value = { id : 1653431482009 Store : Store - 1 Item : null Type : RESTOCK Quantity : 5 } PRE - FILTER : key = Store - 1 , value = { id : 1653431482107 Store : Store - 1 Item : Item - 1 Type : RESTOCK Quantity : 5 } POST - FILTER : key = Store - 1 , value = { id : 1653431482107 Store : Store - 1 Item : Item - 1 Type : RESTOCK Quantity : 5 } PRE - FILTER : key = , value = { id : 1653431482114 Store : Item : Item - 1 Type : RESTOCK Quantity : 5 } PRE - FILTER : key = Store - 1 , value = { id : 1653431482116 Store : Store - 1 Item : Type : RESTOCK Quantity : 5 } PRE - FILTER : key = null , value = { id : 1653431482118 Store : null Item : Item - 1 Type : RESTOCK Quantity : 5 } Solution The topology looks like KStream < String , ItemTransaction > items = builder . stream ( inTopicName , Consumed . with ( Serdes . String (), StoreSerdes . ItemTransactionSerde ())); items . peek (( key , value ) -> System . out . println ( \"PRE-FILTER: key=\" + key + \", value= {\" + value + \"}\" )) . filter (( k , v ) -> ( v . storeName != null && ! v . storeName . isEmpty () && v . sku != null && ! v . sku . isEmpty ())) . peek (( key , value ) -> System . out . println ( \"POST-FILTER: key=\" + key + \", value= {\" + value + \"}\" )) . to ( outTopicName );","title":"Exercise 2: Filter item transaction without a store"},{"location":"lab2/#exercise-3-dead-letter-topic","text":"Problem: from the previous example, we would like to route the messages with errors to a dead letter topic, for future processing. May be a human intervention? The dead letter topic is a classical integration pattern, and when dealing with Kafka and streaming it looks like: Transform the previous topology to support branches and routing the records in error to a dead letter topic. You should use the TestDeadLetterTopic test class, and implement the Kafka Streams topology in the buildTopologyFlow() method: Some guidances: Define a dead letter topic Create a [StreamsBuilder]: Builder for Kafka Streams topology Create a KStream with key as string and value as ItemTransaction Use the concept of branches by splitting the input stream Use lambda function for testing condition on sku and storeName attributes Output the branches to topics Solution Dead letter topic declaration private static TestOutputTopic < String , ItemTransaction > dlTopic ; private static String deadLetterTopicName = \"dl-topic\" ; Define topology final StreamsBuilder builder = new StreamsBuilder (); // 1- get the input stream KStream < String , ItemTransaction > items = builder . stream ( inTopicName , Consumed . with ( Serdes . String (), StoreSerdes . ItemTransactionSerde ())); // 2 build branches Map < String , KStream < String , ItemTransaction >> branches = items . split ( Named . as ( \"B-\" )) . branch (( k , v ) -> ( v . storeName == null || v . storeName . isEmpty () || v . sku == null || v . sku . isEmpty ()), Branched . as ( \"wrong-tx\" ) ). defaultBranch ( Branched . as ( \"good-tx\" )); // Generate to output topic branches . get ( \"B-good-tx\" ). to ( outTopicName ); branches . get ( \"B-wrong-tx\" ). to ( deadLetterTopicName ); Define output topic in setup() dlTopic = testDriver . createOutputTopic ( deadLetterTopicName , new StringDeserializer (), StoreSerdes . ItemTransactionSerde (). deserializer ());","title":"Exercise 3: Dead letter topic"},{"location":"lab2/#exercice-4-using-ktable","text":"Problem: now you want to group record in table and keep last item transaction per store Input examples: { \"id\" : 9 , \"price\" : 30.0 , \"quantity\" : 10 , \"sku\" : \"Item_1\" , \"storeName\" : \"Store_1\" , \"timestamp\" : \"2022-05-19T16:22:37.287937\" , \"type\" : \"RESTOCK\" } { \"id\" : 6 , \"price\" : 10.0 , \"quantity\" : 5 , \"sku\" : \"Item_3\" , \"storeName\" : \"Store_1\" , \"timestamp\" : \"2022-05-12T23:13:31.338671\" , \"type\" : \"SALE\" } Expected result in a Ktable - store: Item_3 Guidances The input topic is keyed by the SKU so we need to rekey to store name: inputTopic . pipeInput ( item . sku , item ); Create a Kstream from the items stream use map to generate new KeyValue with key being the storeName Create a Ktable from the output of the map Use materialized view to keep data in a state store (KeyValueStore) Solution The topology looks like KStream < String , ItemTransaction > items = builder . stream ( inTopicName , Consumed . with ( Serdes . String (), StoreSerdes . ItemTransactionSerde ())); KTable < String , ItemTransaction > lastItemInStore = items . map (( k , v ) -> { return new KeyValue < String , ItemTransaction > ( v . storeName , v ); }). toTable ( Materialized . < String , ItemTransaction , KeyValueStore < Bytes , byte []>> as ( storeName ) . withKeySerde ( Serdes . String ()). withValueSerde ( StoreSerdes . ItemTransactionSerde ())); The trace for the topology: Topologies: Sub-topology: 0 Source: KSTREAM-SOURCE-0000000000 (topics: [my-input-topic]) --> KSTREAM-MAP-0000000001 Processor: KSTREAM-MAP-0000000001 (stores: []) --> KSTREAM-FILTER-0000000004 <-- KSTREAM-SOURCE-0000000000 Processor: KSTREAM-FILTER-0000000004 (stores: []) --> KSTREAM-SINK-0000000003 <-- KSTREAM-MAP-0000000001 Sink: KSTREAM-SINK-0000000003 (topic: KSTREAM-TOTABLE-0000000002-repartition) <-- KSTREAM-FILTER-0000000004 Sub-topology: 1 Source: KSTREAM-SOURCE-0000000005 (topics: [KSTREAM-TOTABLE-0000000002-repartition]) --> KSTREAM-TOTABLE-0000000002 Processor: KSTREAM-TOTABLE-0000000002 (stores: [ItemTable]) --> none <-- KSTREAM-SOURCE-0000000005","title":"Exercice 4 - Using Ktable"},{"location":"lab2/#more-examples","text":"For your information here are some more test classes that demonstrate some streaming examples: TestAccumulateItemSold to demonstrate counting item sold event per sku. It groups by sku, and then count and emit events. Without Ktable caching the sequence of output records is emitted for key that represent changes in the resulting aggregation table. KStream < String , ItemTransaction > items = builder . stream ( inTopicName , Consumed . with ( Serdes . String (), StoreSerdes . ItemTransactionSerde ())); // 2- to compute aggregate we need to group records by key to create KGroupTable or stream // here we group the records by their current key into a KGroupedStream KTable < String , Long > countedItems = items . filter (( k , v ) -> ItemTransaction . SALE . equals ( v . type )) . groupByKey () // 3- change the stream type from KGroupedStream<String, ItemTransaction> to KTable<String, Long> . count (); // Generate to output topic countedItems . toStream (). to ( outTopicName );","title":"More examples"},{"location":"lab2/#final-store-inventory-exercice","text":"As decided during the system design, you need now to implement the proof of concept around streaming restock or sale transaction.","title":"Final store inventory exercice"},{"location":"lab2/#problem-statement","text":"How to get the real-time view of the inventory per store? The ItemTransaction.java represents the message structure of the input topic. Below is an extract of this definition: Here is an example of json message you may see in a topic: { \"id\" : 9 , \"price\" : 30.0 , \"quantity\" : 10 , \"sku\" : \"Item_1\" , \"storeName\" : \"Store_5\" , \"timestamp\" : \"2022-05-19T16:22:37.287937\" , \"type\" : \"RESTOCK\" } { \"id\" : 6 , \"price\" : 10.0 , \"quantity\" : 5 , \"sku\" : \"Item_3\" , \"storeName\" : \"Store_1\" , \"timestamp\" : \"2022-05-12T23:13:31.338671\" , \"type\" : \"SALE\" } We need to compute aggregate for each store level. For example the outcome may look like: { \"storeName\" : \"Store_1\" , \"stock\" :{ \"Item_3\" : 20 , \"Item_2\" : 0 , \"Item_1\" : 20 }} Events could be RESTOCK or SALE. The type attribute defines this. sku represents the item identifier. The input topic is items and the output topic is store.inventory . We assume ItemTransaction fields are all present, so the streaming logic does not need to assess data quality.","title":"Problem statement"},{"location":"lab2/#design-questions","text":"What is the data model you need to use to keep store inventory? What event will be produced to the store.inventory topic? We need to process 5 million messages per day. Day is from 6:00 am to 10 pm every day. Some hints Define a New class to support keeping Store and a Map for items and current stock value. This class will be the event structure to get to the output topic Use a method to update the quantity of an existing inventory instance with a new retrieved event Use KStream aggregate function to create new entry and update existing store entry Solution review and code explanation>>","title":"Design questions"},{"location":"lab2/kstream/","text":"You need to understand some basic on how to use the Kafka Stream APIs so you can develop some simple streaming applications in a scope of a proof of concept. Kafka Stream concepts \u00b6 Any Java or Scala application can use Kafka Streams APIs. Topology \u00b6 The business logic is implemented via a Kafka Streams \"topology\" which represents a graph of processing operators or nodes. Each node within the graph, processes events from the parent node and may generate events for the down stream node(s). This is really close to the Java Streaming APIs or Mutiny APIs, but the APIs used by the topology is from Kafka Streams APIS. Kafka Streams applications are built on top of Kafka producer and consumer APIs and are leveraging Kafka capabilities to do data parallelism processing, to support distributed coordination of partition to task assignment, and to support fault tolerance. In Java the streaming topology will be executed in multiple threads and explicitly started in the Java app. Properties props = buildKafkaProperties (); kafkaStreams = new KafkaStreams ( buildProcessFlow (), props ); kafkaStreams . start (); To build a topology we use a StreamsBuilder class to define the input streams (mapped to a Kakfa topic), the logic to apply to the events and then how to produce results to a Kafka topic. Define a topology // Create a builder final StreamsBuilder builder = new StreamsBuilder (); // (1) // define the KStream abstraction by defining where the data come from (topic) and in which format KStream < String , ItemTransaction > items = builder . stream ( itemSoldInputStreamName , Consumed . with ( Serdes . String (), StoreSerdes . ItemTransactionSerde ())); // (2) final Topology topology = builder . build (); // start the topology in a thread... we will this code later StreamsBuilder to build a topology and run it Define a stream from a topic with serialization of the value to a Java Bean The following figure illustrates the topology concept: So let start by learning more about KStream construct. More Reading If you do not know about topic, kafka producer, and consumer, you may spend time to read some quick kafka concepts More about producer practice And consumer More advanced practices for producer and consumer KStream API \u00b6 KStream is an abstraction of a Kafka record stream. It can be defined from one ot multiple Topics, and will define the structure of the Kafka record key, and the record structure. The following declaration is for consuming from topic named items with Key and Value of type String : KStream < String , String > aStream = builder . stream ( \"items\" , Consumed . with ( Serdes . String (), Serdes . String ())); Then Kstream offers a lot of functions to process the records. Below is a quick summary of the methods you may need to use in the next exercises: Method What it does Example peek Perform an action on each record of KStream. aStream.peek((key, value) -> System.out.println(value) to transform the stream to a topic aStream.to(outTopicName) filter Create a new KStream with records which satisfy the given predicate. .filter((key, value) -> (\"BLUE\".equalsIgnoreCase(value))) split Split this stream into different branches. aStream.split().branch((key, value) -> value.userId == null, Branched.as(\"no-userid\")).defaultBranch(Branched.as(\"non-null\")); groupBy Group the records of this KStream on current key groupByKey Group the records of this KStream on a new key KGroupedStream Intermediate representation of a KStream in order to apply an aggregation operation Ouput of groupByKey aggregate Aggregate the values of records in this stream by the grouped key .aggregate(() -> new StoreInventory(), (store , newItem, existingStoreInventory) -> existingStoreInventory.updateStockQuantity(store,newItem), materializeAsStoreInventoryKafkaStore()); split Split a stream into branches items.split(Named.as(\"B-\")) BranchedKStream Branches the records in the original stream based on the predicates supplied for the branch definitions. .branch((k,v) -> (v.storeName == null), Branched.as(\"wrong-tx\")).defaultBranch(Branched.as(\"good-tx\")); branches.get(\"B-good-tx\").to(outTopicName); branches.get(\"B-wrong-tx\").to(deadLetterTopicName); selectKey Change the key of a record KTable \u00b6 KTable is the second main abstraction of a changelog stream from a primary-keyed table. Each record in this changelog stream is an update on the primary-keyed table with the record key as the primary key. A stream can be considered a changelog of a table, where each data record in the stream captures a state change of the table. The figure below is a simplication of both concepts: A Kstream is first connect to a topic and will receive events with Key,Value structure, as unbounded stream. You can chain Kstream to build a topology, and to a Ktable, which will keep only the last value of a given key. To ouput to a Kafka topic, the final construct is a KStream. KStreams are in memory, Ktables are also in memory but may be persisted. KTable assumes that records from the source topic that have null keys are simply dropped. KTable can be created directly from a Kafka topic (using StreamsBuilder.table operator), as a result of transformations on an existing KTable, or aggregations (aggregate, count, and reduce). Stateful transformations depend on state for processing inputs and producing outputs and require a state store associated with the stream processor. For example, in aggregating operations, a windowing state store is used to collect the latest aggregation results per window. KTables need to additionally maintain their respective state in between events so that operations like aggregations (e.g., COUNT()) can work properly. Every ktable has its own state store. Any operation on the table such as querying, inserting, or updating a row is carried out behind the scenes by a corresponding operation on the table\u2019s state store. These state stores are being materialized on local disk inside your application instances. Kafka Streams uses RocksDB as the default storage engine for persistent state stores. RockDB is a fast key-value server, especially suited for storing data on flash drives. The following figure summarizes all those concepts: There are as many caches as there are threads, but no sharing of caches across threads happens. Records are evicted using a simple LRU scheme after the cache size is reached. The semantics of caching is that data is flushed to the state store and forwarded to the next downstream processor node whenever the earliest of commit.interval.ms or cache.max.bytes.buffering (cache pressure) hits. As illustrated in the example TestAccumulateItemSoldWithCaching when using cache, records are output at the end of the commit interval or when reaching max buffer size. Interesting methods: Method What it does Example filter Create a new KTable that consists of all records of this KTable which satisfy the given predicate join join streams with tables or table to table Read more Apache Kafka - TUTORIAL: WRITE A KAFKA STREAMS APPLICATION KStream API Ktable API Kafka Streams summary Other labs","title":"Kafka Streams Study"},{"location":"lab2/kstream/#kafka-stream-concepts","text":"Any Java or Scala application can use Kafka Streams APIs.","title":"Kafka Stream concepts"},{"location":"lab2/kstream/#topology","text":"The business logic is implemented via a Kafka Streams \"topology\" which represents a graph of processing operators or nodes. Each node within the graph, processes events from the parent node and may generate events for the down stream node(s). This is really close to the Java Streaming APIs or Mutiny APIs, but the APIs used by the topology is from Kafka Streams APIS. Kafka Streams applications are built on top of Kafka producer and consumer APIs and are leveraging Kafka capabilities to do data parallelism processing, to support distributed coordination of partition to task assignment, and to support fault tolerance. In Java the streaming topology will be executed in multiple threads and explicitly started in the Java app. Properties props = buildKafkaProperties (); kafkaStreams = new KafkaStreams ( buildProcessFlow (), props ); kafkaStreams . start (); To build a topology we use a StreamsBuilder class to define the input streams (mapped to a Kakfa topic), the logic to apply to the events and then how to produce results to a Kafka topic. Define a topology // Create a builder final StreamsBuilder builder = new StreamsBuilder (); // (1) // define the KStream abstraction by defining where the data come from (topic) and in which format KStream < String , ItemTransaction > items = builder . stream ( itemSoldInputStreamName , Consumed . with ( Serdes . String (), StoreSerdes . ItemTransactionSerde ())); // (2) final Topology topology = builder . build (); // start the topology in a thread... we will this code later StreamsBuilder to build a topology and run it Define a stream from a topic with serialization of the value to a Java Bean The following figure illustrates the topology concept: So let start by learning more about KStream construct. More Reading If you do not know about topic, kafka producer, and consumer, you may spend time to read some quick kafka concepts More about producer practice And consumer More advanced practices for producer and consumer","title":"Topology"},{"location":"lab2/kstream/#kstream-api","text":"KStream is an abstraction of a Kafka record stream. It can be defined from one ot multiple Topics, and will define the structure of the Kafka record key, and the record structure. The following declaration is for consuming from topic named items with Key and Value of type String : KStream < String , String > aStream = builder . stream ( \"items\" , Consumed . with ( Serdes . String (), Serdes . String ())); Then Kstream offers a lot of functions to process the records. Below is a quick summary of the methods you may need to use in the next exercises: Method What it does Example peek Perform an action on each record of KStream. aStream.peek((key, value) -> System.out.println(value) to transform the stream to a topic aStream.to(outTopicName) filter Create a new KStream with records which satisfy the given predicate. .filter((key, value) -> (\"BLUE\".equalsIgnoreCase(value))) split Split this stream into different branches. aStream.split().branch((key, value) -> value.userId == null, Branched.as(\"no-userid\")).defaultBranch(Branched.as(\"non-null\")); groupBy Group the records of this KStream on current key groupByKey Group the records of this KStream on a new key KGroupedStream Intermediate representation of a KStream in order to apply an aggregation operation Ouput of groupByKey aggregate Aggregate the values of records in this stream by the grouped key .aggregate(() -> new StoreInventory(), (store , newItem, existingStoreInventory) -> existingStoreInventory.updateStockQuantity(store,newItem), materializeAsStoreInventoryKafkaStore()); split Split a stream into branches items.split(Named.as(\"B-\")) BranchedKStream Branches the records in the original stream based on the predicates supplied for the branch definitions. .branch((k,v) -> (v.storeName == null), Branched.as(\"wrong-tx\")).defaultBranch(Branched.as(\"good-tx\")); branches.get(\"B-good-tx\").to(outTopicName); branches.get(\"B-wrong-tx\").to(deadLetterTopicName); selectKey Change the key of a record","title":"KStream API"},{"location":"lab2/kstream/#ktable","text":"KTable is the second main abstraction of a changelog stream from a primary-keyed table. Each record in this changelog stream is an update on the primary-keyed table with the record key as the primary key. A stream can be considered a changelog of a table, where each data record in the stream captures a state change of the table. The figure below is a simplication of both concepts: A Kstream is first connect to a topic and will receive events with Key,Value structure, as unbounded stream. You can chain Kstream to build a topology, and to a Ktable, which will keep only the last value of a given key. To ouput to a Kafka topic, the final construct is a KStream. KStreams are in memory, Ktables are also in memory but may be persisted. KTable assumes that records from the source topic that have null keys are simply dropped. KTable can be created directly from a Kafka topic (using StreamsBuilder.table operator), as a result of transformations on an existing KTable, or aggregations (aggregate, count, and reduce). Stateful transformations depend on state for processing inputs and producing outputs and require a state store associated with the stream processor. For example, in aggregating operations, a windowing state store is used to collect the latest aggregation results per window. KTables need to additionally maintain their respective state in between events so that operations like aggregations (e.g., COUNT()) can work properly. Every ktable has its own state store. Any operation on the table such as querying, inserting, or updating a row is carried out behind the scenes by a corresponding operation on the table\u2019s state store. These state stores are being materialized on local disk inside your application instances. Kafka Streams uses RocksDB as the default storage engine for persistent state stores. RockDB is a fast key-value server, especially suited for storing data on flash drives. The following figure summarizes all those concepts: There are as many caches as there are threads, but no sharing of caches across threads happens. Records are evicted using a simple LRU scheme after the cache size is reached. The semantics of caching is that data is flushed to the state store and forwarded to the next downstream processor node whenever the earliest of commit.interval.ms or cache.max.bytes.buffering (cache pressure) hits. As illustrated in the example TestAccumulateItemSoldWithCaching when using cache, records are output at the end of the commit interval or when reaching max buffer size. Interesting methods: Method What it does Example filter Create a new KTable that consists of all records of this KTable which satisfy the given predicate join join streams with tables or table to table Read more Apache Kafka - TUTORIAL: WRITE A KAFKA STREAMS APPLICATION KStream API Ktable API Kafka Streams summary Other labs","title":"KTable"},{"location":"lab2/lab2-sol/","text":"Lab 2 solution \u00b6 The output inventory class \u00b6 The class needs to keep store name and a map of items and current inventory. The class is StoreInventory public class StoreInventory { public String storeName ; // map <item_id,quantity> public HashMap < String , Long > stock = new HashMap < String , Long > (); This class is used to get to the out topic but inside the Store and aggregate via the update method: public StoreInventory updateStockQuantity ( String key , ItemTransaction newValue ) { this . storeName = key ; if ( newValue . type != null && ItemTransaction . SALE . equals ( newValue . type )) newValue . quantity =- newValue . quantity ; return this . updateStock ( newValue . sku , newValue . quantity ); } public StoreInventory updateStock ( String sku , long newV ) { if ( stock . get ( sku ) == null ) { stock . put ( sku , Long . valueOf ( newV )); } else { Long currentValue = stock . get ( sku ); stock . put ( sku , Long . valueOf ( newV ) + currentValue ); } return this ; } Developing the Topology in test class \u00b6 Continuing test with the TopolofyTestDriver, you will implement the topology with the same structure as before. Here what the topology needs to do: Get ItemTransaction from input stream the Key being the storeName Aggregation wwork on keyed group, so groupByKey the input records Aggregate using the update method. The stream topology looks like: KStream < String , ItemTransaction > items = builder . stream ( inTopicName , Consumed . with ( Serdes . String (), StoreSerdes . ItemTransactionSerde ())); // 2 processing // process items and aggregate at the store level KTable < String , StoreInventory > storeItemInventory = items // use store name as key, which is what the item event is also using . groupByKey () // update the current stock for this <store,item> pair // change the value type . aggregate ( () -> new StoreInventory (), // initializer when there was no store in the table ( store , newItem , existingStoreInventory ) -> existingStoreInventory . updateStockQuantity ( store , newItem ), Materialized . < String , StoreInventory , KeyValueStore < Bytes , byte []>> as ( STORE_INVENTORY_KAFKA_STORE_NAME ) . withKeySerde ( Serdes . String ()) . withValueSerde ( StoreSerdes . StoreInventorySerde ()) ); // Generate to output topic storeItemInventory . toStream (). to ( outTopicName , Produced . with ( Serdes . String (), StoreSerdes . StoreInventorySerde ())); The full application code analysis \u00b6 In fact the topology creation is defined in a business service. The microservice application is using the Liberty runtime and API and the code organization uses the onion architecture introduced in the Domain-driven design: * `domain` contains the business logic and business entities related to item transaction and store inventory. * `infra` is for infrastructure code, containing JAXRS class, event processing, and ser-des. \u251c\u2500\u2500 app \u2502 \u2514\u2500\u2500 StoreAggregatorApplication.java \u251c\u2500\u2500 domain \u2502 \u251c\u2500\u2500 ItemTransaction.java \u2502 \u251c\u2500\u2500 StoreInventory.java \u2502 \u2514\u2500\u2500 StoreInventoryAggregator.java \u2514\u2500\u2500 infra \u251c\u2500\u2500 api \u2502 \u251c\u2500\u2500 StoreInventoryQueries.java \u2502 \u251c\u2500\u2500 StoreInventoryResource.java \u2502 \u251c\u2500\u2500 VersionResource.java \u2502 \u2514\u2500\u2500 dto \u2502 \u251c\u2500\u2500 InventoryQueryResult.java \u2502 \u251c\u2500\u2500 ItemCountQueryResult.java \u2502 \u2514\u2500\u2500 PipelineMetadata.java \u2514\u2500\u2500 events \u251c\u2500\u2500 ItemProcessingAgent.java \u251c\u2500\u2500 JSONSerde.java \u251c\u2500\u2500 KafkaConfig.java \u251c\u2500\u2500 KafkaPropertiesUtil.java \u2514\u2500\u2500 StoreSerdes.java The topology is in the Domain layer in the StoreInventoryAggregator class https://github.com/ibm-cloud-architecture/eda-tech-academy/blob/main/lab2/refarch-eda-store-inventory/src/main/java/ibm/gse/eda/stores/domain/StoreInventoryAggregator.java. The Topology is started in a thread in the ItemProcessingAgent class when the application starts, by looking at the StartupEvent void onStart ( @Observes StartupEvent ev ){ this . kafkaStreams = initializeKafkaStreams (); logger . info ( \"ItemProcessingAgent started\" ); }","title":"Solution Overview"},{"location":"lab2/lab2-sol/#lab-2-solution","text":"","title":"Lab 2 solution"},{"location":"lab2/lab2-sol/#the-output-inventory-class","text":"The class needs to keep store name and a map of items and current inventory. The class is StoreInventory public class StoreInventory { public String storeName ; // map <item_id,quantity> public HashMap < String , Long > stock = new HashMap < String , Long > (); This class is used to get to the out topic but inside the Store and aggregate via the update method: public StoreInventory updateStockQuantity ( String key , ItemTransaction newValue ) { this . storeName = key ; if ( newValue . type != null && ItemTransaction . SALE . equals ( newValue . type )) newValue . quantity =- newValue . quantity ; return this . updateStock ( newValue . sku , newValue . quantity ); } public StoreInventory updateStock ( String sku , long newV ) { if ( stock . get ( sku ) == null ) { stock . put ( sku , Long . valueOf ( newV )); } else { Long currentValue = stock . get ( sku ); stock . put ( sku , Long . valueOf ( newV ) + currentValue ); } return this ; }","title":"The output inventory class"},{"location":"lab2/lab2-sol/#developing-the-topology-in-test-class","text":"Continuing test with the TopolofyTestDriver, you will implement the topology with the same structure as before. Here what the topology needs to do: Get ItemTransaction from input stream the Key being the storeName Aggregation wwork on keyed group, so groupByKey the input records Aggregate using the update method. The stream topology looks like: KStream < String , ItemTransaction > items = builder . stream ( inTopicName , Consumed . with ( Serdes . String (), StoreSerdes . ItemTransactionSerde ())); // 2 processing // process items and aggregate at the store level KTable < String , StoreInventory > storeItemInventory = items // use store name as key, which is what the item event is also using . groupByKey () // update the current stock for this <store,item> pair // change the value type . aggregate ( () -> new StoreInventory (), // initializer when there was no store in the table ( store , newItem , existingStoreInventory ) -> existingStoreInventory . updateStockQuantity ( store , newItem ), Materialized . < String , StoreInventory , KeyValueStore < Bytes , byte []>> as ( STORE_INVENTORY_KAFKA_STORE_NAME ) . withKeySerde ( Serdes . String ()) . withValueSerde ( StoreSerdes . StoreInventorySerde ()) ); // Generate to output topic storeItemInventory . toStream (). to ( outTopicName , Produced . with ( Serdes . String (), StoreSerdes . StoreInventorySerde ()));","title":"Developing the Topology in test class"},{"location":"lab2/lab2-sol/#the-full-application-code-analysis","text":"In fact the topology creation is defined in a business service. The microservice application is using the Liberty runtime and API and the code organization uses the onion architecture introduced in the Domain-driven design: * `domain` contains the business logic and business entities related to item transaction and store inventory. * `infra` is for infrastructure code, containing JAXRS class, event processing, and ser-des. \u251c\u2500\u2500 app \u2502 \u2514\u2500\u2500 StoreAggregatorApplication.java \u251c\u2500\u2500 domain \u2502 \u251c\u2500\u2500 ItemTransaction.java \u2502 \u251c\u2500\u2500 StoreInventory.java \u2502 \u2514\u2500\u2500 StoreInventoryAggregator.java \u2514\u2500\u2500 infra \u251c\u2500\u2500 api \u2502 \u251c\u2500\u2500 StoreInventoryQueries.java \u2502 \u251c\u2500\u2500 StoreInventoryResource.java \u2502 \u251c\u2500\u2500 VersionResource.java \u2502 \u2514\u2500\u2500 dto \u2502 \u251c\u2500\u2500 InventoryQueryResult.java \u2502 \u251c\u2500\u2500 ItemCountQueryResult.java \u2502 \u2514\u2500\u2500 PipelineMetadata.java \u2514\u2500\u2500 events \u251c\u2500\u2500 ItemProcessingAgent.java \u251c\u2500\u2500 JSONSerde.java \u251c\u2500\u2500 KafkaConfig.java \u251c\u2500\u2500 KafkaPropertiesUtil.java \u2514\u2500\u2500 StoreSerdes.java The topology is in the Domain layer in the StoreInventoryAggregator class https://github.com/ibm-cloud-architecture/eda-tech-academy/blob/main/lab2/refarch-eda-store-inventory/src/main/java/ibm/gse/eda/stores/domain/StoreInventoryAggregator.java. The Topology is started in a thread in the ItemProcessingAgent class when the application starts, by looking at the StartupEvent void onStart ( @Observes StartupEvent ev ){ this . kafkaStreams = initializeKafkaStreams (); logger . info ( \"ItemProcessingAgent started\" ); }","title":"The full application code analysis"},{"location":"lab3/","text":"Lab 3: Item inventory demonstration deployment \u00b6 Duration: 20 minutes Goals \u00b6 In this lab, you will learn how to deploy the real-time solution by simply using this repository and a minimum set of commands. The approach is to present reusable structure you may want to use for your own future proof of concept development, so it will be easy to demonstrate your solution. The following diagram illustrates the components, you will deploy in your student's namespace using this repository. More context \u00b6 A traditional solution may be organized with one git repository per application, and at least one GitOps repository to define the deployment artifacts. If you look at the demonstration you are running in this lab, the source code is in the public git account ibm-cloud-architecture with other repositories with following structure: eda-rt-inventory-gitops : the solution gitops repo, built with kam cli which includes everything to declare ArgoCD apps and deployment descriptors eda-gitops-catalog : a git repository to define the different Cloud Pak for Integration operator versions. store simulator application the simulator to send messages to different middleware store aggregator / inventory application to compute store inventory aggregates with Kafka Streams item aggregator / inventory application same for item inventory cross store. pre-requisites \u00b6 See Pre-requisites section in the main page. MAC users can run all the commands in this lab from terminal window. Windows users will have to run the commands from a WSL Command window. Open a CMD window and type 'bash' to enter the WSL prompt. Preparation \u00b6 Each Student will have received a unique identifier and will modify the current settings in this folder with their student id. All the current kubernetes configurations are currently set for poe1 . We assume the following are pre-set in you OpenShift cluster, which is the same as CoC integration cluster: Platform navigator is deployed in cp4i project. Event Streams is installed under cp4i-eventstreams project. Login to the OpenShift cluster assigned to you (poe or ashoka) using the login command from the OpenShift Console Then copy this line: Accept the insecure connection oc login --token = sha256~q......pw --server = https://api.cody.coc-ibm.com:6443 The server uses a certificate signed by an unknown authority. You can bypass the certificate check, but any data you send to the server could be intercepted by others. Use insecure connections? ( y/n ) : y Verify your oc cli works oc get nodes Work under the lab3-4 folder of this repository. Modify existing configuration \u00b6 We will prepare the configuration for the following green components in figure below: The blue components should have been deployed with the Cloud Pak for Integration deployment. The demonstration will run on its own namespace. The env/base folder includes the definition of the namespace, roles, role binding needed to deploy the demonstration. This is a classical way to isolate apps in kubernetes. Running the updateStudent.sh shell script, will modify all the yaml files used by the solution with your student id. As an example we will be student poe10 and the deployment will be done in poe10 namespace. Mac User: export PREFIX = poe10 ./updateStudent.sh Windows user (in Linux shell) export PREFIX = poe10 sudo dos2unix updateStudent.sh sudo dos2unix argocd/updateStudent.sh ./updateStudent.sh [ The argocd/updateStudent.sh will be executed in the next lab ] Folder structure \u00b6 This lab3-4 folder is a reduced version of what the Red Hat's Kubernetes Application Management tool is creating normally. If you want to see a full fledge GitOps version for this demonstration see the eda-rt-inventory-gitops repository . Folder Intent apps Defines deployment, config map, service and route for the 3 applications and kafka connector env Defines the namespace for each deployment, and some service account services Defines MQ broker instance, Kafka Connect cluster, and event streams topics argocd Define the ArgoCD project and apps to monitor this git repo. It will be used for lab 4 Deploy \u00b6 The deployment will configure topics in event streams using a naming convention to avoid conflicts between students, deploy the three apps, deploy MQ broker and Kafka Connect cluster with the MQ source connector configured. Event Gateway, schema registry, and Cloud Object Storage sink connector are not used in this lab Start the deployment make all-no-gitops Verify the solution is up and running oc project $PREFIX oc get pods oc get routes oc get kafkatopic -n cp4i-eventstreams oc get kafkauser -n cp4i-eventstreams Access to the MQ console (replace the namespace and base url) # Open this link in a browser. change poe to another server name if needed https://cpd-cp4i.apps.poe.coc-ibm.com/integration/messaging/ $PREFIX / $PREFIX -mq-ibm-mq/ Verify the queue manager has the ITEMS queue Access to the simulator console Get the URL of the Store Simulator and open it in a browser. oc get route store-simulator -o jsonpath = '{.status.ingress[].host}' Go to the SIMULATOR tab. If you want to test with the MQ source connector, select IBMMQ backend, and starts the Controlled scenario to send predefined messages: Normally you should not see the messages in the ITEMS queue as they are immediatly processed by the Kafka Connector. You should get a set of predefined messages sent to MQ, and then to Kafka $PREFIX-items topic Go to the Event Streams console. If you select Kafka as backend, then the simulator sends directly the messages to Kafka $PREFIX-items topic. The two other options for the Store simulator is to send from 1 to 100 random messages (left choice in the controller view), or continuously send messages (start / stop control in the middle of the page). Access the Event Stream console to look at topic content: # Open this link in a browser. change poe to the OCP cluster name. Login with you user-id using the Enterprise LDAP. https://cpd-cp4i.apps.poe.coc-ibm.com/integration/kafka-clusters/cp4i-eventstreams/es-demo/ The items topic content the store transactions: The item.inventory topic has the aggregates cross stores, as illustrates in figure below: And the store.inventory includes events on current inventory per store: Test the Full Setup \u00b6 From the EventStreams portal, go to Topics and open the items-inventory topic (USERID-item.inventory) Go to Messages. Open the first few messages and take note of the ItemID and CurrentStock. E.g. From the Store Simulator page -> Click on Simulator. Choose IBM MQ for BackEnd. Send one random message. Take note of the message sent. In particular, take note of Item number, Type (either SALE or RESTOCK) and quantity. Go to the EventStreams portal and check if the new message has arrived in USERID-items topic. From the EventStreams portal one new message should also be delivered to the USERID-item.inventory topic. Check the message. The quantity should have been increased (Type RESTOCK) or reduced (SALE) by the quantity number (sent via Store Simulator). You can also do a similar check in USERID-store.inventory topic. Read more on the demonstration script The demonstration instructions are in a separate note as this is a demonstration available in the public git and shareable with anyone. Kafka connector configuration \u00b6 The Kafka connect cluster is defined in the kafka-connect.yam file in the services/kconnect folder. The important part of this file is the Event Streams bootstrap server URL, the kafka version used, and the name of the topic used to persist states of the connector. Each student has its own topic names for offsets, config and status topics. version : 3.0.0 replicas : 2 bootstrapServers : es-demo-kafka-bootstrap.cp4i-eventstreams.svc:9093 image : quay.io/ibmcase/eda-kconnect-cluster-image:latest # ... config : group.id : poe10-connect-cluster offset.storage.topic : poe10-connect-cluster-offsets config.storage.topic : poe10-connect-cluster-configs status.storage.topic : poe10-connect-cluster-status config.storage.replication.factor : 3 offset.storage.replication.factor : 3 status.storage.replication.factor : 3 Recall that the kafka connect cluster runs connectors in parallel and use Kafka consumer and producer API to do the data transfer. The image references a custom image we built to have the MQ source connector and some sink. The Dockerfile for this image is in this repository , you can start from it to add more connector. To set the status of the kafka connect cluster runs the following command: oc get kafkaconnect # example of results NAME DESIRED REPLICAS READY poe10-connect-cluster 2 True The MQ source connector is defined as an app, in the apps/mq-source folder. Below are the important parts to consider: config : mq.queue.manager : poe10MQ mq.connection.name.list : poe10-mq-ibm-mq.poe10.svc mq.channel.name : DEV.APP.SVRCONN mq.queue : ITEMS topic : poe10-items key.converter : org.apache.kafka.connect.storage.StringConverter value.converter : org.apache.kafka.connect.storage.StringConverter mq.record.builder : com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder mq.connection.mode : client mq.message.body.jms : true mq.record.builder.key.header : JMSCorrelationID We do not need to apply any logic on the value conversion. As the messages in MQ are json, we can just consider them as String. The mq.record.builder.key.header: JMSCorrelationID is very important to get the key from the MQ message header. This is a trick here to avoid having a kafks streams program to extract the key from the message and write to another topic, as it could be in real life. The store simulator uses the JMSCorrelationID to post the \"StoreName\" value as a a future key. The Kafka connector use this to write a kafka Producer Record with this key. public void sendMessage ( Item item ) { try { String msg = parser . toJson ( item ); TextMessage message = jmsContext . createTextMessage ( msg ); message . setJMSCorrelationID ( item . storeName ); producer . send ( destination , message ); ... If you want to see the status of the connectors oc get kafkaconnectors # example of results NAME CLUSTER CONNECTOR CLASS MAX TASKS READY mq-source poe10-connect-cluster com.ibm.eventstreams.connect.mqsource.MQSourceConnector 1 True Read more on Kafka connector Techno overview Event Streams Product documentation Strimzi configuration Kafka documentation Kafka connector sink to cloud object storage Kafka connector sink to aws S3 with Camel connector Cleaning your OpenShift project \u00b6 Run the following command to clean the demonstration deployment. You MUST clean the deployment if you will do next GitOps lab. oc project $PREFIX make clean Upon cleaning, check if there are any pods or topics. Troubleshooting \u00b6 Message not sent to MQ \u00b6 This could come from a connection issue between the simulator and MQ. Get the logs for the simulator pod: oc get pod -l app.kubernetes.io/name = store-simulator oc logs <pod_id> Some topics Not created. \u00b6 If the USERID-items topic is not created, try submitting some messages from the Store Simulator. If topics like USERID-item.invetory or USERID-store.inventory is not created, try restarting the following pods: store-simulator-*. item-inventory-*. Then, try sending some messages through the Store Simulator. Running locally \u00b6 Do this step ONLY if you do not have an openshift environment. You will require Docker to complete this step. During proof of concept development you can run Event Streams, MQ and your code on your own laptop with docker engine. We give you a docker compose file to do so. Here are the commands to run the same demonstration locally: cd local docker-compose up -d ./listTopics.sh Access simulator user interface at http://localhost:8080 . Access MQ UI at https://localhost:9443 . Access Kafdrop UI http://localhost:9000 to look at the topic content.","title":"Lab 3 - Real Time Inventory demo"},{"location":"lab3/#lab-3-item-inventory-demonstration-deployment","text":"Duration: 20 minutes","title":"Lab 3: Item inventory demonstration deployment"},{"location":"lab3/#goals","text":"In this lab, you will learn how to deploy the real-time solution by simply using this repository and a minimum set of commands. The approach is to present reusable structure you may want to use for your own future proof of concept development, so it will be easy to demonstrate your solution. The following diagram illustrates the components, you will deploy in your student's namespace using this repository.","title":"Goals"},{"location":"lab3/#more-context","text":"A traditional solution may be organized with one git repository per application, and at least one GitOps repository to define the deployment artifacts. If you look at the demonstration you are running in this lab, the source code is in the public git account ibm-cloud-architecture with other repositories with following structure: eda-rt-inventory-gitops : the solution gitops repo, built with kam cli which includes everything to declare ArgoCD apps and deployment descriptors eda-gitops-catalog : a git repository to define the different Cloud Pak for Integration operator versions. store simulator application the simulator to send messages to different middleware store aggregator / inventory application to compute store inventory aggregates with Kafka Streams item aggregator / inventory application same for item inventory cross store.","title":"More context"},{"location":"lab3/#pre-requisites","text":"See Pre-requisites section in the main page. MAC users can run all the commands in this lab from terminal window. Windows users will have to run the commands from a WSL Command window. Open a CMD window and type 'bash' to enter the WSL prompt.","title":"pre-requisites"},{"location":"lab3/#preparation","text":"Each Student will have received a unique identifier and will modify the current settings in this folder with their student id. All the current kubernetes configurations are currently set for poe1 . We assume the following are pre-set in you OpenShift cluster, which is the same as CoC integration cluster: Platform navigator is deployed in cp4i project. Event Streams is installed under cp4i-eventstreams project. Login to the OpenShift cluster assigned to you (poe or ashoka) using the login command from the OpenShift Console Then copy this line: Accept the insecure connection oc login --token = sha256~q......pw --server = https://api.cody.coc-ibm.com:6443 The server uses a certificate signed by an unknown authority. You can bypass the certificate check, but any data you send to the server could be intercepted by others. Use insecure connections? ( y/n ) : y Verify your oc cli works oc get nodes Work under the lab3-4 folder of this repository.","title":"Preparation"},{"location":"lab3/#modify-existing-configuration","text":"We will prepare the configuration for the following green components in figure below: The blue components should have been deployed with the Cloud Pak for Integration deployment. The demonstration will run on its own namespace. The env/base folder includes the definition of the namespace, roles, role binding needed to deploy the demonstration. This is a classical way to isolate apps in kubernetes. Running the updateStudent.sh shell script, will modify all the yaml files used by the solution with your student id. As an example we will be student poe10 and the deployment will be done in poe10 namespace. Mac User: export PREFIX = poe10 ./updateStudent.sh Windows user (in Linux shell) export PREFIX = poe10 sudo dos2unix updateStudent.sh sudo dos2unix argocd/updateStudent.sh ./updateStudent.sh [ The argocd/updateStudent.sh will be executed in the next lab ]","title":"Modify existing configuration"},{"location":"lab3/#folder-structure","text":"This lab3-4 folder is a reduced version of what the Red Hat's Kubernetes Application Management tool is creating normally. If you want to see a full fledge GitOps version for this demonstration see the eda-rt-inventory-gitops repository . Folder Intent apps Defines deployment, config map, service and route for the 3 applications and kafka connector env Defines the namespace for each deployment, and some service account services Defines MQ broker instance, Kafka Connect cluster, and event streams topics argocd Define the ArgoCD project and apps to monitor this git repo. It will be used for lab 4","title":"Folder structure"},{"location":"lab3/#deploy","text":"The deployment will configure topics in event streams using a naming convention to avoid conflicts between students, deploy the three apps, deploy MQ broker and Kafka Connect cluster with the MQ source connector configured. Event Gateway, schema registry, and Cloud Object Storage sink connector are not used in this lab Start the deployment make all-no-gitops Verify the solution is up and running oc project $PREFIX oc get pods oc get routes oc get kafkatopic -n cp4i-eventstreams oc get kafkauser -n cp4i-eventstreams Access to the MQ console (replace the namespace and base url) # Open this link in a browser. change poe to another server name if needed https://cpd-cp4i.apps.poe.coc-ibm.com/integration/messaging/ $PREFIX / $PREFIX -mq-ibm-mq/ Verify the queue manager has the ITEMS queue Access to the simulator console Get the URL of the Store Simulator and open it in a browser. oc get route store-simulator -o jsonpath = '{.status.ingress[].host}' Go to the SIMULATOR tab. If you want to test with the MQ source connector, select IBMMQ backend, and starts the Controlled scenario to send predefined messages: Normally you should not see the messages in the ITEMS queue as they are immediatly processed by the Kafka Connector. You should get a set of predefined messages sent to MQ, and then to Kafka $PREFIX-items topic Go to the Event Streams console. If you select Kafka as backend, then the simulator sends directly the messages to Kafka $PREFIX-items topic. The two other options for the Store simulator is to send from 1 to 100 random messages (left choice in the controller view), or continuously send messages (start / stop control in the middle of the page). Access the Event Stream console to look at topic content: # Open this link in a browser. change poe to the OCP cluster name. Login with you user-id using the Enterprise LDAP. https://cpd-cp4i.apps.poe.coc-ibm.com/integration/kafka-clusters/cp4i-eventstreams/es-demo/ The items topic content the store transactions: The item.inventory topic has the aggregates cross stores, as illustrates in figure below: And the store.inventory includes events on current inventory per store:","title":"Deploy"},{"location":"lab3/#test-the-full-setup","text":"From the EventStreams portal, go to Topics and open the items-inventory topic (USERID-item.inventory) Go to Messages. Open the first few messages and take note of the ItemID and CurrentStock. E.g. From the Store Simulator page -> Click on Simulator. Choose IBM MQ for BackEnd. Send one random message. Take note of the message sent. In particular, take note of Item number, Type (either SALE or RESTOCK) and quantity. Go to the EventStreams portal and check if the new message has arrived in USERID-items topic. From the EventStreams portal one new message should also be delivered to the USERID-item.inventory topic. Check the message. The quantity should have been increased (Type RESTOCK) or reduced (SALE) by the quantity number (sent via Store Simulator). You can also do a similar check in USERID-store.inventory topic. Read more on the demonstration script The demonstration instructions are in a separate note as this is a demonstration available in the public git and shareable with anyone.","title":"Test the Full Setup"},{"location":"lab3/#kafka-connector-configuration","text":"The Kafka connect cluster is defined in the kafka-connect.yam file in the services/kconnect folder. The important part of this file is the Event Streams bootstrap server URL, the kafka version used, and the name of the topic used to persist states of the connector. Each student has its own topic names for offsets, config and status topics. version : 3.0.0 replicas : 2 bootstrapServers : es-demo-kafka-bootstrap.cp4i-eventstreams.svc:9093 image : quay.io/ibmcase/eda-kconnect-cluster-image:latest # ... config : group.id : poe10-connect-cluster offset.storage.topic : poe10-connect-cluster-offsets config.storage.topic : poe10-connect-cluster-configs status.storage.topic : poe10-connect-cluster-status config.storage.replication.factor : 3 offset.storage.replication.factor : 3 status.storage.replication.factor : 3 Recall that the kafka connect cluster runs connectors in parallel and use Kafka consumer and producer API to do the data transfer. The image references a custom image we built to have the MQ source connector and some sink. The Dockerfile for this image is in this repository , you can start from it to add more connector. To set the status of the kafka connect cluster runs the following command: oc get kafkaconnect # example of results NAME DESIRED REPLICAS READY poe10-connect-cluster 2 True The MQ source connector is defined as an app, in the apps/mq-source folder. Below are the important parts to consider: config : mq.queue.manager : poe10MQ mq.connection.name.list : poe10-mq-ibm-mq.poe10.svc mq.channel.name : DEV.APP.SVRCONN mq.queue : ITEMS topic : poe10-items key.converter : org.apache.kafka.connect.storage.StringConverter value.converter : org.apache.kafka.connect.storage.StringConverter mq.record.builder : com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder mq.connection.mode : client mq.message.body.jms : true mq.record.builder.key.header : JMSCorrelationID We do not need to apply any logic on the value conversion. As the messages in MQ are json, we can just consider them as String. The mq.record.builder.key.header: JMSCorrelationID is very important to get the key from the MQ message header. This is a trick here to avoid having a kafks streams program to extract the key from the message and write to another topic, as it could be in real life. The store simulator uses the JMSCorrelationID to post the \"StoreName\" value as a a future key. The Kafka connector use this to write a kafka Producer Record with this key. public void sendMessage ( Item item ) { try { String msg = parser . toJson ( item ); TextMessage message = jmsContext . createTextMessage ( msg ); message . setJMSCorrelationID ( item . storeName ); producer . send ( destination , message ); ... If you want to see the status of the connectors oc get kafkaconnectors # example of results NAME CLUSTER CONNECTOR CLASS MAX TASKS READY mq-source poe10-connect-cluster com.ibm.eventstreams.connect.mqsource.MQSourceConnector 1 True Read more on Kafka connector Techno overview Event Streams Product documentation Strimzi configuration Kafka documentation Kafka connector sink to cloud object storage Kafka connector sink to aws S3 with Camel connector","title":"Kafka connector configuration"},{"location":"lab3/#cleaning-your-openshift-project","text":"Run the following command to clean the demonstration deployment. You MUST clean the deployment if you will do next GitOps lab. oc project $PREFIX make clean Upon cleaning, check if there are any pods or topics.","title":"Cleaning your OpenShift project"},{"location":"lab3/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"lab3/#message-not-sent-to-mq","text":"This could come from a connection issue between the simulator and MQ. Get the logs for the simulator pod: oc get pod -l app.kubernetes.io/name = store-simulator oc logs <pod_id>","title":"Message not sent to MQ"},{"location":"lab3/#some-topics-not-created","text":"If the USERID-items topic is not created, try submitting some messages from the Store Simulator. If topics like USERID-item.invetory or USERID-store.inventory is not created, try restarting the following pods: store-simulator-*. item-inventory-*. Then, try sending some messages through the Store Simulator.","title":"Some topics Not created."},{"location":"lab3/#running-locally","text":"Do this step ONLY if you do not have an openshift environment. You will require Docker to complete this step. During proof of concept development you can run Event Streams, MQ and your code on your own laptop with docker engine. We give you a docker compose file to do so. Here are the commands to run the same demonstration locally: cd local docker-compose up -d ./listTopics.sh Access simulator user interface at http://localhost:8080 . Access MQ UI at https://localhost:9443 . Access Kafdrop UI http://localhost:9000 to look at the topic content.","title":"Running locally"},{"location":"lab4/","text":"GitOps deployment with Day 2 operations \u00b6 In this exercise, you will use GitOps to deploy ArgoCD apps that monitor your git repository for any configuration changes you are doing via Pull Request or Git Commit operations and then apply those changes to the deployed applications. The figure below illustrates the components involved: In this lab the operators are already installed in the OpenShift cluster under the openshift-operators project, and products are already install too. So this lab is aimed to deploy the components of the real-time inventory demo (the green components in figure above). As stated before you need to fork this repository under your own public git account, as all configurations will be monitored from your own git repository. Pre-Requisites \u00b6 See Pre-requisites section in the main page. MAC users can run all the commands in this lab from terminal window. Windows users will have to run the commands from a WSL Command window. Open a CMD window and type 'bash' to enter the WSL prompt. You should have completed / attempted Lab 3. Specifically, you should have run the \"updateStudent.sh\" script which makes namespace changes in the yaml files. Deployment \u00b6 Verify the OpenShift GitOps Operator is installed on your OpenShift cluster. In fact it should be installed, but this command may be helpful to you in your future proof of concepts. Work in the eda-tech-academy/lab3-4 folder. make verify-argocd-available Should get this output if not installed Installing Complete Or this output if it is already installed. openshift-gitops-operator Installed Prepare the ArgoCD app and project: Each student will have his/her own project within ArgoCD. Automatic way: # under the lab3-4 folder export PREFIX = poe10 export GIT_ACCOUNT = <yourname GIT account name> # same exported variables as before sudo make prepare-argocd Manual way: . [Update the namespace, project, and repoURL elements in the argocd/*.yaml files.] To get the ArgoCD admin user's password use the command oc extract secret/openshift-gitops-cluster -n openshift-gitops --to = - Get the ArgoCD User Interface URL and open it in a web browser oc get route openshift-gitops-server -o jsonpath = '{.status.ingress[].host}' -n openshift-gitops Verify you are able to login to the ArgoCD portal. Go to applications and ensure there are no applications that have been created. Commit and push your changes to your gitops repository (The fork for eda-tech-academy). You can use GitHub desktop or git CLI to push the changes to your repository. Here we are using the git CLI. You can add a remote URl by replacing with your GitHub username in git. git remote add mine https://github.com/<yourusername>/eda-tech-academy.git git commit -am \"update configuration for my student id\" git push -u mine Enter your github id and Token. Please note Github requires Personal Access Token for Github Operations. You can refer here for more details. https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token Bootstrap Argocd: make gitops Verify in the ArgoCD console the apps are started and process the synchronization. Demonstration \u00b6 You should be in the same state as in Lab 3 with the Simulator, the two kafka streams app, MQ and Kafka Connect oc get pods oc get kafkaconnectors oc -n cp4i-eventstreams get kafkatopics Lets do one simple test to see the imapct of ArgoCD. Edit this file: lab3-4/apps/item-inventory/base/deployment.yaml. and change spec.replicas to 2. Push you change to Github using GitHub Desktop or git CLI. Wait for a few minutes and check the pods. oc get pods. You should see the replicas for item-inventory has increased. There should be 2 pods now. You can check ArgoCD view and see 2 pods. Clean up \u00b6 Full clean up the deployment If you want to stop working and clean the OpenShift cluster and event streams elements ```sh make clean-gitops","title":"Lab 4 - EDA GitOps"},{"location":"lab4/#gitops-deployment-with-day-2-operations","text":"In this exercise, you will use GitOps to deploy ArgoCD apps that monitor your git repository for any configuration changes you are doing via Pull Request or Git Commit operations and then apply those changes to the deployed applications. The figure below illustrates the components involved: In this lab the operators are already installed in the OpenShift cluster under the openshift-operators project, and products are already install too. So this lab is aimed to deploy the components of the real-time inventory demo (the green components in figure above). As stated before you need to fork this repository under your own public git account, as all configurations will be monitored from your own git repository.","title":"GitOps deployment with Day 2 operations"},{"location":"lab4/#pre-requisites","text":"See Pre-requisites section in the main page. MAC users can run all the commands in this lab from terminal window. Windows users will have to run the commands from a WSL Command window. Open a CMD window and type 'bash' to enter the WSL prompt. You should have completed / attempted Lab 3. Specifically, you should have run the \"updateStudent.sh\" script which makes namespace changes in the yaml files.","title":"Pre-Requisites"},{"location":"lab4/#deployment","text":"Verify the OpenShift GitOps Operator is installed on your OpenShift cluster. In fact it should be installed, but this command may be helpful to you in your future proof of concepts. Work in the eda-tech-academy/lab3-4 folder. make verify-argocd-available Should get this output if not installed Installing Complete Or this output if it is already installed. openshift-gitops-operator Installed Prepare the ArgoCD app and project: Each student will have his/her own project within ArgoCD. Automatic way: # under the lab3-4 folder export PREFIX = poe10 export GIT_ACCOUNT = <yourname GIT account name> # same exported variables as before sudo make prepare-argocd Manual way: . [Update the namespace, project, and repoURL elements in the argocd/*.yaml files.] To get the ArgoCD admin user's password use the command oc extract secret/openshift-gitops-cluster -n openshift-gitops --to = - Get the ArgoCD User Interface URL and open it in a web browser oc get route openshift-gitops-server -o jsonpath = '{.status.ingress[].host}' -n openshift-gitops Verify you are able to login to the ArgoCD portal. Go to applications and ensure there are no applications that have been created. Commit and push your changes to your gitops repository (The fork for eda-tech-academy). You can use GitHub desktop or git CLI to push the changes to your repository. Here we are using the git CLI. You can add a remote URl by replacing with your GitHub username in git. git remote add mine https://github.com/<yourusername>/eda-tech-academy.git git commit -am \"update configuration for my student id\" git push -u mine Enter your github id and Token. Please note Github requires Personal Access Token for Github Operations. You can refer here for more details. https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token Bootstrap Argocd: make gitops Verify in the ArgoCD console the apps are started and process the synchronization.","title":"Deployment"},{"location":"lab4/#demonstration","text":"You should be in the same state as in Lab 3 with the Simulator, the two kafka streams app, MQ and Kafka Connect oc get pods oc get kafkaconnectors oc -n cp4i-eventstreams get kafkatopics Lets do one simple test to see the imapct of ArgoCD. Edit this file: lab3-4/apps/item-inventory/base/deployment.yaml. and change spec.replicas to 2. Push you change to Github using GitHub Desktop or git CLI. Wait for a few minutes and check the pods. oc get pods. You should see the replicas for item-inventory has increased. There should be 2 pods now. You can check ArgoCD view and see 2 pods.","title":"Demonstration"},{"location":"lab4/#clean-up","text":"Full clean up the deployment If you want to stop working and clean the OpenShift cluster and event streams elements ```sh make clean-gitops","title":"Clean up"},{"location":"lab5/","text":"Intro to Instana Observability for Event Streams (Kafka) Messaging \u00b6 This is a brief introduction to how you can observe your Event Streams Deployment along with applications in a simulated \"Day 2\" operations mode. In this exercise, you will see how Instana can monitor: OCP, Kafka Brokers, Zookeeper, Kafka Connect and custom deployed applications. Demonstration by Instana Tech Sellers \u00b6 The Instana Tech Sellers will show a demo of Instana. Access the Instana Instance connected to the Event Streams Cluster \u00b6 Go directly to Instana Home Page Login using user id: trainerintegration@gmail.com and the password given to you by the instructors. Note: all attendees will use the same login credentials. After you login you should see something like the following. Optional - If this is the first time to the site you might have to dismiss the message in the top right corner by single clicking on it. Explore the left navigation bar by hovering your mouse over the black bar on the left side. You should see something like the following. Base Infrastructure Observability \u00b6 OCP Worker Nodes - Cluster Map \u00b6 Go directly to Infrastructure for OCP Worker Nodes you should see something like the following. Click the down arrow next to \"Hosts (6)\" in the white box in the upper left corner to see all 6 worker nodes. Click one of the worker nodes and you should see something like the following. Note: you can scroll and open accordians. Click \"Open Dashboard\" button and you should see something like the following. OCP Worker Nodes - Cluster k8s Dashboard \u00b6 Go directly to Infrastructure for OCP Worker Nodes you should see something like the following. Note the master and worker nodes are shown along with all the namespaces and the deployments. All Kubernetes Services for the Event Streams Cluster \u00b6 Go directly to Kubernetes Cluster Services for cp4i-eventstreams OCP cluster you should see something like the following. Kafka Brokers \u00b6 Go directly to Kubernetes Cluster Services for es-demo-kafka-brokers Kafka Brokers you should see something like the following. And Scroll down to see the following. Zookeeper Services \u00b6 Go directly to Kubernetes Cluster Services for Zookeeper you should see something like the following. And Scroll down to see the following. Application and Message Dependencies \u00b6 Go directly to Application Dependencies for the Messages you should see something like the following. Application Message Flow and End to End Observability \u00b6 Instana is the ONLY tool that provides full end to end Kafka Message Observability. All Services with Dependencies - Unfiltered \u00b6 Go directly to Map of All Services with Dependencies - Unfiltered you should see something like the following. Message-based Dependencies - Filtered \u00b6 Go directly to Map of Message-based Dependencies - Filtered you should see something like the following. The instructors can demonstrate more capabilities for end to end messages. Failure & Root Cause Analyis \u00b6 Simulate Failure \u00b6 This will be a future exercise and/or demo. Drill down to Root Cause \u00b6 This will be a future exercise and/or demo. Optional/Takeaway - Explore Instana functionality using the Instana Cloud Sandbox \u00b6 Go directly to Instana Cloud Sandbox Instance and login using your IBM email address. After you login using your IBM email addess you should see something like the following. Click on the \"Get Started\" button at the bottom left hand corner. Follow each of the 5 tutorials to get a feel for Instana.","title":"Lab 5 - Monitoring with Instana"},{"location":"lab5/#intro-to-instana-observability-for-event-streams-kafka-messaging","text":"This is a brief introduction to how you can observe your Event Streams Deployment along with applications in a simulated \"Day 2\" operations mode. In this exercise, you will see how Instana can monitor: OCP, Kafka Brokers, Zookeeper, Kafka Connect and custom deployed applications.","title":"Intro to Instana Observability for Event Streams (Kafka) Messaging"},{"location":"lab5/#demonstration-by-instana-tech-sellers","text":"The Instana Tech Sellers will show a demo of Instana.","title":"Demonstration by Instana Tech Sellers"},{"location":"lab5/#access-the-instana-instance-connected-to-the-event-streams-cluster","text":"Go directly to Instana Home Page Login using user id: trainerintegration@gmail.com and the password given to you by the instructors. Note: all attendees will use the same login credentials. After you login you should see something like the following. Optional - If this is the first time to the site you might have to dismiss the message in the top right corner by single clicking on it. Explore the left navigation bar by hovering your mouse over the black bar on the left side. You should see something like the following.","title":"Access the Instana Instance connected to the Event Streams Cluster"},{"location":"lab5/#base-infrastructure-observability","text":"","title":"Base Infrastructure Observability"},{"location":"lab5/#ocp-worker-nodes-cluster-map","text":"Go directly to Infrastructure for OCP Worker Nodes you should see something like the following. Click the down arrow next to \"Hosts (6)\" in the white box in the upper left corner to see all 6 worker nodes. Click one of the worker nodes and you should see something like the following. Note: you can scroll and open accordians. Click \"Open Dashboard\" button and you should see something like the following.","title":"OCP Worker Nodes - Cluster Map"},{"location":"lab5/#ocp-worker-nodes-cluster-k8s-dashboard","text":"Go directly to Infrastructure for OCP Worker Nodes you should see something like the following. Note the master and worker nodes are shown along with all the namespaces and the deployments.","title":"OCP Worker Nodes - Cluster k8s Dashboard"},{"location":"lab5/#all-kubernetes-services-for-the-event-streams-cluster","text":"Go directly to Kubernetes Cluster Services for cp4i-eventstreams OCP cluster you should see something like the following.","title":"All Kubernetes Services for the Event Streams Cluster"},{"location":"lab5/#kafka-brokers","text":"Go directly to Kubernetes Cluster Services for es-demo-kafka-brokers Kafka Brokers you should see something like the following. And Scroll down to see the following.","title":"Kafka Brokers"},{"location":"lab5/#zookeeper-services","text":"Go directly to Kubernetes Cluster Services for Zookeeper you should see something like the following. And Scroll down to see the following.","title":"Zookeeper Services"},{"location":"lab5/#application-message-flow-and-end-to-end-observability","text":"Instana is the ONLY tool that provides full end to end Kafka Message Observability.","title":"Application Message Flow and End to End Observability"},{"location":"lab5/#all-services-with-dependencies-unfiltered","text":"Go directly to Map of All Services with Dependencies - Unfiltered you should see something like the following.","title":"All Services with Dependencies - Unfiltered"},{"location":"lab5/#message-based-dependencies-filtered","text":"Go directly to Map of Message-based Dependencies - Filtered you should see something like the following. The instructors can demonstrate more capabilities for end to end messages.","title":"Message-based Dependencies - Filtered"},{"location":"lab5/#failure-root-cause-analyis","text":"","title":"Failure &amp; Root Cause Analyis"},{"location":"lab5/#simulate-failure","text":"This will be a future exercise and/or demo.","title":"Simulate Failure"},{"location":"lab5/#drill-down-to-root-cause","text":"This will be a future exercise and/or demo.","title":"Drill down to Root Cause"},{"location":"lab5/#optionaltakeaway-explore-instana-functionality-using-the-instana-cloud-sandbox","text":"Go directly to Instana Cloud Sandbox Instance and login using your IBM email address. After you login using your IBM email addess you should see something like the following. Click on the \"Get Started\" button at the bottom left hand corner. Follow each of the 5 tutorials to get a feel for Instana.","title":"Optional/Takeaway - Explore Instana functionality using the Instana Cloud Sandbox"}]}